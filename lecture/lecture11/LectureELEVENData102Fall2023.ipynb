{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9fa29390-1422-43dc-9012-0477e4321995","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport arviz as az\nnp.set_printoptions(suppress=True, formatter={'float_kind':'{:f}'.format}) \n#this option makes Jupyter print numbers in ordinary (as opposed to scientific) notation","metadata":{"trusted":true},"outputs":[],"execution_count":11},{"id":"b3f97230-a6f8-4da0-a7dc-4faa4194dd5c","cell_type":"markdown","source":"# Linear Regression and Poisson Regression\n\nWe shall review linear regression in the context of an applied dataset. We will also use this example to motivate Poisson Regression.\n\n## Review of Linear Regression with an Economics Dataset\n\nThe dataset (called MROZ) comes from an Econometrica paper by Mroz in 1987 and contains data on a bunch of variables for married women in the year 1975. This dataset is analyzed in the Econometrics book by Wooldridge (this book contains many other interesting economic and political datasets). The specific variables included in the Mroz dataset are: \n\n1. inlf: binary variable equaling 1 if the individual worked (i.e., they were 'in the labor force') in the year 1975 and 0 otherwise\n2. hours: number of hours worked in 1975\n3. kidslt6: number of kids < 6 years of age\n4. kidsge6: number of kids 6-18 years of age\n5. age: age in years\n6. educ: years of schooling\n7. wage: hourly wage in 1975\n8. repwage: reported wage at interview in 1976\n9. hushrs: hours worked by husband in 1975\n10. husage: husband's age\n11. huseduc: husband's years of schooling\n12. huswage: husband's hourly wage in 1975\n13. faminc: family income in 1975\n14. mtr: federal marginal tax rate facing woman\n15. motheduc: mother's years of schooling\n16. fatheduc: father's years of schooling\n17. unem: unemployment rate in county of residence\n18. city: =1 if live in Standard metropolitan statistical area\n19. exper: actual labor market experience\n20. nwifeinc: (faminc - wage*hours)/1000\n21. lwage: log(wage)\n22. expersq: $\\text{exper}^2$ (the square of the experience variable)","metadata":{}},{"id":"c34fe720-4362-4bf1-bc51-8f6c0620ee40","cell_type":"code","source":"#Import the MROZ.csv dataset\nmroz = pd.read_csv(\"MROZ.csv\")\nprint(mroz.head(12))","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"    inlf  hours  kidslt6  kidsge6  age  educ    wage  repwage  hushrs  husage  \\\n0      1   1610        1        0   32    12  3.3540     2.65    2708      34   \n1      1   1656        0        2   30    12  1.3889     2.65    2310      30   \n2      1   1980        1        3   35    12  4.5455     4.04    3072      40   \n3      1    456        0        3   34    12  1.0965     3.25    1920      53   \n4      1   1568        1        2   31    14  4.5918     3.60    2000      32   \n5      1   2032        0        0   54    12  4.7421     4.70    1040      57   \n6      1   1440        0        2   37    16  8.3333     5.95    2670      37   \n7      1   1020        0        0   54    12  7.8431     9.98    4120      53   \n8      1   1458        0        2   48    12  2.1262     0.00    1995      52   \n9      1   1600        0        2   39    12  4.6875     4.15    2100      43   \n10     1   1969        0        1   33    12  4.0630     4.30    2450      34   \n11     1   1960        0        1   42    11  4.5918     4.58    2375      47   \n\n    ...  faminc     mtr  motheduc  fatheduc  unem  city  exper   nwifeinc  \\\n0   ...   16310  0.7215        12         7   5.0     0     14  10.910060   \n1   ...   21800  0.6615         7         7  11.0     1      5  19.499981   \n2   ...   21040  0.6915        12         7   5.0     0     15  12.039910   \n3   ...    7300  0.7815         7         7   5.0     0      6   6.799996   \n4   ...   27300  0.6215        12        14   9.5     1      7  20.100058   \n5   ...   19495  0.6915        14         7   7.5     1     33   9.859054   \n6   ...   21152  0.6915        14         7   5.0     0     11   9.152048   \n7   ...   18900  0.6915         3         3   5.0     0     35  10.900038   \n8   ...   20405  0.7515         7         7   3.0     0     24  17.305000   \n9   ...   20425  0.6915         7         7   5.0     0     21  12.925000   \n10  ...   32300  0.5815        12         3   5.0     0     15  24.299953   \n11  ...   28700  0.6215        14         7   5.0     0     14  19.700071   \n\n       lwage  expersq  \n0   1.210154      196  \n1   0.328512       25  \n2   1.514138      225  \n3   0.092123       36  \n4   1.524272       49  \n5   1.556480     1089  \n6   2.120260      121  \n7   2.059634     1225  \n8   0.754336      576  \n9   1.544899      441  \n10  1.401922      225  \n11  1.524272      196  \n\n[12 rows x 22 columns]\n","output_type":"stream"}],"execution_count":12},{"id":"010f2457-1697-413a-87df-d27584988bcf","cell_type":"markdown","source":"Several regressions can be fit on this dataset. Let us start by fitting a linear regression model with \"hours\" as the response variable, and \"kidslt6\", \"kidsge6\", \"age\", \"educ\", \"exper\", \"expersq\", \"huswage\", \"huseduc\", \"hushrs\", \"motheduc\" and \"fatheduc\" as the covariates (or predictor variables). Note that awe are using both \"exper\" and \"expersq\" (which is the square of the variable \"exper\") as covariates in the regression. This is becauswe we would expect \"hours\" to increase with \"exper\" for small values of \"exper\" (for women new to the workforce) but to perhaps decrease with \"exper\" for large values of \"exper\" (for example, people with very large \"exper\" might be closer to retirement and work less leading to smaller \"hours\"). Such a relationship cannot be captured by linear functions but can be captured by quadratic functions which is why both \"exper\" and \"expersq\" are included in the  model. \n\nUsing the statsmodels package in Python, this regression can be carried out in the following way. ","metadata":{}},{"id":"2aa3dcd5-5eef-4d04-b988-4cd0dd08df47","cell_type":"code","source":"#Several regressions can be fit on this dataset. Let us fit one with\n#hours as the response variable, and\n#kidslt6, kidsge6, age, educ, exper, expersq, huswage, huseduc, hushrs, motheduc and fatheduc\n#as covariates\nimport statsmodels.api as sm\n#Define the response variable and covariates\nY = mroz['hours']\nX = mroz[['kidslt6', 'kidsge6', 'age', 'educ', \n       'hushrs',  'huseduc', 'huswage',  'motheduc',\n       'fatheduc', 'exper', 'expersq']].copy()\n#Add a constant (intercept) to the model\nX = sm.add_constant(X)\n\n#Fit the model: \nmodel = sm.OLS(Y, X).fit()\nprint(model.summary())","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  hours   R-squared:                       0.273\nModel:                            OLS   Adj. R-squared:                  0.262\nMethod:                 Least Squares   F-statistic:                     25.30\nDate:                Sat, 30 Sep 2023   Prob (F-statistic):           1.05e-44\nTime:                        03:18:56   Log-Likelihood:                -6045.7\nNo. Observations:                 753   AIC:                         1.212e+04\nDf Residuals:                     741   BIC:                         1.217e+04\nDf Model:                          11                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       1488.5175    293.748      5.067      0.000     911.839    2065.196\nkidslt6     -439.3761     58.748     -7.479      0.000    -554.709    -324.043\nkidsge6      -32.6212     23.202     -1.406      0.160     -78.171      12.928\nage          -30.4462      4.382     -6.948      0.000     -39.049     -21.844\neduc          41.2569     16.470      2.505      0.012       8.924      73.590\nhushrs        -0.0635      0.049     -1.292      0.197      -0.160       0.033\nhuseduc      -16.1572     12.297     -1.314      0.189     -40.298       7.983\nhuswage      -13.7469      7.556     -1.819      0.069     -28.580       1.086\nmotheduc      10.9852     10.368      1.059      0.290      -9.370      31.340\nfatheduc      -4.0224      9.771     -0.412      0.681     -23.205      15.161\nexper         65.9440      9.984      6.605      0.000      46.344      85.544\nexpersq       -0.7289      0.325     -2.241      0.025      -1.368      -0.090\n==============================================================================\nOmnibus:                       82.696   Durbin-Watson:                   1.389\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              120.379\nSkew:                           0.786   Prob(JB):                     7.25e-27\nKurtosis:                       4.167   Cond. No.                     2.54e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.54e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"}],"execution_count":13},{"id":"cafa8507-f749-4900-9ae6-22a1b0a180c9","cell_type":"markdown","source":"The table above gives estimates of the regression coefficients and their standard errors for each of the covariates. These estimates and standard errors are computed in the following way. The data is denoted by $(y_i, x_{i1}, x_{i2}, \\dots, x_{im})$ for $i = 1, \\dots, n$ (here $m$ is the number of covariates and $n$ is the number of observations). One often represents the response data ($y_1, \\dots, y_n$) in a $n \\times 1$ vector called $Y$, and the covariate data $(x_{i1}, \\dots, x_{im})$ for $i = 1, \\dots, n$ in a $n \\times (m+1)$ matrix $X$ as follows:\n\\begin{align*}\n  Y =\n  \\begin{pmatrix}\n    y_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ y_n\n  \\end{pmatrix}\n  ~~\\text{ and }~~\n  X =\n  \\begin{pmatrix}\n   1 & x_{11} & \\cdot & \\cdot & \\cdot & x_{1m} \\\\ \\cdot &  \\cdot &\n   \\cdot & \\cdot & \\cdot & \\cdot\n   \\\\ \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\  1 & x_{i1} &\n   \\cdot & \\cdot & \\cdot & x_{im} \\\\ \\cdot & \\cdot &\n   \\cdot & \\cdot & \\cdot & \\cdot \\\\ \\cdot & \\cdot & \\cdot\n   & \\cdot & \\cdot & \\cdot\\\\ 1 & x_{n1} & \\cdot & \\cdot & \\cdot & x_{nm}\n \\end{pmatrix}\n\\end{align*}\nNote the additional column of ones in the $X$ matrix. In this notation, the estimates of the regression coefficients are computed by solving equation: \n\\begin{align*}\n   X^T X \\beta = X^T Y ~~~\\text{ leading to } ~~~ \\hat{\\beta} = (X^T X)^{-1} X^T Y.\n\\end{align*}\nThe standard errors are computed in the following way. First one computes the matrix:\n\\begin{align*}\n   \\hat{\\sigma}^2 (X^T X)^{-1} ~~~ \\text{ where } ~~ \\hat{\\sigma} := \\sqrt{\\frac{\\sum_{i=1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - \\dots - \\hat{\\beta}_m x_{im} \\right)^2}{n-m-1}}\n\\end{align*}\nThe standard errors are given by the square roots of the diagonal entries of the $(m+1) \\times (m+1)$ matrix  $\\hat{\\sigma}^2 (X^T X)^{-1}$.\n\nLooking back at the statsmodels regression summary, it is common practice to judge the importance of each variable by the value of the estimated regression coefficient as well as the corresponding standard error. The third column (the column titled **t**) of the regression output gives the ratio of the estimate of each regression coefficient to the corresponding standard error. Variables for which the magnitude of this ratio is large are considered important. For example, in this example, the variable \"kidslt6\" has a **t** value of -7.479 i.e., 7.479  in magnitude which is the largest of all other variables so this can be considered the most important variable among all the covariates. On the other hand, the variable \"fatheduc\" has **t** value equal to -0.412 (i.e., 0.412 in absolute value) which is small so this can be considered the least important variable. The right thing to compare these **t**-values are the quantiles of the Student **t** probability distribution with $n-m-1$ degrees of freedom. This comparison is done by the OLS function and its values are given in the fourth column of the regression table. This column (titled $P>|t|$) goes by the same 'p-values' and values close to zero indicate that the variable is important. For example, for 'kidslt6', this $p$-value is rounded to zero while for 'fathedu', this $p$-value is quit large equalling 0.681. \n\nUsually one which looks at the table above and drops variables for which the $p$-value $P > |t|$ is large. In this problem, it might be reasonable to drop the variables \"motheduc\", \"fatheduc\", \"hushrs\", \"huseduc\" and \"kidsge6\" deeming these to be unimportant for explaining the response \"hours\". Below we carry out linear regression on \"hours\" with the other six variables \"kidslt6\", \"age\", \"educ\", \"huswage\", \"exper\", \"expersq\". ","metadata":{}},{"id":"3d080f95-1965-419e-9c19-db256a14b69c","cell_type":"code","source":"Y = mroz['hours']\nX = mroz[['kidslt6', 'age', 'educ', \n        'huswage', 'exper', 'expersq']].copy()\nX = sm.add_constant(X) #add a constant (intercept) to the model\n#Fit the model: \nlinmodel = sm.OLS(Y, X).fit()\nprint(linmodel.summary())","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  hours   R-squared:                       0.266\nModel:                            OLS   Adj. R-squared:                  0.260\nMethod:                 Least Squares   F-statistic:                     44.99\nDate:                Sat, 30 Sep 2023   Prob (F-statistic):           4.67e-47\nTime:                        03:19:00   Log-Likelihood:                -6049.5\nNo. Observations:                 753   AIC:                         1.211e+04\nDf Residuals:                     746   BIC:                         1.215e+04\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       1166.8778    243.738      4.787      0.000     688.384    1645.372\nkidslt6     -433.1229     58.416     -7.414      0.000    -547.803    -318.443\nage          -28.4308      4.067     -6.991      0.000     -36.414     -20.447\neduc          32.6255     12.827      2.543      0.011       7.444      57.807\nhuswage      -13.9353      6.857     -2.032      0.042     -27.397      -0.474\nexper         67.7980      9.896      6.851      0.000      48.371      87.225\nexpersq       -0.7375      0.325     -2.270      0.023      -1.375      -0.100\n==============================================================================\nOmnibus:                       78.707   Durbin-Watson:                   1.383\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              111.647\nSkew:                           0.768   Prob(JB):                     5.70e-25\nKurtosis:                       4.095   Cond. No.                     2.76e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.76e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n","output_type":"stream"}],"execution_count":14},{"id":"6a1bd501-93ac-4635-ae5f-4c56b7b1ff6f","cell_type":"markdown","source":"Instead of using the package StatsModels (specifically the function **sm.OLS**), we can also fit this linear regression model via the Bayesian approach (in PyMC) with flat priors on the regression coefficients (as well as $\\log \\sigma$). This is done as follows. Note that with these priors, it can be mathematically proved (by calculating the posterior explicitly) that the Bayesian approach gives identical results as the frequentist approach. This means that implementation in PyMC should give essentially the same results as found in the sm.OLS summary (with perhaps slight differences due to Monte Carlo fluctuations).","metadata":{}},{"id":"c0625339-cbba-4c9a-bfca-ccf1c90b3024","cell_type":"code","source":"#We can also take the Bayesian Approach and use PyMC:\nimport pymc as pm\nmrozmod = pm.Model()\nwith mrozmod:\n    # Priors for unknown model parameters\n    b0 = pm.Flat(\"b0\")\n    b1 = pm.Flat(\"b1\")\n    b2 = pm.Flat(\"b2\")\n    b3 = pm.Flat(\"b3\")\n    b4 = pm.Flat(\"b4\")\n    b5 = pm.Flat(\"b5\")\n    b6 = pm.Flat(\"b6\")\n    log_sigma = pm.Flat(\"log_sigma\")             \n    sigma = pm.Deterministic(\"sigma\", pm.math.exp(log_sigma))\n    # Expected value of outcome\n    mu = b0 + b1 * mroz['kidslt6'] +   b2 * mroz['age'] + b3 * mroz['educ'] + b4 * mroz['huswage'] + b5 * mroz['exper'] + b6 * mroz['expersq']\n    # Likelihood\n    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=mroz['hours'])\n    idata = pm.sample(2000, chains = 2, return_inferencedata = True)  ","metadata":{"trusted":true},"outputs":[{"name":"stderr","text":"Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 4 jobs)\nNUTS: [b0, b1, b2, b3, b4, b5, b6, log_sigma]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [6000/6000 00:30&lt;00:00 Sampling 2 chains, 0 divergences]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 31 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n","output_type":"stream"}],"execution_count":15},{"id":"6dc8d466-3612-4245-84be-c00698fbe3b4","cell_type":"markdown","source":"The above PyMC code specifies the linear regression model in the following way:\n\\begin{align*}\n   &Y_i \\sim N(\\mu_i, \\sigma^2) ~~ \\text{ for } i = 1, \\dots, n \\\\\n   &\\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_m x_{im} ~~ \\text{ for } i = 1, \\dots, n \\\\\n   &\\beta_0, \\beta_1, \\dots, \\beta_m, \\log \\sigma \\overset{\\text{i.i.d}}{\\sim} \\text{Flat} = \\text{Uniform}[-C, C] \\text{ for very large } C\n\\end{align*}\nLet us check that the Bayesian analysis leads to basically the same answers as statsmodels OLS.","metadata":{}},{"id":"360148c8-d39d-48e6-bf73-8d9670193339","cell_type":"code","source":"b0_samples = idata.posterior['b0'].values.flatten()\nb1_samples = idata.posterior['b1'].values.flatten()\nb2_samples = idata.posterior['b2'].values.flatten()\nb3_samples = idata.posterior['b3'].values.flatten()\nb4_samples = idata.posterior['b4'].values.flatten()\nb5_samples = idata.posterior['b5'].values.flatten()\nb6_samples = idata.posterior['b6'].values.flatten()\n\nallsamples = [b0_samples, b1_samples, b2_samples, b3_samples, b4_samples, b5_samples, b6_samples]\nnames = ['b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6']\nprint(\"Parameter   | Mean     | Std. Dev. | Least Squares | Std. Error\")\nprint(\"------------|----------|----------\")\nfor i, (name, arr) in enumerate(zip(names, allsamples)):\n    print(f\"{name:10}| {np.mean(arr):.6f} | {np.std(arr):.6f} | {linmodel.params.values[i]:.6f}  | {linmodel.bse.values[i]:.6f}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Parameter   | Mean     | Std. Dev. | Least Squares | Std. Error\n------------|----------|----------\nb0        | 1175.858222 | 242.523307 | 1166.877797  | 243.737876\nb1        | -434.363992 | 59.045116 | -433.122873  | 58.416292\nb2        | -28.572263 | 4.043531 | -28.430794  | 4.066762\nb3        | 32.440296 | 13.042272 | 32.625466  | 12.827177\nb4        | -13.836047 | 7.102644 | -13.935253  | 6.857217\nb5        | 67.582198 | 9.766088 | 67.797967  | 9.895837\nb6        | -0.727326 | 0.320257 | -0.737492  | 0.324827\n","output_type":"stream"}],"execution_count":16},{"id":"22fd40cf-2eb5-4bf7-905d-1b079d5a8425","cell_type":"markdown","source":"The estimated regression coefficent $\\hat{\\beta}_1$ for \"kidslt6\" is about -430 which can be interpreted as follows: having a small child reduces the mean hours worked by about 430. This result is clearly meaningless for women with small working hours (e.g., if someone only works for about 300 hours, what does a reduction of 430 mean?). It is also not very meaningful for women with lots of working hours (e.g., if someone works for  more than 3000 hours, a reduction of 430 due to a small kid sounds too small). A much more meaningful and useful interpretation would report reductions in terms of percentages and not absolute hours. For example, a statement such as \"having a small child reduces the mean hours worked by about 40%\" would apply to all working women and be much more interpretable. Such an intepretation cannot be drawn based on this linear regression. \n\nA percentage interpretation can be realized if we change our model by taking the response variable to be $\\log(\\text{Hours worked})$ as opposed to \"hours\". Indeed, in the linear regression model for $\\log(\\text{Hours})$: \n\\begin{align*}\n   \\log(\\text{Hours}) = \\beta_0 + \\beta_1 \\text{kidslt6} + \\beta_2 \\text{age} + \\dots, \n\\end{align*}\nthe coefficient $\\beta_1$ gives the reduction in $\\log(\\text{Hours})$ due to a small child. This implies a percentage increase of $100 \\times(\\exp(\\beta_1) - 1)$ in hours worked because: \n\\begin{align*}\n   \\frac{\\text{Hours}_{\\text{new}} - \\text{Hours}_{\\text{old}}}{\\text{Hours}_{\\text{old}}} \\times 100 = \\left[\\exp \\left(\\log \\text{Hours}_{\\text{new}} - \\log \\text{Hours}_{\\text{old}} \\right) - 1\\right] \\times 100\n\\end{align*}\nIt is much more preferable to use $\\log(\\text{Hours})$ as the response variable in these problems compared to $\\text{Hours}$. However, one big problem is that, in this dataset, there are quite a few observations for which the Hours variable equals 0. So we cannot really work with the variable $\\log \\text{Hours}$. In such case, Poisson regression provides a great alternative model. ","metadata":{}},{"id":"16e76a31-47c9-44b9-a037-6387504e8769","cell_type":"markdown","source":"## Poisson Regression\n\nIn Poisson regression, one modifies the linear regression model:\n\\begin{align*}\n   &\\log(Y_i) \\sim N(\\mu_i, \\sigma^2) ~~ \\text{ for } i = 1, \\dots, n \\\\\n   &\\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_m x_{im} ~~ \\text{ for } i = 1, \\dots, n\n\\end{align*}\n(which cannot be used as the variable $\\log(Y_i)$ is meaningless when $Y_i = 0$) in the following way:\n\\begin{align*}\n   &Y_i \\sim \\text{Poisson}(\\mu_i) ~~ \\text{ for } i = 1, \\dots, n \\\\\n   &\\log \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_m x_{im} ~~ \\text{ for } i = 1, \\dots, n \n\\end{align*}\nThe main change is that the response variable $Y_i$ is now modelled as $\\text{Poisson}(\\mu_i)$. The Poisson distribution is often used to model counts. In this case, $Y_i$ is the number of hours worked in 1975 which is a count variable. The linear relationship between the response and covariates is specified through $\\log \\mu_i$ which enables percentage interpretation for the regression coefficients. \n\nAs in the case of linear regression, there are two ways of implementing Poisson regression:\n\n1. Through an in-built function in the library statsmodels. This function is called **sm.GLM**. GLM stands for Generalized Linear Models which is a general class of models which includes linear regression and Poisson regression (and also logistic regression). While using sm.GLM, we need to specify which GLM we are fitting using the **family** argument.\n2. Using PyMC to do Bayesian inference with flat priors: $\\beta_0, \\dots, \\beta_m \\overset{\\text{i.i.d}}{\\sim} \\text{Flat}$.\n\nMost practitioners use the first option above. Let us first see how it works before turning to the Bayesian PyMC approach.","metadata":{}},{"id":"cdc1b16e-0c20-469f-ba38-73fa09bd8934","cell_type":"code","source":"#Poisson Regression through StatsModels\n# Define the response variable and covariates\nY = mroz['hours']\nX = mroz[['kidslt6', 'age', 'educ', \n        'huswage', 'exper', 'expersq']].copy()\nX = sm.add_constant(X) # Add a constant (intercept) to the model\n# Fit the Poisson regression model\npoiregmodel = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\nprint(poiregmodel.summary())","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                  hours   No. Observations:                  753\nModel:                            GLM   Df Residuals:                      746\nModel Family:                 Poisson   Df Model:                            6\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -3.1563e+05\nDate:                Sat, 30 Sep 2023   Deviance:                   6.2754e+05\nTime:                        03:19:46   Pearson chi2:                 6.60e+05\nNo. Iterations:                     5   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          6.9365      0.012    562.281      0.000       6.912       6.961\nkidslt6       -0.8075      0.004   -193.217      0.000      -0.816      -0.799\nage           -0.0427      0.000   -201.166      0.000      -0.043      -0.042\neduc           0.0528      0.001     83.439      0.000       0.052       0.054\nhuswage       -0.0207      0.000    -54.548      0.000      -0.021      -0.020\nexper          0.1204      0.001    219.231      0.000       0.119       0.121\nexpersq       -0.0018   1.63e-05   -112.090      0.000      -0.002      -0.002\n==============================================================================\n","output_type":"stream"}],"execution_count":17},{"id":"dc2baa08-fc28-41cf-9fe9-7fe29d08a388","cell_type":"markdown","source":"The output above looks very similar to the linear regression output. This model is much more interpretable compared to the linear regression  model that we fit earlier. To see this, consider the interpretation of the coefficient -0.8075 for the \"kidslt6\" variable: having a small kid reduces mean hours worked by 55%. This is clearly a much more interpretable result compared to before. ","metadata":{}},{"id":"8638fb89-cbca-43f5-8c02-a3bddbdc615c","cell_type":"code","source":"#56% comes from:\nprint((np.exp(poiregmodel.params['kidslt6']) - 1)*100)","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"-55.40391074218227\n","output_type":"stream"}],"execution_count":18},{"id":"2117054d-4dcc-46c6-b9c8-94ee2ea38bd8","cell_type":"markdown","source":"### Maximum Likelihood Estimation in Poisson Regression\n\nLet us now understand how sm.GLM is obtaining the estimates and standard errors for the regression coefficients. The estimates are obtained by Maximum Likelihood Estimation. The likelihood in this Poisson Regression Model is given by\n\\begin{align*}\n  \\text{likelihood} &= \\prod_{i=1}^n \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i!} \\\\\n  &= \\prod_{i=1}^n \\frac{e^{-e^{\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_m x_{im}}} e^{y_i(\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_m x_{im})}}{y_i!} \n\\end{align*}\nIt is much more easier to maximize the logarithm of the likelihood (as opposed to the likelihood directly). We can also ignore the $y_i!$ terms in the denominator as they do not involve the parameters $\\beta_0, \\dots, \\beta_m$. Let us also replace the sum $\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_m x_{im}$ by the simpler expression $x_i^T \\beta$ where $\\beta$ denotes the column vector with components $\\beta_0, \\dots, \\beta_m$ and $x_i$ denotes the column vector with components $1, x_{i1}, x_{i2}, \\dots, x_{im}$. With these changes, we obtain the log-likelihood:\n\\begin{align*}\n    \\ell(\\beta) = \\sum_{i=1}^n \\left(-e^{x_i^T \\beta} + y_i x_i^T \\beta \\right)\n\\end{align*}\nThe goal is to maximize this log-likelihood over $\\beta_0, \\beta_1, \\dots, \\beta_m$. We can do this by taking the gradient (i.e., derivatives of $\\ell(\\beta)$ with respect to $\\beta_0, \\dots, \\beta_m$ collected in a column vector), setting the gradient to zero and solving the resulting set of equations. The gradient of $\\ell(\\beta)$ is\n\\begin{align*}\n   \\nabla \\ell(\\beta) = \\sum_{i=1}^n \\left(-e^{x_i^T \\beta} x_i + y_i x_i \\right) = \\sum_{i=1}^n \\left(y_i - e^{x_i^T \\beta} \\right) x_i =  \\sum_{i=1}^n \\left(y_i - \\mu_i \\right) x_i = X^T(Y - \\mu).\n\\end{align*}\nwhere $\\mu$ is the $n \\times 1$ vector with entries $\\mu_1, \\dots, \\mu_n$. Therefore setting the gradient equal to zero leads us to the equation:\n\\begin{align*}\n    X^T \\mu = X^T Y. \n\\end{align*}\nThis gives $m+1$ equations for the $m+1$ parameters $\\beta_0, \\dots, \\beta_m$. Note that the parameters appear in the above equation through $\\mu$. It is interesting to observe that in the case of linear regression (where we have the normal distribution instead of Poisson), if we attempt to calculate the MLE in the same way as above, we would obtain exactly the same equation: $X^T \\mu = X^T Y$. However, in that case, $\\mu = X \\beta$ leading to the equation $X^T X \\beta = X^T Y$ which can be solved as $\\hat{\\beta} = (X^T X)^{-1} X^T Y$. Unfortunately, in the present Poisson regression case, $\\mu$, which has components $e^{x_1^T \\beta}, \\dots, e^{x_n^T \\beta}$, depends on $\\beta$ in a nonlinear fashion and, consequently, it is not possible to solve $X^T \\mu = X^T Y$ for $\\beta$ in closed form. One needs to resort to solving this equation using numerical methods. ","metadata":{}},{"id":"27bc2a9e-376e-466e-869e-9548ae58ea1c","cell_type":"markdown","source":"### MLE via Newton's Method\n\nA very classical method for solving nonlinear equations is the Newton method (also known as the Newton-Raphson method): see https://en.wikipedia.org/wiki/Newton%27s_method. We apply this method to solve $\\nabla \\ell(\\beta) = X^T Y - X^T \\mu = 0$. Newton's method is iterative (starts with a rough guess $\\beta^{(0)}$ for the solution and then successively modifies into better approximations for the solution eventually converging to the correct solution). Newton's interation for changing the current solution approximation $\\beta^{(m)}$ into a better approximation $\\beta^{(m+1)}$ of the solution to $\\nabla \\ell(\\beta) = 0$ is given by: \n\\begin{align*}\n   \\beta^{(m+1)} = \\beta^{(m)} - (H\\ell(\\beta^{(m)}))^{-1} \\nabla \\ell(\\beta^{(m)}),\n\\end{align*}\nwhere $H \\ell(\\beta^{(m)}$ is the Hessian Matrix of the function $\\ell(\\beta)$ evaluated at the point $\\beta = \\beta^{(m)}$. It may be recalled the Hessian matrix is the matrix of second derivatives (see https://en.wikipedia.org/wiki/Hessian_matrix). To actually, implement Newton's method, we would need to calculate the Hessian. We can do this by further differentiating the gradient $\\nabla \\ell(\\beta) = \\sum_{i=1}^n (y_i - e^{x_i^T \\beta}) x_i = X^T(Y - \\mu)$ with respect to $\\beta$. It can be checked that this leads to:\n\\begin{align*}\n   H \\ell(\\beta) = - \\sum_{i=1}^n e^{x_i^T \\beta} x_i x_i^T = - \\sum_{i=1}^n \\mu_i x_i x_i^T.\n\\end{align*}\nWe can rewrite this in matrix form as \n\\begin{align*}\n   H \\ell(\\beta) = - X^T M(\\beta) X \n\\end{align*}\nwhere $M(\\beta)$ is the diagonal matrix with diagonal entries $\\mu_1 = e^{x_1^T \\beta}, \\dots, \\mu_n = e^{x_n^T \\beta}$. Thus Newton's method for Poisson regression is:\n\\begin{align*}\n  \\beta^{(m+1)} = \\beta^{(m)} + \\left(X^T M(\\beta^{(m)}) X \\right)^{-1} \\left( X^T Y - X^T \\mu(\\beta^{(m)}) \\right), \n\\end{align*}\nwhere we wrote $\\mu(\\beta^{(m)})$ to emphasize that this vector $\\mu$ is calculated as $e^{x_1^T \\beta}, \\dots, e^{x_n^T \\beta}$ with $\\beta = \\beta^{(m)}$. This method can be implemented as follows. ","metadata":{}},{"id":"5eb2acc2-568a-4367-a002-edcba1df3d6c","cell_type":"code","source":"#Newton's Method for Calculating MLE in Poisson Regression\nbeta_hat = poiregmodel.params.values \nprint(beta_hat)\n#this is the answer computed by statsmodels and we shall show that Newton's method leads to the same answer if initialized reasonably\n\n#Initialization for Newton's Method\nm = 6\np = 7\nbeta_initial = [5, 0, 0, 0, 0, 0, 0]\nn = mroz.shape[0]\nXmat = X.values\nYvec = mroz['hours'].values\n\n#Newton's method for 100 iterations\nnum_iterations = 100\nfor i in range(num_iterations):\n    log_muvec = np.dot(Xmat, beta_initial)\n    muvec = np.exp(log_muvec)\n    gradient = np.dot(Xmat.T,  Yvec - muvec)\n    M = np.diag(muvec)\n    Hessian = -Xmat.T @ M @ Xmat\n    Hessian_inv = np.linalg.inv(Hessian)\n    beta_initial = beta_initial - Hessian_inv @ gradient\n    print(beta_initial)","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[11.862361 -2.918359 -0.191565 0.219829 -0.093895 0.456819 -0.004969]\n[10.885550 -2.863001 -0.189547 0.218271 -0.093150 0.450808 -0.004889]\n[9.946436 -2.723529 -0.184228 0.214137 -0.091177 0.435017 -0.004679]\n[9.097029 -2.412093 -0.170935 0.203596 -0.086180 0.395908 -0.004161]\n[8.414084 -1.884261 -0.142079 0.179445 -0.074896 0.313363 -0.003085]\n[7.865945 -1.335615 -0.097770 0.136900 -0.055375 0.198124 -0.001679]\n[7.313642 -0.990725 -0.061477 0.090536 -0.034424 0.127770 -0.001108]\n[7.011093 -0.838093 -0.045790 0.061125 -0.022797 0.113816 -0.001399]\n[6.944282 -0.808540 -0.042825 0.053242 -0.020705 0.118791 -0.001758]\n[6.936650 -0.807526 -0.042682 0.052832 -0.020712 0.120342 -0.001827]\n[6.936480 -0.807524 -0.042681 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n[6.936480 -0.807524 -0.042680 0.052831 -0.020714 0.120372 -0.001829]\n","output_type":"stream"}],"execution_count":19},{"id":"4a792426-e045-4a2c-8ffc-48675869d136","cell_type":"markdown","source":"Because of the exponential calculation in $\\mu_i = e^{x_i^T \\beta}$, one needs to be careful with initialization. For example, go back and change the initialization to $\\beta^{(0)} = (0, \\dots, 0)$. In this case, $\\beta^{(1)}$ has some large entries leading to $e^{x_i^T \\beta^{(1)}$ blowing up. If this can be avoided, the method converges to the correct solution. ","metadata":{}},{"id":"b2a0c082-2198-40fb-ba9e-50a4d0ed82f2","cell_type":"markdown","source":"### Bayesian Analysis with Flat Prior\n\nBefore seeing how sm.GLM is calculating the standard errors, let us first look at the Bayesian approach for fitting the Poisson regression model with flat priors. For the Bayesian approach with flat priors, we have\n\\begin{align*}\n   \\text{Prior} = 1  ~~~ \\text{ and } ~~~ \\text{Likelihood} = \\prod_{i=1}^n \\frac{e^{-e^{\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_m x_{im}}} e^{y_i(\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_m x_{im})}}{y_i!} \\propto \\exp(\\ell(\\beta)).\n\\end{align*}\nThe posterior is therefore given by\n\\begin{align*}\n   \\text{Posterior}(\\beta) \\propto \\exp \\left(\\ell(\\beta) \\right) = \\exp \\left(\\sum_{i=1}^n \\left(-e^{x_i^T \\beta} + y_i x_i^T \\beta \\right) \\right)\n\\end{align*}\nThis posterior in $\\beta_0, \\dots, \\beta_m$ cannot be written in terms of standard distributions. One can use PyMC to generate samples from this posterior. However, PyMC can be somewhat unstable and might lead to (significantly different) answers for different runs. For example, in the following code, setting randomseed = 0 gives results similar to the frequentist output while setting randomseed = 4 gives quite different results.","metadata":{}},{"id":"252c9c3d-28a5-4bd1-a8d7-73006167a252","cell_type":"code","source":"#We can also take the Bayesian Approach and use PyMC:\nimport pymc as pm\nmrozpoimod = pm.Model()\nwith mrozpoimod:\n    # Priors for unknown model parameters\n    b0 = pm.Flat(\"b0\")\n    b1 = pm.Flat(\"b1\")\n    b2 = pm.Flat(\"b2\")\n    b3 = pm.Flat(\"b3\")\n    b4 = pm.Flat(\"b4\")\n    b5 = pm.Flat(\"b5\")\n    b6 = pm.Flat(\"b6\")\n    log_mu = b0 + b1 * mroz['kidslt6'] +   b2 * mroz['age'] + b3 * mroz['educ'] + b4 * mroz['huswage'] + b5 * mroz['exper'] + b6 * mroz['expersq']\n    # Likelihood\n    Y_obs = pm.Poisson(\"Y_obs\", mu=np.exp(log_mu), observed=mroz['hours'])\n    idata = pm.sample(2000, chains = 2, random_seed = 0, return_inferencedata = True)  ","metadata":{"trusted":true},"outputs":[{"name":"stderr","text":"Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 4 jobs)\nNUTS: [b0, b1, b2, b3, b4, b5, b6]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [6000/6000 03:04&lt;00:00 Sampling 2 chains, 0 divergences]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 185 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n","output_type":"stream"}],"execution_count":20},{"id":"9963e738-e032-415e-8024-59799dc9f54b","cell_type":"code","source":"b0_samples = idata.posterior['b0'].values.flatten()\nb1_samples = idata.posterior['b1'].values.flatten()\nb2_samples = idata.posterior['b2'].values.flatten()\nb3_samples = idata.posterior['b3'].values.flatten()\nb4_samples = idata.posterior['b4'].values.flatten()\nb5_samples = idata.posterior['b5'].values.flatten()\nb6_samples = idata.posterior['b6'].values.flatten()\n\nallsamples = [b0_samples, b1_samples, b2_samples, b3_samples, b4_samples, b5_samples, b6_samples]\nnames = ['b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6']\nprint(\"Parameter | Estimate | Std. Dev. | Frequentist | Std. Error\")\nprint(\"------------|----------|----------\")\nfor i, (name, arr) in enumerate(zip(names, allsamples)):\n    print(f\"{name:8}| {np.mean(arr):.6f} | {np.std(arr):.6f}  | {poiregmodel.params.values[i]:.6f}  | {poiregmodel.bse.values[i]:.6f}\")\n#These results are quite close to the frequentist output.\n#However PyMC is not very reliable here. Change the random seed from 0 to 4\n#and look at the results. ","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Parameter | Estimate | Std. Dev. | Frequentist | Std. Error\n------------|----------|----------\nb0      | 6.936476 | 0.012442  | 6.936480  | 0.012336\nb1      | -0.807527 | 0.004131  | -0.807524  | 0.004179\nb2      | -0.042681 | 0.000213  | -0.042680  | 0.000212\nb3      | 0.052838 | 0.000637  | 0.052831  | 0.000633\nb4      | -0.020714 | 0.000378  | -0.020714  | 0.000380\nb5      | 0.120361 | 0.000549  | 0.120372  | 0.000549\nb6      | -0.001828 | 0.000016  | -0.001829  | 0.000016\n","output_type":"stream"}],"execution_count":21},{"id":"f63ffee2-0e17-41c3-88d1-0354569d4310","cell_type":"markdown","source":"### Posterior Normal Approximation and Standard Errors\n\nWhen the posterior cannot be written in terms of standard distributions, and when MCMC seems unreliable, an alternative option is to **approximate** the posterior using simple standard distributions. This idea is at the heart of the method of **Variational Inference** which is quite popular in Bayesian inference. In this problem, one can approximate the posterior via a (multivariate) normal distribution. This works in the following way.\n\nOur posterior is proportional to $\\exp(\\ell(\\beta))$. The normal distribution, on the other hand, is proportional to $\\exp \\left(\\text{quadratic function of } \\beta \\right)$. Thus, in order to obtain a normal approximation of the posterior, it makes sense to approximate $\\ell(\\beta)$ by a quadratic function of $\\beta$. This can be done by Taylor approximation but we need to figure out around which point does the Taylor approximation need to be done. The function $\\ell(\\beta)$ is the log-likelihood which is  maximized at the MLE $\\hat{\\beta}$. It makes sense, therefore, to do the Taylor approximation around $\\hat{\\beta}$. The idea is that the approximation will be quite accurate for points near the MLE $\\hat{\\beta}$. For points far from the MLE, the posterior density is probably small anyway so bad approximation for these points might have an insignificant effect. Motivated by these considerations, we Taylor expand the log-likelihood $\\ell(\\beta)$ around $\\hat{\\beta}$ up to second order as follows:\n\\begin{align*}\n    \\ell(\\beta) &\\approx \\ell(\\hat{\\beta}) + \\left<\\nabla \\ell(\\hat{\\beta}), \\beta - \\hat{\\beta} \\right> + \\frac{1}{2} \\left(\\beta - \\hat{\\beta} \\right)^T H\\ell(\\hat{\\beta}) \\left(\\beta - \\hat{\\beta} \\right) \\\\\n    &= \\ell(\\hat{\\beta}) + \\frac{1}{2} \\left(\\beta - \\hat{\\beta} \\right)^T H\\ell(\\hat{\\beta}) \\left(\\beta - \\hat{\\beta} \\right) ~~ \\text{ because } \\nabla \\ell(\\hat{\\beta}) = 0.\n\\end{align*}\nThus the posterior is approximated as:\n\\begin{align*}\n   \\text{posterior}(\\beta) & \\propto \\exp \\left(\\ell(\\beta) \\right) \\\\\n   & \\approx \\exp \\left(\\ell(\\hat{\\beta}) + \\frac{1}{2} \\left(\\beta - \\hat{\\beta} \\right)^T H\\ell(\\hat{\\beta}) \\left(\\beta - \\hat{\\beta} \\right) \\right) \\\\\n   &=  \\exp \\left(\\ell(\\hat{\\beta}) \\right) \\exp \\left(\\frac{1}{2} \\left(\\beta - \\hat{\\beta} \\right)^T H\\ell(\\hat{\\beta}) \\left(\\beta - \\hat{\\beta} \\right) \\right) \\\\\n   &\\propto \\exp \\left(\\frac{1}{2} \\left(\\beta - \\hat{\\beta} \\right)^T H\\ell(\\hat{\\beta}) \\left(\\beta - \\hat{\\beta} \\right) \\right). \n\\end{align*}\nIn the last step above, we ignored $\\exp \\left(\\ell(\\hat{\\beta}) \\right)$ in proportionality because it does not involve the parameters $\\beta$. \n\nNow we can compare the posterior approximation to the density of a multivariate normal (with  mean $\\mu$ and covariance $\\Sigma$) which is proportional to $\\exp \\left(- (\\beta - \\mu)^T \\Sigma^{-1} (\\beta - \\mu) \\right)$. It is then clear that $\\mu = \\hat{\\beta}$ and $\\Sigma^{-1} = -H\\ell(\\hat{\\beta})$. Thus the posterior normal approximation is given by:\n\\begin{align*}\n   \\text{posterior} \\approx N \\left(\\hat{\\beta}, \\left( - H\\ell(\\hat{\\beta}) \\right)^{-1}\\right)\n\\end{align*}\nThis gives a very simple and practical approximation to the complicated posterior, especially because we already know how to calculate the Hessian (it was used in the Newton algorithm for computing the MLE). If one wants, posterior samples can be easily generated from $N \\left(\\hat{\\beta}, \\left( - H\\ell(\\hat{\\beta}) \\right)^{-1}\\right)$. This will be much faster than using PyMC. However, PyMC tries to obtain samples from the actual posterior while this normal distribution is an approximation to the actual posterior. ","metadata":{}},{"id":"54992162-d3a7-4533-bfea-f11a2677523c","cell_type":"markdown","source":"### Standard Errors for Poisson Regression\n\nThe standard errors of Poisson Regression that were reported by sm.GLM are actually computed from the covariance of the posterior normal approximation. In other words, the standard errors are simply the square-roots of the diagonal entries of the matrix: \n\\begin{align*}\n   \\left(-H\\ell(\\hat{\\beta}) \\right)^{-1} = \\left(X^T M(\\hat{\\beta}) X \\right)^{-1}\n\\end{align*}\nwhere $\\hat{\\beta}$ is the MLE and $M(\\beta)$ is the diagonal matrix with diagonal entries $\\mu_1 = e^{x_1^T \\beta}, \\dots, \\mu_n = e^{x_n^T \\beta}$. This can be readily verified in the context of our dataset as follows.","metadata":{}},{"id":"c9aea7c4-f1ed-4f78-af54-3b26a9d11f1b","cell_type":"code","source":"#Standard Error Calculation:\nlog_muvec = np.dot(Xmat, beta_hat)\nmuvec = np.exp(log_muvec)\nM = np.diag(muvec)\nHessian = -Xmat.T @ M @ Xmat\nHessian_inv = np.linalg.inv(Hessian)\nCovMat = -Hessian_inv\nprint(np.sqrt(np.diag(CovMat)))\n#Compare with\nprint(poiregmodel.bse)","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"[0.012336 0.004179 0.000212 0.000633 0.000380 0.000549 0.000016]\nconst      0.012336\nkidslt6    0.004179\nage        0.000212\neduc       0.000633\nhuswage    0.000380\nexper      0.000549\nexpersq    0.000016\ndtype: float64\n","output_type":"stream"}],"execution_count":22},{"id":"5854e4ac-b280-4fd8-87cd-f3bf9a271893","cell_type":"markdown","source":"These standard errors can also be justified using frequentist arguments but that justification is much more complicated (it involves asymptotics and notions such as Fisher information). The Bayesian justification is much simpler and intuitive (based on normal approximation via Taylor expansion). \n\nThe important takeaways are: \n\n1. Poisson Regression is an important alternative to linear regression which is applicable when the response variable involves counts. It gives more interpretable results than linear regression by  allowing one to do **log-linear** modeling in situations where one cannot directly take logarithms of the response variable due to existence of zero counts.\n2. In the case of linear regression, frequentist inference and Bayesian inference with flat priors coincide exactly as can be rigorously mathematically proved.\n3. For Poisson regression, frequentist inference uses the MLE $\\hat{\\beta}$ and provides standard errors by taking square roots of the diagonal entries of the matrix $\\left(-H\\ell(\\hat{\\beta}) \\right)^{-1}$. Bayesian inference with flat priors can give different answers to frequentist inference in Poisson regression. However, if one implements Bayesian inference by not working with the full posterior and instead approximating the posterior by a multivariate normal distribution centered at the MLE, then Bayesian inference gives identical answers to the frequentist inference. ","metadata":{}}]}