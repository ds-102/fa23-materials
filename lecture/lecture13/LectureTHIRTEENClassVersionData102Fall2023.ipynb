{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "plastic-albert",
   "metadata": {},
   "source": [
    "# Nonparametric methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beneficial-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import plot_tree\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90e35bc-fdbb-4daf-8e3b-965ac07e7280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(753, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inlf</th>\n",
       "      <th>hours</th>\n",
       "      <th>kidslt6</th>\n",
       "      <th>kidsge6</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>wage</th>\n",
       "      <th>repwage</th>\n",
       "      <th>hushrs</th>\n",
       "      <th>husage</th>\n",
       "      <th>...</th>\n",
       "      <th>faminc</th>\n",
       "      <th>mtr</th>\n",
       "      <th>motheduc</th>\n",
       "      <th>fatheduc</th>\n",
       "      <th>unem</th>\n",
       "      <th>city</th>\n",
       "      <th>exper</th>\n",
       "      <th>nwifeinc</th>\n",
       "      <th>lwage</th>\n",
       "      <th>expersq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1610</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>3.3540</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2708</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>16310</td>\n",
       "      <td>0.7215</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10.910060</td>\n",
       "      <td>1.210154</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1656</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.3889</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2310</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>21800</td>\n",
       "      <td>0.6615</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>19.499981</td>\n",
       "      <td>0.328512</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>4.5455</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3072</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>21040</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>12.039910</td>\n",
       "      <td>1.514138</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0965</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1920</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>7300</td>\n",
       "      <td>0.7815</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.799996</td>\n",
       "      <td>0.092123</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1568</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>4.5918</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2000</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>27300</td>\n",
       "      <td>0.6215</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>20.100058</td>\n",
       "      <td>1.524272</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>4.7421</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1040</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>19495</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>9.859054</td>\n",
       "      <td>1.556480</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1440</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>16</td>\n",
       "      <td>8.3333</td>\n",
       "      <td>5.95</td>\n",
       "      <td>2670</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>21152</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9.152048</td>\n",
       "      <td>2.120260</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>7.8431</td>\n",
       "      <td>9.98</td>\n",
       "      <td>4120</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>18900</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>10.900038</td>\n",
       "      <td>2.059634</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1458</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>2.1262</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1995</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>20405</td>\n",
       "      <td>0.7515</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>17.305000</td>\n",
       "      <td>0.754336</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>4.6875</td>\n",
       "      <td>4.15</td>\n",
       "      <td>2100</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>20425</td>\n",
       "      <td>0.6915</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>12.925000</td>\n",
       "      <td>1.544899</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0630</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2450</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>32300</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>24.299953</td>\n",
       "      <td>1.401922</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>4.5918</td>\n",
       "      <td>4.58</td>\n",
       "      <td>2375</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>28700</td>\n",
       "      <td>0.6215</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>19.700071</td>\n",
       "      <td>1.524272</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    inlf  hours  kidslt6  kidsge6  age  educ    wage  repwage  hushrs  husage  \\\n",
       "0      1   1610        1        0   32    12  3.3540     2.65    2708      34   \n",
       "1      1   1656        0        2   30    12  1.3889     2.65    2310      30   \n",
       "2      1   1980        1        3   35    12  4.5455     4.04    3072      40   \n",
       "3      1    456        0        3   34    12  1.0965     3.25    1920      53   \n",
       "4      1   1568        1        2   31    14  4.5918     3.60    2000      32   \n",
       "5      1   2032        0        0   54    12  4.7421     4.70    1040      57   \n",
       "6      1   1440        0        2   37    16  8.3333     5.95    2670      37   \n",
       "7      1   1020        0        0   54    12  7.8431     9.98    4120      53   \n",
       "8      1   1458        0        2   48    12  2.1262     0.00    1995      52   \n",
       "9      1   1600        0        2   39    12  4.6875     4.15    2100      43   \n",
       "10     1   1969        0        1   33    12  4.0630     4.30    2450      34   \n",
       "11     1   1960        0        1   42    11  4.5918     4.58    2375      47   \n",
       "\n",
       "    ...  faminc     mtr  motheduc  fatheduc  unem  city  exper   nwifeinc  \\\n",
       "0   ...   16310  0.7215        12         7   5.0     0     14  10.910060   \n",
       "1   ...   21800  0.6615         7         7  11.0     1      5  19.499981   \n",
       "2   ...   21040  0.6915        12         7   5.0     0     15  12.039910   \n",
       "3   ...    7300  0.7815         7         7   5.0     0      6   6.799996   \n",
       "4   ...   27300  0.6215        12        14   9.5     1      7  20.100058   \n",
       "5   ...   19495  0.6915        14         7   7.5     1     33   9.859054   \n",
       "6   ...   21152  0.6915        14         7   5.0     0     11   9.152048   \n",
       "7   ...   18900  0.6915         3         3   5.0     0     35  10.900038   \n",
       "8   ...   20405  0.7515         7         7   3.0     0     24  17.305000   \n",
       "9   ...   20425  0.6915         7         7   5.0     0     21  12.925000   \n",
       "10  ...   32300  0.5815        12         3   5.0     0     15  24.299953   \n",
       "11  ...   28700  0.6215        14         7   5.0     0     14  19.700071   \n",
       "\n",
       "       lwage  expersq  \n",
       "0   1.210154      196  \n",
       "1   0.328512       25  \n",
       "2   1.514138      225  \n",
       "3   0.092123       36  \n",
       "4   1.524272       49  \n",
       "5   1.556480     1089  \n",
       "6   2.120260      121  \n",
       "7   2.059634     1225  \n",
       "8   0.754336      576  \n",
       "9   1.544899      441  \n",
       "10  1.401922      225  \n",
       "11  1.524272      196  \n",
       "\n",
       "[12 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression on the mroz dataset\n",
    "mroz = pd.read_csv(\"MROZ.csv\")\n",
    "print(mroz.shape)\n",
    "mroz.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd119ff3-07ba-4805-a73f-2d82669fba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mroz['inlf'] #this is a binary variable\n",
    "X = mroz[['kidslt6', 'age', 'educ', \n",
    "        'huswage', 'exper', 'expersq']].copy()\n",
    "X = sm.add_constant(X)\n",
    "logimodel = sm.GLM(Y, X, family=sm.families.Binomial()).fit()\n",
    "print(logimodel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98841308-9e20-4ea3-a53b-6992f34b3525",
   "metadata": {},
   "source": [
    "What is the interpretation of the coefficient -1.4516 of the 'kidslt6' variable? It means that having a small kid reduces the log-odds of working by -1.4516. This means that the odds are reduced by a factor of $\\exp(-1.4516) \\approx 0.234$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153227c-e0d8-4c7d-ba7c-4cf28d4fcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We used statsmodels.api for fitting GLMs. Today we shall switch to scikit-learn because it also fits nonparametric models such as decision trees and random forests\n",
    "#Let us re-fit this logistic regression model using scikit-learn\n",
    "Y = mroz['inlf']  # This is a binary variable\n",
    "X = mroz[['kidslt6', 'age', 'educ', 'huswage', 'exper', 'expersq']].copy()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logimodel_sk = LogisticRegression(max_iter = 1000)\n",
    "logimodel_sk.fit(X, Y)\n",
    "print(\"Intercept:\", logimodel_sk.intercept_)\n",
    "print(\"Coefficients:\", logimodel_sk.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985dc605-630e-48eb-bf8f-35cebf376896",
   "metadata": {},
   "source": [
    "These coefficients are slightly different from those given by statsmodels.api because scikit-learn does not fit the MLE exactly but also imposes some L2 regularization (similar to ridge regression). The usual MLE can be obtained by setting penalty = None as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9a101-fa8b-40d7-968a-d2d570423704",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mroz['inlf']  # This is a binary variable\n",
    "X = mroz[['kidslt6', 'age', 'educ', 'huswage', 'exper', 'expersq']].copy()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logimodel_sk = LogisticRegression(penalty=None, max_iter = 1000)\n",
    "logimodel_sk.fit(X, Y)\n",
    "print(\"Intercept:\", logimodel_sk.intercept_)\n",
    "print(\"Coefficients:\", logimodel_sk.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1191f-3ebf-4bc9-aba5-f3b3a75e5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of the logistic regression model:\n",
    "#Split the original data into training and test datasets\n",
    "#We will fit the model on the training dataset and test its accuracy on the test dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=503, test_size=250, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca074c9c-6a13-4b60-b198-506c483391a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us fit logistic regression on the training dataset:\n",
    "logimodel_train = LogisticRegression(penalty = None, max_iter = 1000)\n",
    "logimodel_train.fit(X_train, Y_train)\n",
    "print(\"Intercept:\", logimodel_train.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8efa38-1587-4913-83a1-e4354444baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction for the test data\n",
    "Y_pred = logimodel_train.predict(X_test)\n",
    "print(Y_pred)\n",
    "accuracy_logimodel = np.mean(Y_test == Y_pred)\n",
    "print(f\"Accuracy of Logistic Regression on test set: {accuracy_logimodel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635a9c8-bbd8-4b11-9c5c-7fa9f7212e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(Y_test, Y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad2d5e-49b5-4211-a997-5ca3089167fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using k-NN classification:\n",
    "k_value = 8\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k_value)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn = np.mean(Y_test == Y_pred_knn)\n",
    "print(f\"Accuracy of k-NN Classification on test set: {accuracy_knn}\")\n",
    "#Contingency table (or confusion matrix)\n",
    "print(pd.crosstab(Y_test, Y_pred_knn, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd2455-e165-475b-a0d8-b0c8643bf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_dt = np.mean(Y_test == Y_pred_dt)\n",
    "print(f\"Accuracy of Decision Tree Classification on test set: {accuracy_dt}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_dt, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc987d8-7a0a-4b6c-896e-717c765e7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, feature_names=X.columns,class_names = ('0','1'),  filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1f197-5a86-4034-b008-7c2201e92dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tree is way too big to allow any meaningful interpretation\n",
    "#Here are a few ways to create smaller trees:\n",
    "dt_small = DecisionTreeClassifier(max_depth = 3, random_state = 1)\n",
    "dt_small.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_dt_small = dt_small.predict(X_test)\n",
    "# Calculate the accuracy\n",
    "accuracy_dt_small = np.mean(Y_test == Y_pred_dt_small)\n",
    "print(f\"Accuracy of Decision Tree Classification on test set: {accuracy_dt_small}\")\n",
    "\n",
    "# Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_small, feature_names=X.columns, class_names = ('0','1'),  filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14da5d-a95e-4b48-9b64-9a7efdc1e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=1, max_features = None)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = np.mean(Y_test == Y_pred_rf)\n",
    "print(f\"Accuracy of Random Forest Classification on test set: {accuracy_rf}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_rf, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a662d-acb8-4f6a-97ec-96e4dd956581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Random Forest Algorithm has many hyperparameters that one can hope to tune to achieve better accuracy in practice\n",
    "#help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da8219-a24c-4a47-b8e5-14dd85a3ad83",
   "metadata": {},
   "source": [
    "## Some Simulated Toy Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d96c5-46b9-4fce-80be-0d23b4a95e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE ONE:\n",
    "n = 750\n",
    "n_train = 500\n",
    "n_test = 250\n",
    "\n",
    "x1 = np.random.uniform(-1, 1, n)\n",
    "x2 = np.random.uniform(-1, 1, n)\n",
    "X = np.vstack([x1, x2]).transpose()\n",
    "\n",
    "Y = (x1 * x2 > 0).astype(np.int64)\n",
    "\n",
    "#Split the dataset into training and test datasets of sizes 500 and 250 respectively\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=500, test_size=250, random_state=1)\n",
    "\n",
    "x1_train = X_train[:,0]\n",
    "x2_train = X_train[:,1]\n",
    "x1_test = X_test[:,0]\n",
    "x2_test = X_test[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cac08-4e72-4e91-95cd-7f0246f1eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case where there are only two features, the data can be plotted \n",
    "#color of the points denotes Y\n",
    "def draw_results(x1, x2, color, plot_title=''):\n",
    "    plt.figure()\n",
    "    plt.scatter(x1, x2, c=color, cmap='viridis', alpha=0.7);\n",
    "    plt.colorbar()\n",
    "    plt.title(plot_title)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.tight_layout()\n",
    "\n",
    "draw_results(x1_train, x2_train, color=Y_train, plot_title='Training data')\n",
    "draw_results(x1_test, x2_test, color=Y_test, plot_title='Test data (ground truth)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b133d1b-ff47-4f63-96cd-d1f5979c920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method One: Logistic regression:\n",
    "logimodel_train = LogisticRegression(penalty = None, max_iter = 1000)\n",
    "logimodel_train.fit(X_train, Y_train)\n",
    "print(\"Intercept:\", logimodel_train.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train.coef_)\n",
    "#Prediction for the test data\n",
    "Y_pred = logimodel_train.predict(X_test)\n",
    "print(Y_pred)\n",
    "accuracy_logimodel = np.mean(Y_test == Y_pred)\n",
    "print(f\"Accuracy of Logistic Regression on test set: {accuracy_logimodel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0549a1b-556f-40a9-8a4e-5d14dfce855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "pred_probs_logistic = logimodel_train.predict_proba(X_test)[:,1]\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=pred_probs_logistic, \n",
    "    plot_title=\"Predicted probability of y=1 (logistic)\"\n",
    ")\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=Y_pred, \n",
    "    plot_title=\"Logistic model prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c246bfd-663e-4acb-b671-018f745362a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression will work here if we do feature engineering (include x1 * x2 as a third feature):\n",
    "# Create a new feature: x1 * x2\n",
    "#the following function returns an array like X but with a new feature that is x1 * x2\n",
    "def add_mult_feature(X):\n",
    "    \"\"\"Returns an array like X, but with a new feature that's X1 * X2\"\"\"\n",
    "    new_feature = X[:, 0] * X[:, 1]\n",
    "    return np.hstack([X, new_feature[:, None]])\n",
    "\n",
    "X_train_feat = add_mult_feature(X_train)\n",
    "X_test_feat = add_mult_feature(X_test)\n",
    "\n",
    "logimodel_train_feat = LogisticRegression(penalty = None, max_iter = 2000)\n",
    "logimodel_train_feat.fit(X_train_feat, Y_train)\n",
    "print(\"Intercept:\", logimodel_train_feat.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train_feat.coef_)\n",
    "#Prediction for the test data\n",
    "Y_pred_feat = logimodel_train_feat.predict(X_test_feat)\n",
    "print(Y_pred_feat)\n",
    "accuracy_logimodel_feat = np.mean(Y_test == Y_pred_feat)\n",
    "print(f\"Accuracy of Logistic Regression (with new feature x1*x2) on test set: {accuracy_logimodel_feat}\")\n",
    "\n",
    "pred_probs_logistic_feat = logimodel_train_feat.predict_proba(X_test_feat)[:,1]\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=pred_probs_logistic_feat, \n",
    "    plot_title=\"Predicted probability of y=1 (logistic)\"\n",
    ")\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=Y_pred_feat, \n",
    "    plot_title=\"Logistic model prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad9d77-b5ed-42bf-9a77-83b7dee352b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method Two: k-NN\n",
    "#Using k-NN classification:\n",
    "k_value = 8\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k_value)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn = np.mean(Y_test == Y_pred_knn)\n",
    "print(f\"Accuracy of k-NN Classification on test set: {accuracy_knn}\")\n",
    "#Contingency table (or confusion matrix)\n",
    "print(pd.crosstab(Y_test, Y_pred_knn, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530de0d-25eb-4bd7-be0e-8fa7161ced2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the results:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "probs_knn = knn.predict_proba(X_test)[:, 1]\n",
    "y_hat_knn = (probs_knn > 0.5).astype(np.int64)\n",
    "\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=probs_knn, \n",
    "    plot_title=\"Predicted probability of y=1 (k-NN)\"\n",
    ")\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=y_hat_knn, \n",
    "    plot_title=\"k-NN Model prediction\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0768dd5-9f12-49fe-bb81-6d258058849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Three: Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_dt = np.mean(Y_test == Y_pred_dt)\n",
    "print(f\"Accuracy of Decision Tree Classification on test set: {accuracy_dt}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_dt, rownames=['Actual'], colnames=['Predicted'], margins=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc9563-857d-4f99-8230-d4d0191fc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, feature_names=['x1', 'x2'],class_names = ('0','1'),  filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65d1fe-62d2-41b4-a0cf-1e01d355dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Four: Random Forest\n",
    "rf = RandomForestClassifier(random_state=1, max_features = None)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = np.mean(Y_test == Y_pred_rf)\n",
    "print(f\"Accuracy of Random Forest Classification on test set: {accuracy_rf}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_rf, rownames=['Actual'], colnames=['Predicted'], margins=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ac869-7172-4cd6-8349-73be51ce6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE TWO: Let us add some training noise to the previous dataset:\n",
    "Y_train_noisy = Y_train.copy()\n",
    "\n",
    "pts_to_flip = np.random.random(n_train) < 0.1 #we are selecting 10% of the datapoints\n",
    "Y_train_noisy[pts_to_flip] = 1 - Y_train_noisy[pts_to_flip] #we are corrupting the responses of these datapoints\n",
    "\n",
    "draw_results(x1_train, x2_train, color=Y_train_noisy, plot_title='Training data with noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e378d-18a7-4b15-bd8e-bb6d7d7bf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of Logistic Regression on this corrupted dataset:\n",
    "logimodel_train = LogisticRegression(penalty = None, max_iter = 1000)\n",
    "logimodel_train.fit(X_train, Y_train_noisy)\n",
    "print(\"Intercept:\", logimodel_train.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train.coef_)\n",
    "#Prediction for the test data\n",
    "Y_pred = logimodel_train.predict(X_test)\n",
    "print(Y_pred)\n",
    "accuracy_logimodel = np.mean(Y_test == Y_pred)\n",
    "print(f\"Accuracy of Logistic Regression on test set: {accuracy_logimodel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9dcde9-356a-40aa-be0a-cce8330c6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with the x1*x2 feature\n",
    "logimodel_train_feat = LogisticRegression(penalty = None, max_iter = 1000)\n",
    "logimodel_train_feat.fit(X_train_feat, Y_train_noisy)\n",
    "print(\"Intercept:\", logimodel_train_feat.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train_feat.coef_)\n",
    "#Prediction for the test data\n",
    "Y_pred_feat = logimodel_train_feat.predict(X_test_feat)\n",
    "print(Y_pred_feat)\n",
    "accuracy_logimodel_feat = np.mean(Y_test == Y_pred_feat)\n",
    "print(f\"Accuracy of Logistic Regression (with new feature x1*x2) on test set: {accuracy_logimodel_feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbb0f1-c2ac-4e0b-86e6-a281af460953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of k-NN on this corrupted data:\n",
    "k_value = 8\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k_value)\n",
    "knn.fit(X_train, Y_train_noisy)\n",
    "Y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn = np.mean(Y_test == Y_pred_knn)\n",
    "print(f\"Accuracy of k-NN Classification on test set: {accuracy_knn}\")\n",
    "#Contingency table (or confusion matrix)\n",
    "print(pd.crosstab(Y_test, Y_pred_knn, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa72b69-dae2-4066-af11-4b233072c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of Decision Tree on corrupted data:\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, Y_train_noisy)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_dt = np.mean(Y_test == Y_pred_dt)\n",
    "print(f\"Accuracy of Decision Tree Classification on test set: {accuracy_dt}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_dt, rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "# Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, feature_names=['x1', 'x2'],class_names = ('0','1'),  filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beecd36-2d53-457e-b067-fd10ec9efefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of Random Forest:\n",
    "rf = RandomForestClassifier(random_state=1, max_features = None)\n",
    "rf.fit(X_train, Y_train_noisy)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = np.mean(Y_test == Y_pred_rf)\n",
    "print(f\"Accuracy of Random Forest Classification on test set: {accuracy_rf}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_rf, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
