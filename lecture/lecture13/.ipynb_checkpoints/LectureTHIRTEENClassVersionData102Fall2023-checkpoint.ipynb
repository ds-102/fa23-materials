{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "plastic-albert",
   "metadata": {},
   "source": [
    "# Nonparametric methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import plot_tree\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e35bc-fdbb-4daf-8e3b-965ac07e7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression on the mroz dataset\n",
    "mroz = pd.read_csv(\"MROZ.csv\")\n",
    "print(mroz.shape)\n",
    "mroz.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd119ff3-07ba-4805-a73f-2d82669fba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mroz['inlf'] #this is a binary variable\n",
    "X = mroz[['kidslt6', 'age', 'educ', \n",
    "        'huswage', 'exper', 'expersq']].copy()\n",
    "X = sm.add_constant(X)\n",
    "logimodel = sm.GLM(Y, X, family=sm.families.Binomial()).fit()\n",
    "print(logimodel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98841308-9e20-4ea3-a53b-6992f34b3525",
   "metadata": {},
   "source": [
    "What is the interpretation of the coefficient -1.4516 of the 'kidslt6' variable? It means that having a small kid reduces the log-odds of working by -1.4516. This means that the odds are reduced by a factor of $\\exp(-1.4516) \\approx 0.234$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153227c-e0d8-4c7d-ba7c-4cf28d4fcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We used statsmodels.api for fitting GLMs. Today we shall switch to scikit-learn because it also fits nonparametric models such as decision trees and random forests\n",
    "#Let us re-fit this logistic regression model using scikit-learn\n",
    "Y = mroz['inlf']  # This is a binary variable\n",
    "X = mroz[['kidslt6', 'age', 'educ', 'huswage', 'exper', 'expersq']].copy()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logimodel_sk = LogisticRegression(max_iter = 1000)\n",
    "logimodel_sk.fit(X, Y)\n",
    "print(\"Intercept:\", logimodel_sk.intercept_)\n",
    "print(\"Coefficients:\", logimodel_sk.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985dc605-630e-48eb-bf8f-35cebf376896",
   "metadata": {},
   "source": [
    "These coefficients are slightly different from those given by statsmodels.api because scikit-learn does not fit the MLE exactly but also imposes some L2 regularization (similar to ridge regression). The usual MLE can be obtained by setting penalty = None as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9a101-fa8b-40d7-968a-d2d570423704",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mroz['inlf']  # This is a binary variable\n",
    "X = mroz[['kidslt6', 'age', 'educ', 'huswage', 'exper', 'expersq']].copy()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logimodel_sk = LogisticRegression(penalty=None, max_iter = 1000)\n",
    "logimodel_sk.fit(X, Y)\n",
    "print(\"Intercept:\", logimodel_sk.intercept_)\n",
    "print(\"Coefficients:\", logimodel_sk.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1191f-3ebf-4bc9-aba5-f3b3a75e5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of the logistic regression model:\n",
    "#Split the original data into training and test datasets\n",
    "#We will fit the model on the training dataset and test its accuracy on the test dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=503, test_size=250, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca074c9c-6a13-4b60-b198-506c483391a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us fit logistic regression on the training dataset:\n",
    "logimodel_train = LogisticRegression(penalty = None, max_iter = 1000)\n",
    "logimodel_train.fit(X_train, Y_train)\n",
    "print(\"Intercept:\", logimodel_train.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8efa38-1587-4913-83a1-e4354444baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction for the test data\n",
    "Y_pred = logimodel_train.predict(X_test)\n",
    "print(Y_pred)\n",
    "accuracy_logimodel = np.mean(Y_test == Y_pred)\n",
    "print(f\"Accuracy of Logistic Regression on test set: {accuracy_logimodel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635a9c8-bbd8-4b11-9c5c-7fa9f7212e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(Y_test, Y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad2d5e-49b5-4211-a997-5ca3089167fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using k-NN classification:\n",
    "k_value = 8\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k_value)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn = np.mean(Y_test == Y_pred_knn)\n",
    "print(f\"Accuracy of k-NN Classification on test set: {accuracy_knn}\")\n",
    "#Contingency table (or confusion matrix)\n",
    "print(pd.crosstab(Y_test, Y_pred_knn, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd2455-e165-475b-a0d8-b0c8643bf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_dt = np.mean(Y_test == Y_pred_dt)\n",
    "print(f\"Accuracy of Decision Tree Classification on test set: {accuracy_dt}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_dt, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc987d8-7a0a-4b6c-896e-717c765e7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, feature_names=X.columns,class_names = ('0','1'),  filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1f197-5a86-4034-b008-7c2201e92dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tree is way too big to allow any meaningful interpretation\n",
    "#Here are a few ways to create smaller trees:\n",
    "dt_small = DecisionTreeClassifier(max_depth = 3, random_state = 1)\n",
    "dt_small.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_dt_small = dt_small.predict(X_test)\n",
    "# Calculate the accuracy\n",
    "accuracy_dt_small = np.mean(Y_test == Y_pred_dt_small)\n",
    "print(f\"Accuracy of Decision Tree Classification on test set: {accuracy_dt_small}\")\n",
    "\n",
    "# Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_small, feature_names=X.columns, class_names = ('0','1'),  filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14da5d-a95e-4b48-9b64-9a7efdc1e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=1, max_features = None)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = np.mean(Y_test == Y_pred_rf)\n",
    "print(f\"Accuracy of Random Forest Classification on test set: {accuracy_rf}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_rf, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a662d-acb8-4f6a-97ec-96e4dd956581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Random Forest Algorithm has many hyperparameters that one can hope to tune to achieve better accuracy in practice\n",
    "#help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da8219-a24c-4a47-b8e5-14dd85a3ad83",
   "metadata": {},
   "source": [
    "## Some Simulated Toy Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d96c5-46b9-4fce-80be-0d23b4a95e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE ONE:\n",
    "n = 750\n",
    "n_train = 500\n",
    "n_test = 250\n",
    "\n",
    "x1 = np.random.uniform(-1, 1, n)\n",
    "x2 = np.random.uniform(-1, 1, n)\n",
    "X = np.vstack([x1, x2]).transpose()\n",
    "\n",
    "Y = (x1 * x2 > 0).astype(np.int64)\n",
    "\n",
    "#Split the dataset into training and test datasets of sizes 500 and 250 respectively\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=500, test_size=250, random_state=1)\n",
    "\n",
    "x1_train = X_train[:,0]\n",
    "x2_train = X_train[:,1]\n",
    "x1_test = X_test[:,0]\n",
    "x2_test = X_test[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cac08-4e72-4e91-95cd-7f0246f1eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case where there are only two features, the data can be plotted \n",
    "#color of the points denotes Y\n",
    "def draw_results(x1, x2, color, plot_title=''):\n",
    "    plt.figure()\n",
    "    plt.scatter(x1, x2, c=color, cmap='viridis', alpha=0.7);\n",
    "    plt.colorbar()\n",
    "    plt.title(plot_title)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.tight_layout()\n",
    "\n",
    "draw_results(x1_train, x2_train, color=Y_train, plot_title='Training data')\n",
    "draw_results(x1_test, x2_test, color=Y_test, plot_title='Test data (ground truth)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b133d1b-ff47-4f63-96cd-d1f5979c920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method One: Logistic regression:\n",
    "logimodel_train = LogisticRegression(penalty = None, max_iter = 1000)\n",
    "logimodel_train.fit(X_train, Y_train)\n",
    "print(\"Intercept:\", logimodel_train.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train.coef_)\n",
    "#Prediction for the test data\n",
    "Y_pred = logimodel_train.predict(X_test)\n",
    "print(Y_pred)\n",
    "accuracy_logimodel = np.mean(Y_test == Y_pred)\n",
    "print(f\"Accuracy of Logistic Regression on test set: {accuracy_logimodel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0549a1b-556f-40a9-8a4e-5d14dfce855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "pred_probs_logistic = logimodel_train.predict_proba(X_test)[:,1]\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=pred_probs_logistic, \n",
    "    plot_title=\"Predicted probability of y=1 (logistic)\"\n",
    ")\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=Y_pred, \n",
    "    plot_title=\"Logistic model prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c246bfd-663e-4acb-b671-018f745362a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression will work here if we do feature engineering (include x1 * x2 as a third feature):\n",
    "# Create a new feature: x1 * x2\n",
    "#the following function returns an array like X but with a new feature that is x1 * x2\n",
    "def add_mult_feature(X):\n",
    "    \"\"\"Returns an array like X, but with a new feature that's X1 * X2\"\"\"\n",
    "    new_feature = X[:, 0] * X[:, 1]\n",
    "    return np.hstack([X, new_feature[:, None]])\n",
    "\n",
    "X_train_feat = add_mult_feature(X_train)\n",
    "X_test_feat = add_mult_feature(X_test)\n",
    "\n",
    "logimodel_train_feat = LogisticRegression(penalty = None, max_iter = 2000)\n",
    "logimodel_train_feat.fit(X_train_feat, Y_train)\n",
    "print(\"Intercept:\", logimodel_train_feat.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train_feat.coef_)\n",
    "#Prediction for the test data\n",
    "Y_pred_feat = logimodel_train_feat.predict(X_test_feat)\n",
    "print(Y_pred_feat)\n",
    "accuracy_logimodel_feat = np.mean(Y_test == Y_pred_feat)\n",
    "print(f\"Accuracy of Logistic Regression (with new feature x1*x2) on test set: {accuracy_logimodel_feat}\")\n",
    "\n",
    "pred_probs_logistic_feat = logimodel_train_feat.predict_proba(X_test_feat)[:,1]\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=pred_probs_logistic_feat, \n",
    "    plot_title=\"Predicted probability of y=1 (logistic)\"\n",
    ")\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=Y_pred_feat, \n",
    "    plot_title=\"Logistic model prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad9d77-b5ed-42bf-9a77-83b7dee352b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method Two: k-NN\n",
    "#Using k-NN classification:\n",
    "k_value = 8\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k_value)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn = np.mean(Y_test == Y_pred_knn)\n",
    "print(f\"Accuracy of k-NN Classification on test set: {accuracy_knn}\")\n",
    "#Contingency table (or confusion matrix)\n",
    "print(pd.crosstab(Y_test, Y_pred_knn, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530de0d-25eb-4bd7-be0e-8fa7161ced2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the results:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "probs_knn = knn.predict_proba(X_test)[:, 1]\n",
    "y_hat_knn = (probs_knn > 0.5).astype(np.int64)\n",
    "\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=probs_knn, \n",
    "    plot_title=\"Predicted probability of y=1 (k-NN)\"\n",
    ")\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=y_hat_knn, \n",
    "    plot_title=\"k-NN Model prediction\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0768dd5-9f12-49fe-bb81-6d258058849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Three: Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_dt = np.mean(Y_test == Y_pred_dt)\n",
    "print(f\"Accuracy of Decision Tree Classification on test set: {accuracy_dt}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_dt, rownames=['Actual'], colnames=['Predicted'], margins=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc9563-857d-4f99-8230-d4d0191fc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, feature_names=['x1', 'x2'],class_names = ('0','1'),  filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65d1fe-62d2-41b4-a0cf-1e01d355dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Four: Random Forest\n",
    "rf = RandomForestClassifier(random_state=1, max_features = None)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = np.mean(Y_test == Y_pred_rf)\n",
    "print(f\"Accuracy of Random Forest Classification on test set: {accuracy_rf}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_rf, rownames=['Actual'], colnames=['Predicted'], margins=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ac869-7172-4cd6-8349-73be51ce6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE TWO: Let us add some training noise to the previous dataset:\n",
    "Y_train_noisy = Y_train.copy()\n",
    "\n",
    "pts_to_flip = np.random.random(n_train) < 0.1 #we are selecting 10% of the datapoints\n",
    "Y_train_noisy[pts_to_flip] = 1 - Y_train_noisy[pts_to_flip] #we are corrupting the responses of these datapoints\n",
    "\n",
    "draw_results(x1_train, x2_train, color=Y_train_noisy, plot_title='Training data with noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e378d-18a7-4b15-bd8e-bb6d7d7bf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of Logistic Regression on this corrupted dataset:\n",
    "logimodel_train = LogisticRegression(penalty = None, max_iter = 1000)\n",
    "logimodel_train.fit(X_train, Y_train_noisy)\n",
    "print(\"Intercept:\", logimodel_train.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train.coef_)\n",
    "#Prediction for the test data\n",
    "Y_pred = logimodel_train.predict(X_test)\n",
    "print(Y_pred)\n",
    "accuracy_logimodel = np.mean(Y_test == Y_pred)\n",
    "print(f\"Accuracy of Logistic Regression on test set: {accuracy_logimodel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9dcde9-356a-40aa-be0a-cce8330c6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with the x1*x2 feature\n",
    "logimodel_train_feat = LogisticRegression(penalty = None, max_iter = 1000)\n",
    "logimodel_train_feat.fit(X_train_feat, Y_train_noisy)\n",
    "print(\"Intercept:\", logimodel_train_feat.intercept_)\n",
    "print(\"Coefficients:\", logimodel_train_feat.coef_)\n",
    "#Prediction for the test data\n",
    "Y_pred_feat = logimodel_train_feat.predict(X_test_feat)\n",
    "print(Y_pred_feat)\n",
    "accuracy_logimodel_feat = np.mean(Y_test == Y_pred_feat)\n",
    "print(f\"Accuracy of Logistic Regression (with new feature x1*x2) on test set: {accuracy_logimodel_feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbb0f1-c2ac-4e0b-86e6-a281af460953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of k-NN on this corrupted data:\n",
    "k_value = 8\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = k_value)\n",
    "knn.fit(X_train, Y_train_noisy)\n",
    "Y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn = np.mean(Y_test == Y_pred_knn)\n",
    "print(f\"Accuracy of k-NN Classification on test set: {accuracy_knn}\")\n",
    "#Contingency table (or confusion matrix)\n",
    "print(pd.crosstab(Y_test, Y_pred_knn, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa72b69-dae2-4066-af11-4b233072c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of Decision Tree on corrupted data:\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, Y_train_noisy)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_dt = np.mean(Y_test == Y_pred_dt)\n",
    "print(f\"Accuracy of Decision Tree Classification on test set: {accuracy_dt}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_dt, rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "# Plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt, feature_names=['x1', 'x2'],class_names = ('0','1'),  filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beecd36-2d53-457e-b067-fd10ec9efefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of Random Forest:\n",
    "rf = RandomForestClassifier(random_state=1, max_features = None)\n",
    "rf.fit(X_train, Y_train_noisy)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = np.mean(Y_test == Y_pred_rf)\n",
    "print(f\"Accuracy of Random Forest Classification on test set: {accuracy_rf}\")\n",
    "\n",
    "# Create the confusion matrix (contingency table)\n",
    "print(pd.crosstab(Y_test, Y_pred_rf, rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
