{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e5e0c0-8f0e-40af-93d5-5d4629f2e9ba",
   "metadata": {},
   "source": [
    "# A study of some techniques for Monte Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a5c13-dc7e-4bd9-ab6d-2470b1397edd",
   "metadata": {},
   "source": [
    "We have so far seen several examples of Bayesian Modeling using PyMC3. The output here is always in the form of posterior Monte Carlo samples. These samples can be used to approximate the underlying actual posterior distributions. We shall look closely at two standard sampling techniques for generating posterior samples: \n",
    "1. Rejection Sampling\n",
    "2. Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63828a-fc91-498b-9c7c-1da51930c362",
   "metadata": {},
   "source": [
    "## Rejection Sampling\n",
    "\n",
    "Rejection sampling is a very old method for obtaining Monte Carlo Samples. Suppose the goal is to obtain samples from a density $f_{\\text{Target}}(u)$. Rejection sampling provides an algorithm for generating samples from $f_{\\text{Target}}(u)$ using another density $f_{\\text{Proposal}}(u)$. This proposal density should satisfy the following two properties: \n",
    "1. Obtaining samples from $f_{\\text{Proposal}}(u)$ should be easy. \n",
    "2. There must exist a positive real number $M$ such that $$f_{\\text{Target}}(u) \\leq M f_{\\text{Proposal}}(u)$$ for all values of $u$. Although the method will work, in principle, for any positive $M$ satisfying the above condition, it becomes extremely inefficient if $M$ gets large as will be clear below. So we want $M$ to satisfy the above condition and also to be not too large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01477f15-1aed-471a-829c-d03b7abce6de",
   "metadata": {},
   "source": [
    "Rejection Sampling is based on the following very simple (Bayesian) fact. Consider the following Bayesian model: \n",
    "\\begin{align*}\n",
    "    \\text{Prior}: \\Theta \\sim f_{\\text{Proposal}} ~~~ \\text{ and } ~~~ \\text{Likelihood}: Y \\mid \\Theta = u \\sim \\text{Bernoulli} \\left(\\frac{f_{\\text{Target}}(u)}{M f_{\\text{Proposal}}(u)} \\right)\n",
    "\\end{align*}\n",
    "So in this model, the unknown parameter $\\Theta$ has the prior density $f_{\\text{Proposal}}$. The data is binary and the likelihood is given by the Bernoulli distribution with parameter $\\frac{f_{\\text{Target}}(u)}{M f_{\\text{Proposal}}(u)}.$ Note that the Bernoulli parameter should be always between 0 and 1 which is why we need the condition $f_{\\text{Target}}(u) \\leq M f_{\\text{Proposal}}(u)$ for all values of $u$. The key fact underlying Rejection Sampling is that the posterior density of $\\Theta$ in this model given $Y = 1$ is exactly $f_{\\text{Target}}$:\n",
    "\\begin{align*}\n",
    "   \\Theta \\mid Y = 1  ~~ \\sim f_{\\text{Target}}\n",
    "\\end{align*}\n",
    "This is proved by a simple application of Bayes rule as follows: \n",
    "\\begin{align*}\n",
    "    f_{\\Theta \\mid Y = 1}(u) &= \\frac{f_{\\Theta}(u) \\mathbb{P}\\{Y = 1 \\mid \\Theta = u\\}}{\\int f_{\\Theta}(v) \\mathbb{P}\\{Y = 1 \\mid \\Theta = v\\} dv} \\\\\n",
    "    &= \\frac{f_{\\text{Proposal}}(u) \\frac{f_{\\text{Target}}(u)}{M f_{\\text{Proposal}}(u)}}{\\int f_{\\text{Proposal}}(v) \\frac{f_{\\text{Target}}(v)}{M f_{\\text{Proposal}}(v)} dv} \\\\\n",
    "    &= \\frac{\\frac{f_{\\text{Target}}(u)}{M}}{\\int \\frac{f_{\\text{Target}}(v)}{M } dv} = f_{\\text{Target}}(u). \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860c786-2edc-4993-83b4-e79a7da24cb8",
   "metadata": {},
   "source": [
    "Based on this idea, the algorithm for Rejection Sampling goes as follows. \n",
    "1. Generate lots of samples $(\\Theta^{(i)}, Y^{(i)}$ from the probability model described above. In other words, take $\\Theta^{(i)} \\sim f_{\\text{Proposal}}$ and $Y^{(i)} \\mid \\Theta = \\Theta^{(i)} \\sim  \\text{Bernoulli} \\left(\\frac{f_{\\text{Target}}(\\Theta^{(i)})}{M f_{\\text{Proposal}}(\\Theta^{(i)})} \\right)$\n",
    "2. Based on the samples, approximate the distribution of $\\Theta \\mid Y = 1$. This is done by discarding (or **rejecting**) the samples for which $Y^{(i)} = 0$ and keeping only the ones with $Y^{(i)} = 1$. The $\\Theta^{(i)}$'s in the remaining samples approximate $f_{\\text{Target}}$.\n",
    "\n",
    "Here is how this algorithm works for sampling from Beta distributions. Here $f_{\\text{Target}}$ is a Beta density and $f_{\\text{Proposal}}$ is the uniform density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fd09e-4564-4e2f-bf43-a17172058d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc8f6d-f92e-41f6-b8b7-7f7cbe933df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rejection sampling for generating samples from Beta(4, 1):\n",
    "#Our proposal distribution will be Uniform[0, 1]\n",
    "#The value of M can be taken to be the largest value of the density. \n",
    "M = 4\n",
    "N = 20000 #this is the number of proposal samples that we will generate\n",
    "prior_samples = np.random.rand(N)\n",
    "p_prior_samples = prior_samples ** 3\n",
    "Y_samples = np.random.binomial(n = 1, p = p_prior_samples)\n",
    "posterior_samples = prior_samples[Y_samples == 1]\n",
    "print(len(posterior_samples))\n",
    "plt.hist(posterior_samples, bins = 500, density = True, alpha = 0.6, label = 'Rejection Sampling Samples from Beta(4, 1)') \n",
    "x = np.linspace(0, 1, 1000)\n",
    "from scipy.stats import beta\n",
    "pdf_values = beta.pdf(x, 4, 1)\n",
    "plt.plot(x, pdf_values, 'r-', label = 'Beta(4, 1) Density')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Histogram of Samples from Rejection Sampling and Beta(4, 1) density')\n",
    "plt.show()\n",
    "#The match between the histogram and the true density is not bad but we are only \n",
    "#getting about 1/4 of the total samples (others are rejected because of Y = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07bf55-260d-4638-9b85-f2ff17d501c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rejection sampling for generating samples from Beta(20, 2):\n",
    "#Our proposal distribution will be Uniform[0, 1]\n",
    "#The value of M can be taken to be anything larger than the largest value of the density. \n",
    "M = 8\n",
    "N = 20000 #this is the number of proposal samples that we will generate\n",
    "prior_samples = np.random.rand(N)\n",
    "p_prior_samples = 420 * (prior_samples ** 19) * (1 - prior_samples) * (1/8)\n",
    "Y_samples = np.random.binomial(n = 1, p = p_prior_samples)\n",
    "posterior_samples = prior_samples[Y_samples == 1]\n",
    "print(len(posterior_samples))\n",
    "plt.hist(posterior_samples, bins = 500, density = True, alpha = 0.6, label = 'Rejection Sampling Samples from Beta(20, 2)') \n",
    "x = np.linspace(0, 1, 1000)\n",
    "from scipy.stats import beta\n",
    "pdf_values = beta.pdf(x, 20, 2)\n",
    "plt.plot(x, pdf_values, 'r-', label = 'Beta(20, 2) Density')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Histogram of Samples from Rejection Sampling and Beta(20, 2) density')\n",
    "plt.show()\n",
    "#The match between the histogram and the true density is not bad but we are only \n",
    "#getting about 1/8 of the total samples (others are rejected because of Y = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c7a3b-3229-4449-8db8-a7032c765846",
   "metadata": {},
   "source": [
    "In general, the marginal probability of $Y = 1$ in the rejection sampling Bayesian model equals $1/M$: \n",
    "\\begin{align*}\n",
    "  \\mathbb{P} \\{Y = 1\\} &= \\int f_{\\Theta}(v) \\mathbb{P}\\{Y = 1 \\mid \\Theta = v\\} dv \\\\ &= \\int f_{\\text{Proposal}}(v) \\frac{f_{\\text{Target}}(v)}{M f_{\\text{Proposal}}(v)} dv = \\frac{1}{M} \\int f_{\\text{Target}}(v) dv = \\frac{1}{M}\n",
    "\\end{align*}\n",
    "So if $N$ samples $(\\Theta^{(i)}, Y^{(i)})$ are originally generated from the prior (proposal density), then only about $N/M$ will have $Y^{(i)} = 1$. Thus we can expect to have about $N/M$ samples from the posterior (target density). Thus if $M$ is large, we will have very few samples from the target density. The main trick in Rejection Sampling therefore is to choose the proposal density so that $M$ is not too large (this, of course, may not be always possible because the proposal density also needs to be such that samples can be generated from it easily). This is the reason why Rejection Sampling is inefficient in most practical instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a646c1-5854-4372-a102-f2145b836e8e",
   "metadata": {},
   "source": [
    "# Gibbs Sampling\n",
    "\n",
    "The Gibbs Sampler is a very standard method for drawing posterior samples. The goal is to generate samples from a target density $f_{\\text{Target}}(u)$. The Gibbs sampler is applicable when $u$ is a multivariate vector. To start with, let us assume that $u$ is bivariate consisting of two components $u_1$ and $u_2$ i.e., $u = (u_1, u_2)$, and that $f_{\\text{Target}}(u_1, u_2)$ is the joint density of two random variables $U_1$ and $U_2$. The Gibbs sampler will produce samples of the form: \n",
    "\\begin{align*}\n",
    "   (u_1^{(0)}, u_2^{(0)}) \\rightarrow (u_1^{(1)}, u_2^{(1)}) \\rightarrow (u_1^{(2)}, u_2^{(2)}) \\rightarrow \\dots \\rightarrow (u_1^{(T)}, u_2^{(T)})\n",
    "\\end{align*}\n",
    "for a large $T$, by going through the following steps: \n",
    "1. Initialize at arbitrary $u_1^{(0)}$ and $u_2^{(0)}$. Repeat the following for $t = 0, 1, 2, \\dots$\n",
    "2. Given the current sample $(u_1^{(t)}, u_2^{(t)})$,\n",
    "   1. draw $u_1^{(t+1)}$ randomly from the conditional distribution of $U_1$ given $U_2 = u_2^{(t)}$. In other words $$u_1^{(t+1)} \\sim        \\text{density of } U_1 \\mid U_2 = u_2^{(t)}.$$\n",
    "   2. draw $u_2^{(t+1)}$ randomly from the conditional distribution of $U_2$ given $U_1 = u_1^{(t+1)}$. In other words $$u_2^{(t+1)} \\sim    \\text{density of } U_2 \\mid U_1 = u_1^{(t+1)}.$$\n",
    "\n",
    "The Gibbs sampler is a very simple and intuitive algorithm. It works by repeatedly sampling from the conditional distributions $U_1 \\mid U_2 = u_2$ and $U_2 \\mid U_1 = u_1$ by using the most recent values for $u_1$ and $u_2$. Let us illustrate how the Gibbs sampler works by solving the problem on normal mean estimation from last lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747187c-1d0d-4904-950a-cf9edb88b41b",
   "metadata": {},
   "source": [
    "Here is the normal mean estimation problem from last lecture. A simple experiment to measure the length of an object led to the following 15 measurements: $17.62, 17.61, 17.61, 17.62, 17.62, 17.615, 17.615, 17.625, 17.61, 17.62, 17.62, 17.605, 17.61, 17.62, 17.61.$  What is the best estimate for the length of this object?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140054b-85fe-4aa2-955d-dddac532f200",
   "metadata": {},
   "source": [
    "The usual analysis of this problem first assumes that $$Y_1, \\dots, Y_n \\overset{\\text{i.i.d}}{\\sim} N(\\theta, \\sigma^2).$$ One then estimates $\\theta$ by the sample mean: $\\bar{Y} = \\frac{Y_1 + \\dots + Y_n}{n}$. After this, one uses\n",
    "\\begin{align*}\n",
    "    \\bar{Y} \\sim N\\left(\\theta, \\frac{\\sigma^2}{n} \\right) \\text{ or, equivalently } \\frac{\\bar{Y} - \\theta}{\\sqrt{\\frac{\\sigma^2}{n}}} = \\frac{\\sqrt{n}(\\bar{Y} - \\theta)}{\\sigma}\\sim N(0, 1). \n",
    "\\end{align*}\n",
    "The above distributional statement cannot be used for constructing confidence intervals for $\\theta$ because $\\sigma$ is unknown. One then estimates $\\sigma$ as\n",
    "\\begin{align*}\n",
    "   \\hat{\\sigma} := \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})^2}, \n",
    "\\end{align*}\n",
    "and uses the following fact: \n",
    "\\begin{align*}\n",
    "    \\frac{\\sqrt{n}(\\bar{Y} - \\theta)}{\\hat{\\sigma}}\\sim \\mathbf{t}_{n-1}\n",
    "\\end{align*}\n",
    "where $\\mathbf{t}_{n-1}$ is the Student $t$-distribution  with $n-1$ degrees of freedom. From this, one can construct a $100(1-\\alpha)$ confidence interval for $\\theta$ as:\n",
    "\\begin{align*}\n",
    "   \\left[\\bar{Y} - \\frac{\\sigma}{\\sqrt{n}} \\mathbf{t}_{n-1, \\alpha/2}, \\bar{Y} + \\frac{\\sigma}{\\sqrt{n}} \\mathbf{t}_{n-1, \\alpha/2}  \\right]\n",
    "\\end{align*}\n",
    "where $\\mathbf{t}_{n-1, \\alpha/2}$ is the point on the positive real line to the right of which the $\\mathbf{t}_{n-1}$ distribution assigns probability $\\alpha/2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67df69-b993-4094-8b59-370f0357022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 15\n",
    "y_obs = np.array([17.62, 17.61, 17.61, 17.62, 17.62, 17.615, 17.615, 17.625, 17.61, 17.62, 17.62, 17.605, 17.61, 17.62, 17.61])\n",
    "ybar = np.mean(y_obs)\n",
    "sighat = np.std(y_obs, ddof = 1)\n",
    "display(ybar, sighat)\n",
    "alpha = 0.05\n",
    "from scipy.stats import t\n",
    "t_critical_value = t.ppf(1 - alpha/2, n-1)\n",
    "print(t_critical_value)\n",
    "#95% confidence interval\n",
    "ci_lower = ybar - sighat*(1/np.sqrt(n))*t_critical_value\n",
    "ci_upper = ybar + sighat*(1/np.sqrt(n))*t_critical_value\n",
    "display([ci_lower, ci_upper])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d0f4b-079a-4719-b926-1a7831050e8e",
   "metadata": {},
   "source": [
    "In our Bayesian analysis of this problem, there were two parameters: $\\theta$ which is the unknown length of the object, and $\\sigma$ which represents the uncertainty in each individual measurement. We used the following prior and likelihood: \n",
    "\\begin{align*}\n",
    "  \\text{Prior: } \\theta \\sim \\text{Uniform}[-80, 80] ~~ \\text{ and } ~~ \\log \\sigma \\sim \\text{Uniform}[-10, 10]\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "  \\text{Likelihood: } y_1, \\dots, y_n \\mid \\theta, \\sigma \\overset{\\text{i.i.d}}{\\sim} N(\\theta, \\sigma^2)\n",
    "\\end{align*}\n",
    "We then specified this model in PyMC and obtained posterior samples from PyMC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58aee8f-ed68-4f99-9145-b12cc8921506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "n = 15\n",
    "y_obs = np.array([17.62, 17.61, 17.61, 17.62, 17.62, 17.615, 17.615, 17.625, 17.61, 17.62, 17.62, 17.605, 17.61, 17.62, 17.61])\n",
    "measurement_model = pm.Model()\n",
    "with measurement_model:\n",
    "    theta = pm.Uniform(\"theta\", lower = -80, upper = 80)\n",
    "    log_sigma = pm.Uniform(\"log_sigma\", lower = -10, upper = 10)\n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.exp(log_sigma))\n",
    "    Y = pm.Normal(\"Y\", mu = theta, sigma = sigma, observed=y_obs)\n",
    "    #Sample from posterior:\n",
    "    idata = pm.sample(2000, chains = 2, return_inferencedata = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b100dae-2182-455e-97e5-ede78f24e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The posterior samples can be used to estimate theta (and sigma) along with uncertainty quantification as follows: \n",
    "theta_samples = idata.posterior['theta'].values.flatten()\n",
    "log_sigma_samples = idata.posterior['log_sigma'].values.flatten()\n",
    "sigma_samples = np.exp(log_sigma_samples)\n",
    "#Our best estimate of the unknown length can be taken to be the mean of the posterior samples for theta:\n",
    "thetamean = np.mean(theta_samples)\n",
    "display(thetamean)\n",
    "#A 95% interval for theta based on the posterior samples is computed as follows:\n",
    "lower_limit_theta = np.percentile(theta_samples, 2.5)\n",
    "upper_limit_theta = np.percentile(theta_samples, 97.5)\n",
    "display([lower_limit_theta, upper_limit_theta])\n",
    "#This interval should be very similar to the confidence interval for theta derived previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543cd66-aad7-4d30-80cc-d299ea34bd66",
   "metadata": {},
   "source": [
    "We now obtain posterior samples for $U_1 := \\theta$ and $U_2 := \\sigma$ by the Gibbs sampling algorithm (instead of using PyMC). In other words, we need to sample from the target density $f_{\\theta, \\sigma \\mid \\text{data}}(t, s)$. Note that, by Bayes rule, this posterior density equals: \n",
    "\\begin{align*}\n",
    "   f_{\\theta, \\sigma \\mid \\text{data}}(t, s) = \\frac{f_{\\theta, \\sigma}(t, s) f_{\\text{data} \\mid \\theta = t, \\sigma = s}(\\text{data})}{\\int \\int f_{\\theta, \\sigma}(t, s) f_{\\text{data} \\mid \\theta = t, \\sigma = s}(\\text{data}) dt ds}\n",
    "\\end{align*}\n",
    "The numerator above is simply $prior \\times likelihood$ and the denominator is the normalizing constant which makes the density (over $t, s$) integrate to one. One often ignores the denominator and writes the above relation using the \"proportional to\" sign ($\\propto$) as:\n",
    "\\begin{align*}\n",
    "   f_{\\theta, \\sigma \\mid \\text{data}}(t, s) \\propto \\text{Prior} \\times \\text{Likelihood} = f_{\\theta, \\sigma}(t, s) f_{\\text{data} \\mid \\theta = t, \\sigma = s}(\\text{data})\n",
    "\\end{align*}\n",
    "The prior is given by\n",
    "\\begin{align*}\n",
    "  f_{\\theta, \\sigma}(t, s) &= f_{\\theta}(t) \\times f_{\\sigma}(s), ~~\\text{assuming prior independence betweeen } \\theta ~ \\text{ and } ~\\sigma \\\\\n",
    "  &= \\frac{I\\{-80 \\leq t \\leq 80\\}}{160} \\times f_{\\log \\sigma}(\\log s) \\frac{d \\log s}{ds}, ~~ \\text{ where the identity } f_{\\sigma}(s) = f_{\\log \\sigma}(\\log s) \\frac{d \\log s}{ds} ~ \\text{ follows from the Jacobian formula} \\\\\n",
    "  &= \\frac{I\\{-80 \\leq t \\leq 80\\}}{160} \\times \\frac{I\\{-10 \\leq \\log s \\leq 10\\}}{20} \\frac{1}{s} \\\\\n",
    "  &= \\frac{I\\{-80 \\leq t \\leq 80\\}}{160} \\times \\frac{I\\{\\exp(-10) \\leq s\\leq \\exp(10)\\}}{20} \\frac{1}{s} \\\\\n",
    "  &\\propto I\\{-80 \\leq t \\leq 80\\} I\\{\\exp(-10) \\leq s \\leq \\exp(10)\\} \\frac{1}{s}\n",
    "\\end{align*}\n",
    "where, in the last step, we ignored the constant factors $160$ and $20$ by replacing the equality sign with the proportional sign. \n",
    "\n",
    "The Likelihood is given by\n",
    "\\begin{align*}\n",
    "   f_{\\text{data} \\mid \\theta = t, \\sigma = s}(\\text{data}) &= f_{y_1, \\dots, y_n \\mid \\theta = t, \\sigma = s}(y_1, \\dots, y_n) \\\\\n",
    "   &= f_{y_1 \\mid \\theta = t, \\sigma = s}(y_1) \\times f_{y_2 \\mid \\theta = t, \\sigma = s}(y_2) \\times \\dots \\times f_{y_n \\mid \\theta = t, \\sigma = s}(y_n) \\\\\n",
    "   &= \\frac{1}{s\\sqrt{2\\pi}} \\exp \\left(-\\frac{(y_1 - t)^2}{2s^2} \\right) \\times \\frac{1}{s\\sqrt{2\\pi}} \\exp \\left(-\\frac{(y_2 - t)^2}{2s^2} \\right) \\times \\dots \\times \\frac{1}{s\\sqrt{2\\pi}} \\exp \\left(-\\frac{(y_n - t)^2}{2s^2} \\right) \\\\\n",
    "   &= \\frac{1}{s^n (\\sqrt{2 \\pi})^n} \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right).\n",
    "\\end{align*}\n",
    "The constant factor $\\frac{1}{(\\sqrt{2\\pi})^n}$ does not depend on the parameters $t, s$ and so it can be ignored in proportionality leading to\n",
    "\\begin{align*}\n",
    "   f_{\\text{data} \\mid \\theta = t, \\sigma = s}(\\text{data}) \\propto \\frac{1}{s^n}  \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right).\n",
    "\\end{align*}\n",
    "Combining the above derived formulae for the prior and likelihood, we obtain the posterior as:\n",
    "\\begin{align*}\n",
    "& f_{\\theta, \\sigma \\mid \\text{data}}(t, s) \\\\ &\\propto \\text{Prior} \\times \\text{Likelihood} \\\\\n",
    "&\\propto I\\{-80 \\leq t \\leq 80\\} I\\{\\exp(-10) \\leq s \\leq \\exp(10)\\} \\frac{1}{s} \\times \\frac{1}{s^n}  \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right) \\\\\n",
    "&=  I\\{-80 \\leq t \\leq 80\\} I\\{\\exp(-10) \\leq s \\leq \\exp(10)\\} \\frac{1}{s^{n+1}}  \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right).\n",
    "\\end{align*}\n",
    "The above relation is true up to proportionality. If we want to have equality, we have to normalize the right hand side leading to:\n",
    "\\begin{align*}\n",
    "  f_{\\theta, \\sigma \\mid \\text{data}}(t, s) = \\frac{I\\{-80 \\leq t \\leq 80\\} I\\{\\exp(-10) \\leq s \\leq \\exp(10)\\} \\frac{1}{s^{n+1}}  \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right)}{\\int_{-80}^{80} \\int_{e^{-10}}^{e^{10}} \\frac{1}{s^{n+1}}  \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right) dt ds}\n",
    "\\end{align*}\n",
    "We now use the Gibbs sampler to generate samples $(t^{(0)}, s^{(0)})$, $(t^{(1)}, s^{(1)})$, $\\dots$, $(t^{(T)}, s^{(T)})$ from this somewhat complicated looking bivariate density. The main thing to figure out for Gibbs are the conditional distributions $\\theta \\mid \\sigma = s, \\text{data}$ and $\\sigma \\mid \\theta = t, \\text{data}$. For $f_{\\theta \\mid \\sigma = s, \\text{data}}(t)$, write\n",
    "\\begin{align*}\n",
    "   f_{\\theta \\mid \\sigma = s, \\text{data}}(t) &\\propto f_{\\theta, \\sigma \\mid \\text{data}}(t, s) \\\\\n",
    "   &\\propto I\\{-80 \\leq t \\leq 80\\} I\\{\\exp(-10) \\leq s \\leq \\exp(10)\\} \\frac{1}{s^{n+1}}  \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right).\n",
    "\\end{align*}\n",
    "$f_{\\theta \\mid \\sigma = s, \\text{data}}(t)$ is a density in the variable $t$ which means that any multiplicative factor not depending on $t$ can be ignored in proportionality. We thus get\n",
    "\\begin{align*}\n",
    "   f_{\\theta \\mid \\sigma = s, \\text{data}}(t) &\\propto f_{\\theta, \\sigma \\mid \\text{data}}(t, s) \\\\\n",
    "   &\\propto I\\{-80 \\leq t \\leq 80\\} \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right).\n",
    "\\end{align*}\n",
    "Using the elementary equality\n",
    "\\begin{align*}\n",
    "   (y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2 = (y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\dots + (y_n - \\bar{y})^2 + n(\\bar{y} - t)^2,\n",
    "\\end{align*}\n",
    "we get\n",
    "\\begin{align*}\n",
    "   f_{\\theta \\mid \\sigma = s, \\text{data}}(t) \n",
    "   &\\propto I\\{-80 \\leq t \\leq 80\\} \\exp \\left(-\\frac{(y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\dots + (y_n - \\bar{y})^2 + n(\\bar{y} - t)^2}{2s^2} \\right) \\\\\n",
    "   &= I\\{-80 \\leq t \\leq 80\\} \\exp \\left(-\\frac{(y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\dots + (y_n - \\bar{y})^2}{2 s^2} \\right) \\exp \\left(-\\frac{n(\\bar{y} - t)^2}{2s^2} \\right)\n",
    "\\end{align*}\n",
    "The multiplicative factor $\\exp \\left(-\\frac{(y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\dots + (y_n - \\bar{y})^2}{2 s^2} \\right)$ does not depend on $t$ and can be ignored in proportionality leading to the simpler expression: \n",
    "\\begin{align*}\n",
    "f_{\\theta \\mid \\sigma = s, \\text{data}}(t) \\propto I\\{-80 \\leq t \\leq 80\\} \\exp \\left(-\\frac{n(\\bar{y} - t)^2}{2s^2} \\right) =  I\\{-80 \\leq t \\leq 80\\} \\exp \\left(-\\frac{(t - \\bar{y})^2}{2\\frac{s^2}{n}} \\right)\n",
    "\\end{align*}\n",
    "In other words, $\\theta \\mid \\sigma = s, \\text{data}$ is the normal distribution $N(\\bar{y}, \\frac{s^2}{n})$ conditioned to lie between $[-80, 80]$. Generating a random sample from this distribution is easy: sample from $N(\\bar{y}, \\frac{s^2}{n})$. Keep the sample unless it lies outside $[-80, 80]$. If it lies outside $[-80, 80]$, just throw this sample away and repeat. Note that, in our dataset, $\\bar{y} = 17.615$ and $s^2/n$ will be generally small, so the samples from $N(\\bar{y}, s^2/n)$ will almost always lie between $-80$ and $80$. \n",
    "\n",
    "For the other conditional $\\sigma \\mid \\theta = t, \\text{data}$, write\n",
    "\\begin{align*}\n",
    "   f_{\\sigma \\mid \\theta = t, \\text{data}}(s) &\\propto f_{\\theta, \\sigma \\mid \\text{data}}(t, s) \\\\\n",
    "   &\\propto I\\{-80 \\leq t \\leq 80\\} I\\{\\exp(-10) \\leq s \\leq \\exp(10)\\} \\frac{1}{s^{n+1}}  \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right).\n",
    "\\end{align*}\n",
    "Now we can ignore the factor $I\\{-80 \\leq t \\leq 80\\}$ which does not depend on $s$. We then get\n",
    "\\begin{align*}\n",
    "   f_{\\sigma \\mid \\theta = t, \\text{data}}(s)\n",
    "   &\\propto  I\\{\\exp(-10) \\leq s \\leq \\exp(10)\\} \\frac{1}{s^{n+1}}  \\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2s^2} \\right).\n",
    "\\end{align*}\n",
    "To parse this density, first of all, we can replace the somewhat complicated indicator function by the simpler $I\\{s > 0\\}$ because $\\exp(-10) \\approx 0$ and $\\exp(10) \\approx \\infty$. The rest of the terms involve a power term and an exponential term resembling a Gamma density. However, this seems like a Gamma density in $1/s^2$ as the exponent involves $1/s^2$. We can convert it back to a regular Gamma density by calculating the density of $1/\\sigma^2$ using the Jacobian formula: \n",
    "\\begin{align*}\n",
    "   f_{\\frac{1}{\\sigma^2} \\mid \\theta = t, \\text{data}}(u) = f_{\\sigma \\mid \\theta = t, \\text{data}}\\left(\\sqrt{\\frac{1}{u}} \\right)\\frac{1}{2u^{3/2}} \\propto I\\{u > 0\\} u^{(n-2)/2}\\exp \\left(-\\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2} u \\right)\n",
    "\\end{align*}\n",
    "We thus get\n",
    "\\begin{align*}\n",
    "   \\frac{1}{\\sigma^2} \\mid \\theta = t, \\text{data} \\sim \\text{Gamma} \\left(\\text{shape}  = \\frac{n}{2}, \\text{rate} = \\frac{(y_1 - t)^2 + (y_2 - t)^2 + \\dots + (y_n - t)^2}{2} \\right)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f601220-4d9c-4499-a8c6-0bbfa7cad374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 15\n",
    "y_obs = np.array([17.62, 17.61, 17.61, 17.62, 17.62, 17.615, 17.615, 17.625, 17.61, 17.62, 17.62, 17.605, 17.61, 17.62, 17.61])\n",
    "T = 4000\n",
    "\n",
    "#Initialize\n",
    "theta_0 = np.mean(y_obs)\n",
    "sigma_0 = np.std(y_obs, ddof = 1)\n",
    "\n",
    "#Create a list to store pairs (theta, sigma)\n",
    "theta_sigma_pairs = [(theta_0, sigma_0)]\n",
    "\n",
    "#Run the Gibbs steps\n",
    "for t in range(1, T+1):\n",
    "    s = theta_sigma_pairs[t-1][1]\n",
    "    theta_t = np.random.normal(np.mean(y_obs), np.sqrt(s**2/n))\n",
    "    deviations = [(y_i - theta_t)**2 for y_i in y_obs]\n",
    "    u_t = np.random.gamma(n/2, 2/sum(deviations))\n",
    "    sigma_t = 1/np.sqrt(u_t)\n",
    "    theta_sigma_pairs.append((theta_t, sigma_t))\n",
    "\n",
    "theta_values = [pair[0] for pair in theta_sigma_pairs]\n",
    "sigma_values = [pair[1] for pair in theta_sigma_pairs]\n",
    "theta_Gibbs = np.array(theta_values)\n",
    "sigma_Gibbs = np.array(sigma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69871a6e-e5f7-46fc-8583-c56d02d674d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our best estimate of theta can be taken to be the mean of the posterior samples for theta:\n",
    "thetamean_Gibbs = np.mean(theta_Gibbs)\n",
    "display(thetamean_Gibbs)\n",
    "#A 95% interval for theta based on the posterior samples is computed as follows:\n",
    "lower_limit_theta = np.percentile(theta_Gibbs, 2.5)\n",
    "upper_limit_theta = np.percentile(theta_Gibbs, 97.5)\n",
    "display([lower_limit_theta, upper_limit_theta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c33710-70b6-4218-afd2-8b82e0eb381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of posterior theta samples for theta\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(theta_Gibbs, bins = 500, color = 'blue')\n",
    "plt.hist(theta_samples, bins = 500, color = 'red')\n",
    "plt.xlabel('Values of theta')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Posterior theta Gibbs samples')\n",
    "plt.show();\n",
    "\n",
    "plt.hist(sigma_Gibbs, bins = 500, color = 'blue')\n",
    "plt.hist(sigma_samples, bins = 500, color = 'red')\n",
    "plt.xlabel('values of sigma')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Posterior sigma Gibbs samples')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee554c9a-c250-423f-bd7c-621ca105d121",
   "metadata": {},
   "source": [
    "An important fact in the above example is that for priors $\\theta \\in \\text{Uniform}[-C, C]$ and $\\log \\sigma \\sim \\text{Uniform}[-C, C])$ when $C \\rightarrow \\infty$, the following is true: \n",
    "\\begin{align*}\n",
    "   \\frac{\\sqrt{n}(\\bar{y} - \\theta)}{\\hat{\\sigma}} \\bigg \\vert \\text{data} \\sim \\mathbf{t}_{n-1}\n",
    "\\end{align*}\n",
    "In other words, the posterior distribution for $\\theta$ (without any conditioning on $\\sigma$) after centering by $\\bar{y}$ and scaling by $\\hat{\\sigma}/\\sqrt{n}$ is the Student $\\mathbf{t}_{n-1}$ distribution. This result is similar to the result on which the usual frequentist inference is based. Thus frequentist and Bayesian inference coincide in this problem. This will change if the prior is altered to be more informative. \n",
    "\n",
    "In PyMC, it might be somewhat painful to provide ranges for the priors that are uninformative. In such cases, there is an alternative prior specification via the \"Flat Distribution\". The Flat distribution should be thought of as $\\text{Uniform}[-C, C]$ as $C \\rightarrow \\infty$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0311a-6895-4c5b-bcb1-62cb8ede0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "n = 15\n",
    "y_obs = np.array([17.62, 17.61, 17.61, 17.62, 17.62, 17.615, 17.615, 17.625, 17.61, 17.62, 17.62, 17.605, 17.61, 17.62, 17.61])\n",
    "measurement_model = pm.Model()\n",
    "with measurement_model:\n",
    "    theta = pm.Flat(\"theta\") #now no range needs to be specified\n",
    "    log_sigma = pm.Flat(\"log_sigma\")\n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.exp(log_sigma))\n",
    "    Y = pm.Normal(\"Y\", mu = theta, sigma = sigma, observed=y_obs)\n",
    "    #Sample from posterior:\n",
    "    idata = pm.sample(2000, chains = 2, return_inferencedata = True) \n",
    "\n",
    "#The posterior samples can be used to estimate theta (and sigma) along with uncertainty quantification as follows: \n",
    "theta_samples = idata.posterior['theta'].values.flatten()\n",
    "log_sigma_samples = idata.posterior['log_sigma'].values.flatten()\n",
    "sigma_samples = np.exp(log_sigma_samples)\n",
    "#Our best estimate of the unknown length can be taken to be the mean of the posterior samples for theta:\n",
    "thetamean = np.mean(theta_samples)\n",
    "display(thetamean)\n",
    "#A 95% interval for theta based on the posterior samples is computed as follows:\n",
    "lower_limit_theta = np.percentile(theta_samples, 2.5)\n",
    "upper_limit_theta = np.percentile(theta_samples, 97.5)\n",
    "display([lower_limit_theta, upper_limit_theta])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dde7e0-bcb9-4a62-a15a-659835d081f4",
   "metadata": {},
   "source": [
    "## Gibbs Sampler in the more general Multivariate Case\n",
    "\n",
    "The Gibbs sampler in the multivariate case also works similarly. Consider the problem of generating samples from a target density $f_{\\text{Target}}(u)$ where $u$ is a multivariate vector. We decompose $u$ as \n",
    "\\begin{align*}\n",
    "   u = (u_{(1)}, \\dots, u_{(k)}) \n",
    "\\end{align*}\n",
    "for $k$ subvectors $u_{(1)}, \\dots, u_{(k)}$. Assume that $f_{\\text{Target}}(u)$ is the joint density of $U_{(1)}, U_{(2)}, \\dots, U_{(k)}$. The Gibbs sampler will produce samples of the form: \n",
    "\\begin{align*}\n",
    "   (u_{(1)}^{(0)}, \\dots, u_{(k)}^{(0)}) \\rightarrow (u_{(1)}^{(1)}, \\dots, u_{(k)}^{(1)}) \\rightarrow (u_{(1)}^{(2)}, \\dots, u_{(k)}^{(2)}) \\rightarrow \\dots \\rightarrow (u_{(1)}^{(T)}, \\dots, u_{(k)}^{(T)})\n",
    "\\end{align*}\n",
    "for a large $T$, by going through the following steps: \n",
    "1. Initialize at arbitrary $u_{(1)}^{(0)}, u_{(2)}^{(0)}, \\dots, u_{(k)}^{(0)}$. Repeat the following for $t = 0, 1, 2, \\dots$\n",
    "2. Given the current sample $(u_{(1)}^{(t)}, u_{(2)}^{(t)}, \\dots, u_{(k)}^{(t)})$, sample $u_{(j)}^{(t+1)}$ for $j = 1, \\dots, k$ in order according to the conditional distribution of $U_{(j)}$ given $U_{(1)} = u_{(1)}^{(t+1)}, \\dots, U_{(j-1)}^{(t+1)} = u_{(j-1)}^{(t+1)}, U_{(j)}^{(t)} = u_{(j)}^{(t)}, \\dots, U_{(k)}^{(t)} = u_{(k)}^{(t)}$.\n",
    "\n",
    "The components $U_{(1)}, \\dots, U_{(k)}$ of $U$ are chosen so that the conditional distributions are easy to sample from. For a somewhat challenging exercise in Gibbs sampling, try to write down the Gibbs sampler for the Gaussian Mixture Models from last lecture. \n",
    "\n",
    "The Gibbs Sampler is an example of a Markov Chain Monte Carlo algorithm. Markov Chain here refers to the fact that the samples $(u_{(1)}^{(t)}, \\dots, u_{(k)}^{(t)})$ generated by the Gibbs Sampler form a Markov Chain. The Gibbs Sampler can be seen as a special case of a more general class of algorithms called Metropolis-Hastings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f198a-fddb-42ed-ae81-0242d2b11d87",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "We shall next study some regression models including linear regression, logistic regression and other generalized linear models (GLMs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751d43c-2c8d-4138-9352-3af9ad6930a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2, multivariate_normal\n",
    "\n",
    "#Pearson Height Data\n",
    "height = pd.read_table(\"PearsonHeightData.txt\")\n",
    "print(height.shape)\n",
    "n = len(height)\n",
    "print(height.head(10))\n",
    "\n",
    "#Plot the Data:\n",
    "plt.scatter(height['Father'], height['Son'], label = 'Data', s = 7, alpha = 1)\n",
    "plt.xlabel('Father Height')\n",
    "plt.ylabel('Son Height')\n",
    "plt.title(\"Pearson's data on the heights of fathers and their sons\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bb161-b3f3-4352-8a14-a37e36a77406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "X = sm.add_constant(height['Father'])\n",
    "print(X.head(10))\n",
    "model = sm.OLS(height['Son'], X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96195fe6-a1fc-4c2f-80f1-d15b456b289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the regression line on the scatterplot of the data:\n",
    "plt.figure(figsize = (13, 9))\n",
    "plt.scatter(height['Father'], height['Son'], label = 'Data', s = 7, alpha = 1)\n",
    "plt.plot(height['Father'], model.predict(X), color = 'red', label = 'Regression Line', linewidth = 0.6)\n",
    "plt.xlabel('Father Height')\n",
    "plt.ylabel('Son Height')\n",
    "plt.title(\"Pearson's data on the heights of fathers and their sons\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415f85d-484d-4389-b559-8be91b5be267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Standard Error:\n",
    "residuals = height['Son'] - model.predict(X)\n",
    "sighat = np.sqrt(np.sum(residuals**2) / (n - 2))\n",
    "print(sighat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c83e60-fb0e-4521-90ac-da30dff1ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Errors of Regression Coefficient Estimates: \n",
    "X_matrix = np.column_stack([np.ones(n), height['Father']])\n",
    "Sigma_mat = (sighat**2) * np.linalg.inv(X_matrix.T @ X_matrix)\n",
    "stderrs = np.sqrt(np.diag(Sigma_mat))\n",
    "print(\"Standard Errors:\", stderrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315da4c5-05e9-460b-aec6-371ee535b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Linear Regression through PyMC\n",
    "linregmod = pm.Model()\n",
    "with linregmod:\n",
    "    # Priors for unknown model parameters\n",
    "    b0 = pm.Flat(\"b0\")\n",
    "    b1 = pm.Flat(\"b1\")\n",
    "    log_sigma = pm.Flat(\"log_sigma\")\n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.exp(log_sigma))\n",
    "    # Expected value of outcome\n",
    "    mu = b0 + b1 * height['Father']\n",
    "    # Likelihood\n",
    "    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=height['Son'])\n",
    "    idata = pm.sample(2000, chains = 2, return_inferencedata = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4784c-37fa-466f-9822-90a8767d461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_samples = idata.posterior['b0'].values.flatten()\n",
    "b1_samples = idata.posterior['b1'].values.flatten()\n",
    "log_sigma_samples = idata.posterior['log_sigma'].values.flatten()\n",
    "sigma_samples = np.exp(log_sigma_samples)\n",
    "\n",
    "display([np.mean(b0_samples), np.std(b0_samples)])\n",
    "display([np.mean(b1_samples), np.std(b1_samples)])\n",
    "display([np.mean(sigma_samples)])\n",
    "#These numbers should be close to the numbers reported in the regression summary i.e., the Bayesian solution gives very similar answers to the frequentist solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b7da7-635d-4200-80bc-167ed26152ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing posterior variability\n",
    "N = 200\n",
    "# Plotting the data, regression line, and posterior samples\n",
    "plt.figure(figsize = (13, 9))\n",
    "plt.scatter(height['Father'], height['Son'], label='Data', alpha=1, s = 7)\n",
    "for k in range(N):\n",
    "    plt.plot(height['Father'], b0_samples[k] + b1_samples[k]*height['Father'], color='blue', alpha=0.2)\n",
    "plt.plot(height['Father'], model.predict(X), color='red', label='Regression Line', linewidth = 1)\n",
    "plt.xlabel('Father Height')\n",
    "plt.ylabel('Son Height')\n",
    "plt.title(\"Height Data with Posterior Samples\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Generally the posterior in linear regression will be quite narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2a5ea-b7a3-47ac-a389-a4a49102a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#US Population Example:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcfe9a5-0493-4148-9451-4d01d1d50aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "uspop_raw = pd.read_csv(\"POPTHM.csv\")\n",
    "print(uspop_raw.head(15))\n",
    "print(uspop_raw.tail(15))\n",
    "print(uspop_raw.shape)\n",
    "#Monthly Data downloaded from FRED. \n",
    "#Data given for each month equals the average of the estimated population on the first day of the month\n",
    "#and the first day of the next month. \n",
    "#The units are thousands of dollars so 200,000 actually refers to 200 million. \n",
    "\n",
    "# Plot raw data\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.plot(uspop_raw['POPTHM'])\n",
    "plt.ylabel('Population (in thousands)')\n",
    "plt.xlabel('Months since start')\n",
    "plt.title('US Population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf6166-bcff-41dc-b7c6-9b30cb5b92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression of Population with Time\n",
    "time = np.arange(1, uspop_raw.shape[0] + 1)\n",
    "X = sm.add_constant(time)\n",
    "lin_model = sm.OLS(uspop_raw['POPTHM'], X).fit()\n",
    "print(lin_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3331ec-b3f2-4d15-9892-211cec7e7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data with the fitted regression line:\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.plot(time, uspop_raw['POPTHM'], label=\"Data\", color=\"black\")\n",
    "plt.plot(time, lin_model.fittedvalues, color=\"red\", label=\"Fitted\")\n",
    "plt.ylabel('US Population')\n",
    "plt.xlabel('Time (months)')\n",
    "plt.title('Population of the United States')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d2fbf-f4f7-46cb-baba-26b77f454aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate of sigma\n",
    "n = uspop_raw.shape[0]\n",
    "sighat = np.sqrt(sum(lin_model.resid**2) / (n-2))\n",
    "#Standard Errors of estimates:\n",
    "stderrs = lin_model.bse\n",
    "\n",
    "# Compute variance-covariance matrix\n",
    "X_matrix = np.column_stack([np.ones(len(time)), time])\n",
    "Sigma_mat = sighat**2 * inv(X_matrix.T @ X_matrix)\n",
    "\n",
    "# Print relevant statistics\n",
    "print(\"Residual Standard Error:\", sighat)\n",
    "print(\"Standard Errors from Model:\", stderrs)\n",
    "print(\"Variance-Covariance Matrix:\", Sigma_mat)\n",
    "print(\"Standard Errors from Matrix:\", np.sqrt(np.diag(Sigma_mat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7243c0-16b1-469f-98b9-dd3aa603afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Inference for Regression\n",
    "linregmod = pm.Model()\n",
    "with linregmod:\n",
    "    # Priors for unknown model parameters\n",
    "    b0 = pm.Flat(\"b0\")\n",
    "    b1 = pm.Flat(\"b1\")\n",
    "    log_sigma = pm.Flat(\"log_sigma\")\n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.exp(log_sigma))\n",
    "    # Expected value of outcome\n",
    "    mu = b0 + b1 * time\n",
    "    # Likelihood\n",
    "    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=uspop_raw['POPTHM'])\n",
    "    idata = pm.sample(3000, chains = 2, return_inferencedata = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571ef1b-6289-4c9c-9f9a-88eefb7d4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_samples = idata.posterior['b0'].values.flatten()\n",
    "b1_samples = idata.posterior['b1'].values.flatten()\n",
    "log_sigma_samples = idata.posterior['log_sigma'].values.flatten()\n",
    "sigma_samples = np.exp(log_sigma_samples)\n",
    "\n",
    "display([np.mean(b0_samples), np.std(b0_samples)])\n",
    "display([np.mean(b1_samples), np.std(b1_samples)])\n",
    "display([np.mean(sigma_samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cdc3e2-61df-4295-a32d-585de672146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing posterior variability\n",
    "N = 200\n",
    "# Plotting the data, regression line, and posterior samples\n",
    "plt.figure(figsize = (13, 9))\n",
    "plt.plot(time, uspop_raw['POPTHM'], label='Data', color = 'black')\n",
    "for k in range(N):\n",
    "    plt.plot(time, b0_samples[k] + b1_samples[k]*time, color='blue', alpha=0.2)\n",
    "plt.plot(time, lin_model.predict(X), color='red', label='Regression Line', linewidth = 1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('US Population')\n",
    "plt.title(\"Population of the United States\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff02e1-7490-4811-8773-f072072a7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a simulated example\n",
    "n = 400\n",
    "x = np.arange(1, n+1)\n",
    "sig = 1000\n",
    "dt = 5 + 0.8 * ((x-(n/2)) ** 2) + sig * np.random.randn(n)\n",
    "plt.figure(figsize = (13, 9))\n",
    "plt.plot(x, dt)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Data\")\n",
    "plt.title(\"A simulated data for regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5462ba5-6e58-4147-943b-e68a5393983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting a line to the above data\n",
    "X = sm.add_constant(x)\n",
    "lin_model = sm.OLS(dt, X).fit()\n",
    "print(lin_model.summary())\n",
    "residuals = dt - lin_model.predict(X)\n",
    "sighat = np.sqrt(np.sum(residuals**2) / (n - 2))\n",
    "print(sighat)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(x, dt)\n",
    "plt.plot(x, lin_model.fittedvalues, color = \"red\", label = \"Linear Fit\", linewidth = 0.8)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Data\")\n",
    "plt.title(\"A simulated data for regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6e0db-a61e-46b5-a9c6-e993eafe2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Linear Regression through PyMC\n",
    "linregmod = pm.Model()\n",
    "with linregmod:\n",
    "    # Priors for unknown model parameters\n",
    "    b0 = pm.Flat(\"b0\")\n",
    "    b1 = pm.Flat(\"b1\")\n",
    "    log_sigma = pm.Flat(\"log_sigma\")             \n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.exp(log_sigma))\n",
    "    # Expected value of outcome\n",
    "    mu = b0 + b1 * x\n",
    "    # Likelihood\n",
    "    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=dt)\n",
    "    idata = pm.sample(2000, chains = 2, return_inferencedata = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68010eb3-3249-49b7-aff0-91db5221a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_samples = idata.posterior['b0'].values.flatten()\n",
    "b1_samples = idata.posterior['b1'].values.flatten()\n",
    "N = 200\n",
    "plt.figure(figsize = (13, 9))\n",
    "plt.plot(x, dt)\n",
    "for k in range(N):\n",
    "    plt.plot(x, b0_samples[k] + b1_samples[k] * x, color='blue', alpha=0.2)\n",
    "plt.plot(x, lin_model.fittedvalues, color = \"red\", label = \"Linear Fit\", linewidth = 1.2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Data\")\n",
    "plt.title(\"A simulated data for regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694e94c-b89b-4d1c-b095-1f82a50809a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us fit a quadratic function to this dataset:\n",
    "X = sm.add_constant(np.column_stack([x, x**2]))  # Adds constant term (for intercept) and quadratic term\n",
    "quad_model = sm.OLS(dt, X).fit()\n",
    "print(quad_model.summary())\n",
    "residuals = dt - quad_model.predict(X)\n",
    "sighat = np.sqrt(np.sum(residuals**2) / (n - 3))\n",
    "print(sighat)\n",
    "\n",
    "#Plotting\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.plot(x, dt, label=\"Data\", color=\"black\")\n",
    "plt.plot(x, quad_model.fittedvalues, color=\"red\", label=\"Quadratic Fit\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Data\")\n",
    "plt.title(\"A Simulated Dataset\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7e45b-ac86-4e5e-bbb6-758f1513d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian quadratic fitting:\n",
    "quadregmod = pm.Model()\n",
    "with quadregmod:\n",
    "    # Priors for unknown model parameters\n",
    "    b0 = pm.Flat(\"b0\")\n",
    "    b1 = pm.Flat(\"b1\")\n",
    "    b2 = pm.Flat(\"b2\")\n",
    "    log_sigma = pm.Flat(\"log_sigma\")             \n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.exp(log_sigma))\n",
    "    # Expected value of outcome\n",
    "    mu = b0 + b1 * x + b2 * (x ** 2)\n",
    "    # Likelihood\n",
    "    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=dt)\n",
    "    idata = pm.sample(2000, chains = 2, return_inferencedata = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d0b90-5c4b-4183-8303-c650acb0f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_samples = idata.posterior['b0'].values.flatten()\n",
    "b1_samples = idata.posterior['b1'].values.flatten()\n",
    "b2_samples = idata.posterior['b2'].values.flatten()\n",
    "N = 200\n",
    "plt.figure(figsize = (13, 9))\n",
    "plt.plot(x, dt)\n",
    "for k in range(N):\n",
    "    plt.plot(x, b0_samples[k] + b1_samples[k] * x + b2_samples[k] * (x ** 2), color='blue', alpha=0.2)\n",
    "plt.plot(x, quad_model.fittedvalues, color = \"red\", label = \"Linear Fit\", linewidth = 1.2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Data\")\n",
    "plt.title(\"A simulated data for regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8554c-04bb-4de3-999a-952248a55827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another illustration of linear regression\n",
    "sales_raw = pd.read_csv(\"MRTSSM4453USN.csv\")\n",
    "#This dataset gives monthly data on beer, wine and liquor sales in the US from January 1992 to August 2022\n",
    "print(sales_raw.tail())\n",
    "\n",
    "dt = sales_raw.iloc[:, 1].values\n",
    "\n",
    "# Plot the data\n",
    "time = np.arange(1, len(dt) + 1)\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.plot(time, dt, label=\"Data\", color=\"black\")\n",
    "plt.ylabel(\"Millions of Dollars\")\n",
    "plt.title(\"Retail Sales: Beer, Wine and Liquor Stores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e804a26-a75e-4cb9-82ae-99701653413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct harmonic regression terms and quadratic terms\n",
    "t = time\n",
    "d = 12\n",
    "v1 = np.cos(2 * np.pi * 1 * t / d)\n",
    "v2 = np.sin(2 * np.pi * 1 * t / d)\n",
    "v3 = np.cos(2 * np.pi * 2 * t / d)\n",
    "v4 = np.sin(2 * np.pi * 2 * t / d)\n",
    "v5 = np.cos(2 * np.pi * 3 * t / d)\n",
    "v6 = np.sin(2 * np.pi * 3 * t / d)\n",
    "v7 = np.cos(2 * np.pi * 4 * t / d)\n",
    "v8 = np.sin(2 * np.pi * 4 * t / d)\n",
    "v9 = t\n",
    "v10 = t**2\n",
    "\n",
    "# Fit the regression model\n",
    "X = sm.add_constant(np.column_stack([v1, v2, v3, v4, v5, v6, v7, v8, v9, v10]))\n",
    "lin_mod = sm.OLS(dt, X).fit()\n",
    "print(lin_mod.summary())\n",
    "\n",
    "# Plot original and fitted data\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.plot(t, dt, label=\"Data\", color=\"black\")\n",
    "plt.plot(t, lin_mod.fittedvalues, color=\"red\", label=\"Fitted\")\n",
    "plt.xlabel(\"Time (months)\")\n",
    "plt.ylabel(\"Sales Data\")\n",
    "plt.title(\"Retail Sales: Beer, Wine and Liquor Stores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
