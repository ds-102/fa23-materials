{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155f5c1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab06.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf14851",
   "metadata": {},
   "source": [
    "# Lab 6: Model Selection and Uncertainty Quantification with GLMs\n",
    "Welcome to the Data 102 Lab 6. In this lab, we are going to continue our discussion of GLMs, and give you the opportunity to practice concepts around model selection and uncertainty quantification with GLMs.\n",
    "\n",
    "#### The code and responses you need to write are are represented by `...`. There is additional documentation for each part as you go along.\n",
    "\n",
    "##### Please read the introduction and the instructions to each problem carefully.\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53c2ec",
   "metadata": {},
   "source": [
    "**Collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e3d65",
   "metadata": {},
   "source": [
    "## Submission\n",
    "See the [Gradescope Submission Guidelines](https://edstem.org/us/courses/42657/discussion/3350112) for details on how to submit your lab.\n",
    "\n",
    "**For full credit, this assignment should be completed and submitted before Wednesday, October 11th, 2023 at 11:59 PM PST.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38647e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import hashlib\n",
    "from scipy.stats import poisson, norm, gamma, uniform, multivariate_normal\n",
    "import statsmodels.api as sm\n",
    "import arviz as az\n",
    "  \n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d74cf",
   "metadata": {},
   "source": [
    "# Atlantic Hurricane Season\n",
    "\n",
    "As in Lab 5 Question 2, we will be exploring hurricane data from the 2020 Atlantic hurrican season. Just like in the last lab, we're investigating the number of named storms recorded since 1880 and examining the relationship between rising Sea Surface Temperature (SST) and the frequency of named storms.\n",
    "\n",
    "For this lab we extracted the number of tropical storms from the [HURDAT Database](https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2019-052520.txt). We also extracted data on Sea Surface Temperatures from the [National Center for Atmosferic Research](https://climatedataguide.ucar.edu/climate-data/global-surface-temperature-data-gistemp-nasa-goddard-institute-space-studies-giss). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7539fdf",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The `Num_Storms` column contains the number of named storms recorded each year between 1880 and 2019. The `Temp_Anomaly` column contains the deviation in yearly SST from the mean of 1951-1980."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run the code to load the data\n",
    "data_source = \"hurricane_data.csv\"\n",
    "df = pd.read_csv(data_source)\n",
    "df = df[[\"Year\", \"Num_Storms\", \"Temp_Anomaly\"]]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719d329",
   "metadata": {},
   "source": [
    "## Review: Model Selection Metrics\n",
    "\n",
    "In lab 5, we spent some time learning how to build, interpret, and implement GLMs from a frequentist and bayesian perspective. Here, we expand on that discussion by considering **model selection**: how do we compare the models that we've built?\n",
    "\n",
    "In class, we looked at a few metrics that can help us decide on a model to use: \n",
    "\n",
    "### Log-likelihood:  $\\log{p(y|x,\\beta)}$\n",
    "This is simply the log of the likelihood that we're maximizing when we fit a GLM. The motivation behind using this value comes from a simple idea: the more probable our data is under our model (the higher the log-likelihood), the better it explains the data. This metric is generally best when comparing models with similar numbers of parameters.\n",
    "\n",
    "### Deviance: $(-2) \\times \\text{log-likelihood}(\\text{model}) + \\text{constant}$\n",
    "While you don't need to know the exact details of how this is calculated, the deviance is a measure of how much worse our model is relative to a perfect, or saturated model (i.e. one that perfectly predicts every point). Generally, the lower the deviance, the better the model.\n",
    "\n",
    "### Penalized Likelihood Information Criteria (AIC and BIC)\n",
    "We also introduced the idea of penalized likelihood information criteria with the **Akaike Information Criteria (AIC)** and the **Bayes Information Criteria (BIC)**:\n",
    "$$\n",
    "\\text{AIC}(\\text{model}) = (-2) \\times \\text{log-likelihood}(\\text{model}) + 2 \\times \\text{number of parameters in model}\n",
    "$$\n",
    "$$\n",
    "\\text{BIC}(\\text{model}) = (-2) \\times \\text{log-likelihood}(\\text{model}) + (\\log n) \\times \\text{number of parameters in model}\n",
    "$$\n",
    "The reason why we prefer using metrics like the AIC and BIC is that they penalize models with more parameters. If a model utilizes many features to make a decision, this could lead to overfitting. This would cause an increase in the log-likelihood of the data, even though the model may be a poor representation of the relationship we're trying to understand. As with deviance, smaller AIC and BIC values mean better models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c909f7",
   "metadata": {},
   "source": [
    "## Model Fitting and Selection\n",
    "\n",
    "For this part of the lab, we'll consider three models for this dataset: \n",
    "\n",
    "1. Linear regression model for Storms on SST\n",
    "2. Linear regression model for log(Storms) on SST\n",
    "3. Poisson regression model for Storms on SST\n",
    "4. Negative Binomial regression model for Storms on SST\n",
    "\n",
    "Using the metrics we've learned from class, we can compare and contrast these models against one another, and reach a decision about which best fits our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afa4f3",
   "metadata": {},
   "source": [
    "Before we begin, let's first look at a scatterplot of `Num_Storms` over `Temp_Anomaly` (SST) to get a rough understanding of the relationship between the two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Num_Storms']\n",
    "X = df['Temp_Anomaly']\n",
    "plt.scatter(X, Y, s = 10)\n",
    "#plt.plot([min(poireg_fitted_values), max(poireg_fitted_values)],[min(poireg_fitted_values), max(poireg_fitted_values)], color = 'red', linestyle = '--')\n",
    "plt.xlabel('SST')\n",
    "plt.ylabel('Number of Storms')\n",
    "plt.title('Storms plotted against SST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd2ff7",
   "metadata": {},
   "source": [
    "There seems to be a clear increasing relationship between the two variables. By fitting models, we can obtain a rigorous quantitative understanding of the relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7012333",
   "metadata": {},
   "source": [
    "## Model 1: Linear Regression for Storms on SST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631b948",
   "metadata": {},
   "source": [
    "### 1a) Fit the model\n",
    "\n",
    "In the cell below, use `statsmodels.api` to fit a linear regression (with intercept) for Storms on SST.\n",
    "\n",
    "**Hint**: To add an intercept, `sm.add_constant` might come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f1f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model ONE: linear regression for Storms on SST\n",
    "model_one = ...\n",
    "print(model_one.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733f857",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd39aa",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1b) Interpreting our Model\n",
    "\n",
    "Using the model summary you produced in `q1a`, interpret the `Temp_Anomaly` coefficient in the context of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cce715",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f013c3",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Now that we've fit our model, let's visually inspect its fit! As we continue building and comparing models, it's important to visually confirm the quality of our models, beyond just using metrics to compare one another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adcb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the data along with the regression equation: \n",
    "b0_one, b1_one = model_one.params.values\n",
    "x_vals = np.linspace(df['Temp_Anomaly'].min(), df['Temp_Anomaly'].max(), 100)\n",
    "y_vals_one = b0_one + b1_one * x_vals\n",
    "plt.scatter(df['Temp_Anomaly'], Y, s = 10)\n",
    "plt.plot(x_vals, y_vals_one, color='red',label=f'y = {b0_one:.2f} + {b1_one:.2f}x')\n",
    "plt.legend()\n",
    "plt.xlabel('SST')\n",
    "plt.ylabel('Number of Storms')\n",
    "plt.title('Storms plotted against SST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14dc07a",
   "metadata": {},
   "source": [
    "### 1c) Calculating AIC\n",
    "\n",
    "Now, let's find the AIC for our model for purposes of comparison. Fill out the cell below with the value of the AIC for this model.\n",
    "\n",
    "**Hint**: This one's pretty straightforward: you can calculate it using the log-likelihood, or use a model attribute to get this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e486db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aic_one = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da4f0b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f080cd",
   "metadata": {},
   "source": [
    "## Model 2: Linear Regression for $\\log(\\text{Storms})$ on SST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45bdcf",
   "metadata": {},
   "source": [
    "### 2a) Fit the model\n",
    "\n",
    "In the cell below, use `statsmodels.api` to fit a linear regression (with intercept) for $\\log(\\text{Storms})$ on SST.\n",
    "\n",
    "**Hint**: To add an intercept, `sm.add_constant` might come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043cc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_two = ...\n",
    "print(model_two.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8dbe7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f7b87c",
   "metadata": {},
   "source": [
    "Based on the output, linear regression fitted the model equation: $\\log(y) = 2.1645 + 0.5197 x$. The interpretation of the coefficient 0.5197 is the following: every unit increase in SST leads to an increase of 0.5197 in the average number of $\\log(\\text{storms})$. In the next figure, we plot the linear regression equation fitted by Model One with the equation $y = \\exp(2.1645 + 0.5197 x)$ fitted by Model Two. \n",
    "\n",
    "Let's again visualize the model we've outputted, and compare it visually to the first model we built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the data along with the regression equations: \n",
    "b0_two, b1_two = model_two.params.values\n",
    "x_vals = np.linspace(df['Temp_Anomaly'].min(), df['Temp_Anomaly'].max(), 100)\n",
    "y_vals_two = np.exp(b0_two + b1_two * x_vals)\n",
    "plt.scatter(df['Temp_Anomaly'], Y, s = 10)\n",
    "plt.plot(x_vals, y_vals_one, color='red',label=f'Model One: y = {b0_one:.2f} + {b1_one:.2f}x')\n",
    "plt.plot(x_vals, y_vals_two, color='green',label=f'Model Two: y = exp({b0_two:.2f} + {b1_two:.2f}x)')\n",
    "plt.legend()\n",
    "plt.xlabel('SST')\n",
    "plt.ylabel('Number of Storms')\n",
    "plt.title('Storms plotted against SST')\n",
    "plt.show()\n",
    "#The two fitted functions clearly look similar even though the model two fit is a bit lower than model one fit in the middle of the range of SST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47206aa6",
   "metadata": {},
   "source": [
    "### 2c) Calculating AIC\n",
    "\n",
    "The AIC for this model is given by 149.2 which seems substantially smaller than the AIC for Model one (which was 763.1). Actually, these two AICs are not comparable because the first model treats $y_1, \\dots, y_n$ as the dataset while the second model treats $\\log(y_1), \\dots \\log(y_2)$ as the dataset. To make the AICs comparable, we can convert the second AIC so it applies to $y_1, \\dots, y_n$. For this, we can use the density change of variable formula: \n",
    "\\begin{align*}\n",
    "   f_{Y}(y) = f_{\\log Y}(\\log y) \\left| \\frac{d}{dy} \\log y \\right| = f_{\\log Y}(\\log y) \\frac{1}{y}.\n",
    "\\end{align*}\n",
    "Thus\n",
    "\\begin{align*}\n",
    "  \\text{likelihood for } Y_1, \\dots, Y_n = \\left(\\text{likelihood for } \\log Y_1, \\dots, \\log Y_n \\right) \\times \\frac{1}{y_1} \\frac{1}{y_2} \\dots \\frac{1}{y_n}\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "   & \\text{AIC for model two in terms of } Y_1, \\dots, Y_n \\\\ \n",
    "   &= -2  \\left(\\text{log-likelihood for } Y_1, \\dots, Y_n \\right) + 2 \\times \\text{number of parameters} \\\\\n",
    "   &= -2  \\left(\\text{log-likelihood for } \\log Y_1, \\dots, \\log Y_n \\right) + 2 \\times \\text{number of parameters}  - 2 \\log \\left(\\frac{1}{y_1} \\frac{1}{y_2} \\dots \\frac{1}{y_n}\\right) \\\\\n",
    "   &= \\left(\\text{AIC for model two in terms of } \\log Y_1, \\dots, \\log Y_n \\right)+ 2 \\sum_{i=1}^n \\log y_i.\n",
    "\\end{align*}\n",
    "Using this derivation, calculate the aic of the model in the cell below.\n",
    "\n",
    "**Note**: To avoid rounding errors and pass the autograder test, you'll want to use the `model.aic` attribute to access the exact AIC value for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d09e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aic_two = ...\n",
    "aic_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c65f85",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece633c",
   "metadata": {},
   "source": [
    "Now, when we compare model one's AIC and model two's AIC, we can see that Model One has a (slightly) lower AIC compared to Model Two. Even though the two models seem to give similar fits, we can prefer Model One because of the smaller AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da82499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare with aic_one\n",
    "print([aic_one, aic_two])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f13b1",
   "metadata": {},
   "source": [
    "## Model 3: Poisson Regression for Storms on SST\n",
    "\n",
    "The Poisson regression model is given by \n",
    "\n",
    "$$Y_i \\sim \\text{Poisson}(\\mu_i)$$\n",
    "\n",
    "$$\\mu_i = \\exp(\\beta_0 + \\beta_1 X_i)$$ \n",
    "\n",
    "and $X_i$ is the SST deviation in Year $i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3611237",
   "metadata": {},
   "source": [
    "### 3a) Fit the model\n",
    "\n",
    "In the cell below, use `statsmodels.api` to fit a poisson regression (with intercept) for Storms on SST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c574ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_three = ...\n",
    "print(model_three.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9894ece",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2267941",
   "metadata": {},
   "source": [
    "### 3b) Comparing Models 2 and 3\n",
    "The fitted coefficients give estimates for $\\beta_0$ and $\\beta_1$. The estimate 0.4866 for $\\beta_1$ implies that for unit increase in 'Temp_Anomaly', the mean number of storms increases by a factor of $\\exp(0.4866) \\approx 1.63$ (or equivalently, by 63%). Thus, clearly, increased SST is related to more storms. The p-value for SST is close to 0 indicating statistical significance. \n",
    "\n",
    "Take a moment to compare the coefficients you got for Model 2 and Model 3. They seem very similar! This begs the question: how are these two models any different?\n",
    "\n",
    "**Choose the option that best fills in the blanks in the following statements. For the second statement, you may use the same choice multiple times:**\n",
    "\n",
    "The mathematic models fit by model 2 and 3 are \\_\\_\\_**(1)**\\_\\_\\_.\n",
    "\n",
    "- **A.** Different\n",
    "- **B.** The same\n",
    "\n",
    "Mathematically, we can represent the model 2 as fitting the equation \\_\\_\\_**(2)**\\_\\_\\_ whereas we can represent model 3 as fitting the equation \\_\\_\\_**(3)**\\_\\_\\_.\n",
    "\n",
    "- **A.** $\\mathbb{E} (\\log Y) = \\beta_0 + \\beta_1 X_i$\n",
    "- **B.** $\\log \\mathbb{E} (Y) = \\beta_0 + \\beta_1 X_i$\n",
    "\n",
    "Your answer should be strings, assigning either `\"A\"` or `\"B\"` to each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca491c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blank_1 = ...\n",
    "blank_2 = ...\n",
    "blank_3 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc0147",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3cf1c1",
   "metadata": {},
   "source": [
    "Again, let's plot all three regression lines in one figure and compare them visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826cb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the data along with the regression equation: \n",
    "b0_three, b1_three = model_three.params.values\n",
    "x_vals = np.linspace(df['Temp_Anomaly'].min(), df['Temp_Anomaly'].max(), 100)\n",
    "y_vals_three = np.exp(b0_three + b1_three * x_vals)\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.scatter(df['Temp_Anomaly'], Y)\n",
    "plt.plot(x_vals, y_vals_one, color='red',label=f'y = {b0_one:.2f} + {b1_one:.2f}x (linear regression of y on x)')\n",
    "plt.plot(x_vals, y_vals_two, color='green',label=f'y = exp({b0_two:.2f} + {b1_two:.2f}x) (linear regression of log(y) on x)')\n",
    "plt.plot(x_vals, y_vals_three, color='black',label=f'y = exp({b0_three:.2f} + {b1_three:.2f}x) (Poisson regression of y on x)')\n",
    "plt.legend()\n",
    "plt.xlabel('SST')\n",
    "plt.ylabel('Number of Storms')\n",
    "plt.title('Storms plotted against SST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c22b91",
   "metadata": {},
   "source": [
    "From the above figure, we can see that near the middle of the range of SST, there is little difference between linear regression and Poisson regression. However, near the extremes, Poisson regression gives larger fits compared to linear regression. The model two fitted line is somewhat below the other two fits in the middle region. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b664da",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3c) Is a Change of Variable Needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641552c6",
   "metadata": {},
   "source": [
    "Given your answer to `q3b`, when we do our AIC calculation, do we need to use the density change of variable formula as in `q2c`? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f63a5",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2fa858",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3d) Calculating AIC\n",
    "\n",
    "Fill out the cell below with the value of the AIC for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1ba1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aic_three = ...\n",
    "aic_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a884155",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfdd03",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([aic_one, aic_two, aic_three])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c0040",
   "metadata": {},
   "source": [
    "So Model three has the smaller AIC compared to models one and two. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f246002f",
   "metadata": {},
   "source": [
    "## Model 4: Negative Binomial Regression for Y on X\n",
    "\n",
    "The Negative Binomial regression model is given by \n",
    "\n",
    "$$Y_i \\sim \\text{NB}(\\mu_i, \\alpha)$$\n",
    "\n",
    "$$\\mu_i = \\exp(\\beta_0 + \\beta_1 X_i)$$ \n",
    "\n",
    "Here $NB(\\mu_i, \\alpha)$ is the Negative Binomial distribution with mean $\\mu_i$ and overdispersion parameter $\\alpha$ ($NB(\\mu, \\alpha)$ and $\\text{Poisson}(\\mu)$ are very close to each other when $\\alpha$ is close to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc54e28",
   "metadata": {},
   "source": [
    "### 4a) Fit the model\n",
    "\n",
    "In the cell below, use `statsmodels.api` to fit a negative binomail regression (with intercept) for Storms on SST.\n",
    "\n",
    "**Note**: While this can be fit in two ways in statsmodels (`sm.GLM(..., family = sm.families.NegativeBinomial()).fit()` and `sm.NegativeBinomial(...).fit()`), these methods differ slightly in their treatment of $\\alpha$. The latter function also treats $\\alpha$ as an unknown parameter and estimates it, including a row in the output for the parameter $\\alpha$. The former function expects to be given a value of $\\alpha$ and takes $\\alpha = 1$ as default if not supplied. **For this question, use `sm.NegativeBinomial(...).fit()` to pass the tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68cae74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_four = ...\n",
    "print(model_four.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa7588",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435d2f6",
   "metadata": {},
   "source": [
    "This model seems to give essentially the same coefficient estimates as Poisson regression. The overdispersion parameter $\\alpha$ is estimated by a small number (0.029) (note $NB(\\mu, \\alpha) \\approx \\text{Poisson}(\\mu)$ when $\\alpha \\approx 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77eb3ed",
   "metadata": {},
   "source": [
    "### 4b) Calculating AIC\n",
    "\n",
    "Once again, let's find the AIC for our model for purposes of comparison. Fill out the cell below with the value of the AIC for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c966d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aic_four = ...\n",
    "aic_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f204992",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33853b7d",
   "metadata": {},
   "source": [
    "All the four AIC are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AIC of model one: {aic_one}\")\n",
    "print(f\"AIC of model two: {aic_two}\")\n",
    "print(f\"AIC of model three: {aic_three}\")\n",
    "print(f\"AIC of model four: {aic_four}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db64e4",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4c) Choosing the best model\n",
    "\n",
    "Given the AICs that you calculated, which model is the best fit for our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef975b71",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556e3d4",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Model Checking and Posterior Predictive Distributions\n",
    "\n",
    "So far in this lab, we've talked through issues of *model selection*. Now, we will pivot to viewing GLMs from a Bayesian perspective, and check if our model is right for the task at hand. \n",
    "\n",
    "To do this, remember that frequentist models are equivalent to Bayesian ones with a flat prior ($\\text{uniform}[-C, C]$ for $C \\rightarrow \\infty$). That means that we can find a posterior for these models, and eventually find **posterior predictive distributions** for our data (more on that in the next part of this question). With these distributions, we can then perform a series of sanity checks to make sure that our model is at least a plausible explanation for the underlying relationship we see in the data.\n",
    "\n",
    "## Posterior Normal Approximation\n",
    "\n",
    "But how do we find the posterior here? Usually, as in labs 4 and 5, we can use approximate inference to get empirical posteriors. Here, we'll use a different approximation technique we've covered in lecture 11: **posterior normal approximation**.\n",
    "\n",
    "To describe this, let us focus on Poisson regression for concreteness where the parameters are $\\beta_0$ and $\\beta_1$ (the analysis for negative binomial is entirely analogous). We assume a flat prior ($\\text{uniform}[-C, C]$ for $C \\rightarrow \\infty$). The posterior distribution for $\\beta_0, \\beta_1$ is then approximated by the multivariate normal distribution with mean given by the MLE and the covariance given by the negative of the inverse of the Hessian matrix of the log-likelihood evaluated at the MLE. If that sounded like a lot, don't worry: **this covariance matrix is given by the output of statsmodels!** For example, for model 3, the mean and covariance of the posterior normal distribution can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6773bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_normal_mean = model_three.params.values  #this is the mean of the posterior normal approximation\n",
    "print(posterior_normal_mean)\n",
    "posterior_normal_covariance = model_three.cov_params() #this is the covariance matrix of the posterior normal approximation\n",
    "print(posterior_normal_covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6aeca",
   "metadata": {},
   "source": [
    "The marginal normal densities corresponding to this multivariate (in this case, bivariate) normal density can be plotted as follows. These give the marginal posterior densities for $\\beta_0$ and $\\beta_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff96bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "mu_0, mu_1 = posterior_normal_mean\n",
    "V_0, V_1 = np.diag(posterior_normal_covariance)\n",
    "\n",
    "# Generate separate x values for plotting each density\n",
    "x_0 = np.linspace(mu_0 - 3*np.sqrt(V_0), mu_0 + 3*np.sqrt(V_0), 1000)\n",
    "x_1 = np.linspace(mu_1 - 3*np.sqrt(V_1), mu_1 + 3*np.sqrt(V_1), 1000)\n",
    "\n",
    "# Initialize subplots\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plot the first normal density\n",
    "ax[0].plot(x_0, norm.pdf(x_0, mu_0, np.sqrt(V_0)), color='blue', label=f'N({mu_0:.4f}, {V_0:.4f})')\n",
    "ax[0].set_title(f'Posterior for beta_0: N({mu_0:.4f}, {V_0:.4f})')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the second normal density\n",
    "ax[1].plot(x_1, norm.pdf(x_1, mu_1, np.sqrt(V_1)), color='red', label=f'N({mu_1:.4f}, {V_1:.4f})')\n",
    "ax[1].set_title(f'Posterior for beta_1: N({mu_1:.4f}, {V_1:.4f})')\n",
    "ax[1].legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4709d37",
   "metadata": {},
   "source": [
    "**Note**: Under our flat prior, Bayesian analysis with this normal posterior gives **exactly the same results** as frequentist analysis. For example, posterior mean, median or mode coincide with the MLE. Posterior standard deviations coincide with frequentist standard errors. Credible intervals coincide with confidence intervals and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1fd1c7",
   "metadata": {},
   "source": [
    "### Sampling from the posterior\n",
    "\n",
    "Now, to properly set ourselves up to do some posterior predictive checks, we'll need to create a function to draw samples from our approximate posteriors on $\\beta_0$ and $\\beta_1$. Fill in the function below to sample from our posterior distribution!\n",
    "\n",
    "**Hint**: Given that we're using a normal approximation of our posterior, `np.random.multivariate_normal` may come in handy here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72c005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) #Do not change this line!\n",
    "\n",
    "def sample_from_posterior(N):\n",
    "    \"\"\"\n",
    "    Draws samples from our approximate distribution on beta_0 and beta_1 from Model 3. \n",
    "    \n",
    "    Inputs:\n",
    "        N: int, number of samples to return\n",
    "        \n",
    "    Outputs:\n",
    "        samples: np.array of shape (N, 2) containing samples from the posterior \n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "posterior_samples = sample_from_posterior(1000)\n",
    "posterior_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743d79a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ce2d9",
   "metadata": {},
   "source": [
    "Now, let's visualize the samples we've drawn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca385a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms of samples along with 'density plots': these are plots of estimated densities and are supposed to approximate the posterior normal densities.\n",
    "# Initialize subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Plot histogram for the first column\n",
    "axes[0].hist(posterior_samples[:, 0], bins=100, alpha=0.3, color='blue', density = True)\n",
    "az.plot_kde(posterior_samples[:, 0], ax=axes[0], label=\"Density Plot\")\n",
    "axes[0].set_title('Histogram for beta_0 samples')\n",
    "\n",
    "# Plot histogram for the second column\n",
    "axes[1].hist(posterior_samples[:, 1], bins=100, alpha=0.3, color='green', density = True)\n",
    "az.plot_kde(posterior_samples[:, 1], ax=axes[1], label=\"Density Plot\")\n",
    "axes[1].set_title('Histogram for beta_1 samples')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b89282",
   "metadata": {},
   "source": [
    "## Posterior Predictive Checks\n",
    "\n",
    "In order to validate our Bayesian Poisson Regression model, we can perform **Posterior Predictive Checks (PPC)**. After performing Bayesian Regression we have access to a generating distribution for the response $Y'_i|X_i$. This generating distribution is the **posterior predictive distribution**, and it gives us the distribution of the *possible* outputs (denoted $Y'_i$) and associated probabilities ($p(Y'_i|X_i)$) that an input $X_i$ could potentially have. The crux of PPC is to sample from this **posterior predictive distribution** and to compare these samples to our original response data.\n",
    "\n",
    "The code below computes PPC samples and plots their distribution. Note that here, the band labeled \"Posterior predictive $y$\" is a collection of curves, each of which is the density plot of $y$ for a given draw of the coefficient vector $\\beta$ from the posterior. Specifically to generate a red line we follow these steps: \n",
    "1. First, we take one of our posterior samples of $\\beta_0$ and $\\beta_1$.\n",
    "2. Then we draw samples from the distribution $p(Y_i| \\beta_0,\\beta_1, X_i)$ for each data point $X_i$.\n",
    "3. Then we plot the density of the resulting $Y$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c30b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Posterior Predictive Samples (using samples from the posterior_samples)\n",
    "X = sm.add_constant(df['Temp_Anomaly'])\n",
    "\n",
    "# Reshaping samples and data to vectorize computation\n",
    "X_reshaped = X.values[:, np.newaxis, :]\n",
    "posterior_samples_reshaped = posterior_samples[np.newaxis, :, :]\n",
    "\n",
    "# Step 2: drawing samples from p(Y_i | betas, X_i)\n",
    "poisson_means = np.exp(np.sum(X_reshaped * posterior_samples_reshaped, axis = -1)) # Deriving mean of each Poisson likelihood under different values of beta and different inputs\n",
    "poisson_samples_matrix = np.random.poisson(poisson_means) # Drawing samples from each Poisson Distribution\n",
    "\n",
    "print(poisson_samples_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160453c6",
   "metadata": {},
   "source": [
    "With these posterior predictive samples, we can generate kde density plots for each sampled pair of $\\beta_0$ and $\\beta_1$ (these density plots are generated from the package arviz). For purposes of comparison, we also want to superimpose the average posterior predictive distribution (averaged over all the $n$ samples in the original dataset). This latter quantity is computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab787e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation of the average posterior predictive distribution\n",
    "from scipy.stats import poisson\n",
    "num_rows = X.shape[0]\n",
    "y_values = np.arange(0, 36)\n",
    "average_probabilities = np.zeros_like(y_values, dtype=float)\n",
    "\n",
    "for row in range(num_rows):\n",
    "    poisson_mean = np.exp(np.dot(posterior_normal_mean, X.iloc[row]))\n",
    "    average_probabilities += poisson.pmf(y_values, poisson_mean)\n",
    "\n",
    "average_probabilities /= num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562088f",
   "metadata": {},
   "source": [
    "Now, we're ready to plot our posterior predictive distributions, and compare them against the distribution of our observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bbb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the densities of all samples\n",
    "for sample in range(poisson_samples_matrix.shape[1]):\n",
    "    plot_kwargs = {\"color\": \"red\", \"linewidth\": 1, \"alpha\": 0.2}\n",
    "    if sample == 0:\n",
    "        plot_kwargs[\"label\"] = \"Posterior Predictive\"\n",
    "    az.plot_kde(poisson_samples_matrix[:, sample], plot_kwargs=plot_kwargs)\n",
    "\n",
    "# Plot the density of the Y data\n",
    "az.plot_kde(Y, plot_kwargs={\"color\": \"black\", \"linewidth\": 2, \"label\": \"Observed Response Data\"})\n",
    "\n",
    "# Superimpose the average_probabilities using a blue line\n",
    "y_values = np.arange(0, 36)  # Corresponding to y = 0, 1, ..., 35\n",
    "plt.plot(y_values, average_probabilities, color='blue', label=\"Posterior Predictive Mean for Bayesian Poisson Regression with SST\", linewidth=2, linestyle='--', alpha = 1)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Named Storms')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Posterior Predictive Check')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3858d",
   "metadata": {},
   "source": [
    "When we visualize this data, notice that the black line (i.e the distribution of our observed $Y$) falls snugly within the red region. This is exactly what we're looking for: the red region, composed of samples from our posterior predictive distribution, represents our model's belief about the distribution of $Y$. Interpreted in the context of our problem, the red region tells us how it believes frequencies of named storms are distributed, and since the observed distribution falls inside of the region, our models expectations and our observed reality line up nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc240ae",
   "metadata": {},
   "source": [
    "As a comparison, let's use this posterior predictive check on an obviously poor model: a poisson regression model that uses only the intercept ($\\beta_0$) as a parameter. The analogous plot for this non-SST model can be obtained by the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only intercept model\n",
    "X_intercept_only = sm.add_constant(X.drop('Temp_Anomaly', axis = 1))\n",
    "model_noSST = sm.GLM(Y, X_intercept_only, family = sm.families.Poisson()).fit()\n",
    "print(model_noSST.summary()) #note that deviance is now much higher\n",
    "print(model_noSST.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f1619",
   "metadata": {},
   "source": [
    "Note that the AIC of the intercept only model is 817.9 which is much higher than the AIC (756.5) of model three with both intercept and SST.\n",
    "\n",
    "Now, we'll use the exact same code as above to generate the posterior predictive check visualization on our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain the posterior normal distribution for this model:\n",
    "posterior_normal_mean = model_noSST.params.values  #this is the mean of the posterior normal approximation\n",
    "print(posterior_normal_mean)\n",
    "posterior_normal_covariance = model_noSST.cov_params() #this is the covariance matrix of the posterior normal approximation\n",
    "print(posterior_normal_covariance)\n",
    "\n",
    "#Generate posterior samples:\n",
    "n_samples = 1000\n",
    "beta_samples = sample_from_posterior(n_samples)\n",
    "\n",
    "#Generating Posterior Predictive Samples (using beta_samples)\n",
    "X = X_intercept_only\n",
    "n = X.shape[0]\n",
    "X_reshaped = X.values[:, np.newaxis, :]\n",
    "beta_samples_reshaped = beta_samples[np.newaxis, :, :]\n",
    "poisson_means = np.exp(np.sum(X_reshaped * beta_samples_reshaped, axis = -1))\n",
    "poisson_samples_matrix = np.random.poisson(poisson_means)\n",
    "print(poisson_samples_matrix.shape)\n",
    "\n",
    "#Calculation of the average posterior predictive distribution\n",
    "from scipy.stats import poisson\n",
    "num_rows = X.shape[0]\n",
    "y_values = np.arange(0, 36)\n",
    "average_probabilities = np.zeros_like(y_values, dtype=float)\n",
    "for row in range(num_rows):\n",
    "    poisson_mean = np.exp(np.dot(posterior_normal_mean, X.iloc[row]))\n",
    "    average_probabilities += poisson.pmf(y_values, poisson_mean)\n",
    "average_probabilities /= num_rows\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the densities of all samples\n",
    "for sample in range(poisson_samples_matrix.shape[1]):\n",
    "    plot_kwargs = {\"color\": \"red\", \"linewidth\": 1, \"alpha\": 0.2}\n",
    "    if sample == 0:\n",
    "        plot_kwargs[\"label\"] = \"Posterior Predictive\"\n",
    "    az.plot_kde(poisson_samples_matrix[:, sample], plot_kwargs=plot_kwargs)\n",
    "\n",
    "# Plot the density of the Y data\n",
    "az.plot_kde(Y, plot_kwargs={\"color\": \"black\", \"linewidth\": 2, \"label\": \"Observed Response Data\"})\n",
    "\n",
    "# Superimpose the average_probabilities using a blue line\n",
    "y_values = np.arange(0, 36)  # Corresponding to y = 0, 1, ..., 35\n",
    "plt.plot(y_values, average_probabilities, color='blue', label=\"Posterior Predictive Mean\", linewidth=2, linestyle='--', alpha = 1)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Posterior Predictive Check for model without SST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e4943",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 5b) With and Without SST\n",
    "\n",
    "Compare the two plots above. In your opinion, which model is a better fit for the observed data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d313b6",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c61353",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Congratulations! You have finished Lab 6! ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcde75e",
   "metadata": {},
   "source": [
    "Below, you will see two cells. Running the first cell will automatically generate a PDF of all questions that need to be manually graded, and running the second cell will automatically generate a zip with your autograded answers. **You are responsible for both the coding portion (the zip from Lab 6) and the written portion (the PDF of written responses from Lab 6) to their respective Gradescope portals.** The coding proportion should be submitted to the `Lab 6` assignment as a single zip file, and the written portion should be submitted to `Lab 6 PDF` assignment as a single pdf file. When submitting the written portion, please ensure you select pages appropriately.\n",
    "\n",
    "If there are issues with automatically generating the PDF in the first cell, you can try downloading the notebook as a PDF by clicking on `File -> Save and Export Notebook As... -> PDF`. If that doesn't work either, you can manually take screenshots of your answers to the manually graded questions and submit those. Either way, **you are responsible for ensuring your submission follows our requirements, we will NOT be granting regrade requests for submissions that don't follow instructions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"lab06.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('lab06.pdf')):\n",
    "    img = mpimg.imread('cute_flemish.jpg')\n",
    "    imgplot = plt.imshow(img)\n",
    "    imgplot.axes.get_xaxis().set_visible(False)\n",
    "    imgplot.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    display(HTML(\"Download your PDF <a href='lab06.pdf' download>here</a>.\"))\n",
    "else:\n",
    "    print(\"\\n Pdf generation fails, please try the other methods described above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae866e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c33d2d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885324a",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(model_one.params.loc['const'], 9.561762020058577)\n>>> assert np.isclose(model_one.params.loc['Temp_Anomaly'], 5.331674995178005)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(aic_one, 763.0930286975074)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(model_two.params.loc['const'], 2.164485733398375)\n>>> assert np.isclose(model_two.params.loc['Temp_Anomaly'], 0.5197417751409866)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(aic_two, 767.7904016150487)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(model_three.params.loc['const'], 2.241430978996669)\n>>> assert np.isclose(model_three.params.loc['Temp_Anomaly'], 0.4865848805886122)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert blank_1 == \"A\"\n>>> assert blank_2 == \"A\"\n>>> assert blank_3 == \"B\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(aic_three, 756.5039666707843)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(model_four.params.loc['const'], 2.2416921213381418)\n>>> assert np.isclose(model_four.params.loc['Temp_Anomaly'], 0.4844909261096873)\n>>> assert np.isclose(model_four.params.loc['alpha'], 0.02896445044437466)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(aic_four, 753.3965529372495)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert posterior_samples.shape == (1000, 2)\n>>> assert np.allclose(posterior_samples[0, :], [2.23145102, 0.51522611])\n>>> assert np.allclose(posterior_samples[-1, :], [2.22448402, 0.4727535])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
