{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01d4dc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab04.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6315b",
   "metadata": {},
   "source": [
    "# Lab 4:  Rejection Sampling and More PyMC Practice\n",
    "\n",
    "Welcome to the 4th Data 102 lab! \n",
    "\n",
    "The goal of this Lab is to get you familiar with rejection sampling, as well as give you additional practice utilizing Bayesian analysis methods with PyMC.\n",
    "\n",
    "##### Please read the introduction and the instructions to each problem carefully.\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdac4a",
   "metadata": {},
   "source": [
    "**Collaborators**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6ad2b",
   "metadata": {},
   "source": [
    "## Submission\n",
    "See the [Gradescope Submission Guidelines](https://edstem.org/us/courses/42657/discussion/3350112) for details on how to submit your lab. Unlike the last few weeks, as part of the submission process, you'll need to **generate the PDFs and upload them to a separate gradescope assignment on your own; the autograder will no longer do that for you!**\n",
    "\n",
    "Again, since this lab involves sampling, **tests may take awhile to run. Please submit as early as possible, as last minute submissions may overwhelm Datahub, preventing yourself and others from submitting on-time.**\n",
    "\n",
    "**For full credit, this assignment should be completed and submitted before Wednesday, September 27th, 2023 at 11:59 PM PST.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ee228",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal, norm, uniform\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "\n",
    "import pymc as pm\n",
    "\n",
    "import hashlib\n",
    "\n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "def get_hash(num, significance = 4):\n",
    "    num = round(num, significance)\n",
    "    \"\"\"Helper function for assessing correctness\"\"\"\n",
    "    return hashlib.md5(str(num).encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f531047",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this Lab you are given a two dimensional unnormalized density function $q(x,y)$ represented by `target_density` below. The goal of Question 1 of this lab is to build up a sampler that can output samples from the distribution proportional to $q(x,y)$. \n",
    "\n",
    "In **Question 1** we will compute samples via *Rejection Sampling*. In part **1.a** we will build a sampler for a 1-dimensional projection of the density. In part **1.b** we will extend the approach to two dimensions.\n",
    "\n",
    "*Throughout Question 1, we will assume that our computers have access only to normal and uniform random variables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the target unnormalized density from which we would like to sample\n",
    "# Run this to define the function\n",
    "# No TODOs here\n",
    "@np.vectorize # <- decorator, makes function run faster\n",
    "def target_density(x, y):\n",
    "    mean1 = [1, 1.7]\n",
    "    mean2 = [2, 1.3]\n",
    "    mean3 = [1.5, 1.5]\n",
    "    mean4 = [2, 2.1]\n",
    "    mean5 = [1, 1.2]\n",
    "    cov1=0.2*np.array([[0.2, -0.05], [-0.05, 0.1]])\n",
    "    cov2 = 0.3*np.array([[0.1, 0.07], [0.07, 0.2]])\n",
    "    cov3= np.array([[0.1, 0], [0, 0.1]])\n",
    "    cov4 = 0.1*np.array([[0.3, 0.04], [0.04, 0.2]])\n",
    "    cov5 = 0.1*np.array([[0.4, -0.04], [-0.04, 0.2]])\n",
    "    return(multivariate_normal.pdf([x, y], mean=mean1, cov=cov1) + \n",
    "           multivariate_normal.pdf([x, y], mean=mean2, cov=cov2) +\n",
    "           2*multivariate_normal.pdf([x, y], mean=mean3, cov=cov3) +\n",
    "           0.5*multivariate_normal.pdf([x, y], mean=mean4, cov=cov4)+\n",
    "           0.5*multivariate_normal.pdf([x, y], mean=mean5, cov=cov5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443a3d9",
   "metadata": {},
   "source": [
    "#### Let's visualize this density. \n",
    "\n",
    "Run the cell below to see a 3D plot of the function along with a contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eaf268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run the cell to make plots\n",
    "# Create a meshgrid of coordinates\n",
    "coords = np.arange(0.5, 2.5, 0.02)\n",
    "X, Y = np.meshgrid(coords, coords)\n",
    "\n",
    "# Compute the value of the target density at all pairs of (x,y) values\n",
    "Z = target_density(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the 3D plot of the target density\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "ax0 = fig.add_subplot(121, projection='3d')\n",
    "ax1 = fig.add_subplot(122)\n",
    "\n",
    "surf = ax0.plot_surface(X,Y,Z, cmap=cm.plasma, linewidth=0, antialiased=False,alpha = 0.9,)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax0.set_zlim(0, 7)\n",
    "ax0.set_xlabel(\"X\")\n",
    "ax0.set_ylabel(\"Y\")\n",
    "ax0.set_zlabel(\"Z\")\n",
    "ax0.set_title(\"3D plot of the target density\")\n",
    "\n",
    "# Rotate the axes: you can change these numbers in order to see the distribution from other angles\n",
    "ax0.view_init(50, 25)\n",
    "\n",
    "# Plot the contour plot of the density\n",
    "cont = ax1.contour(X,Y,Z, levels = 20, cmap=cm.plasma, linewidths=1)\n",
    "ax1.set_xlabel(\"X\")\n",
    "ax1.set_ylabel(\"Y\")\n",
    "ax1.set_title(\"Contour plot of the target density\")\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5, ax=ax1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d93f48",
   "metadata": {},
   "source": [
    "Take a moment to examine the plots. Make sure you can see correspondances between each peak in the 3D plot on the left; and the \"high-altitude\" regions in the countour plot on the right.\n",
    "\n",
    "Next we will plot 1-dimensional projections of the target densities onto the $X$ and $Y$ axis. These correspond to conditional target distributions of the form $q(x, y=y')$ and $q(x=x', y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff856e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "# Run the cell below to define the plotting functions\n",
    "\n",
    "COORDINATES = np.arange(0, 3, 0.02)\n",
    "def plot_x_cond(y_val):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(12)\n",
    "    axs[0].contour(X,Y,Z, levels = 20, cmap=cm.plasma, alpha = 0.8, linewidths=0.8)\n",
    "    axs[0].axhline(y_val,  ls=\"--\", color = 'olive', lw = 2)\n",
    "    axs[0].set_xlabel(\"X\")\n",
    "    axs[0].set_ylabel(\"Y\")\n",
    "    axs[0].set_title(\"Contour plot of the target density\")\n",
    "    \n",
    "    axs[1].plot(COORDINATES, target_density(COORDINATES, y_val), color = 'olive')\n",
    "    axs[1].set_ylim(0,10)\n",
    "    axs[1].set_xlim(0,3)\n",
    "    axs[1].set_xlabel(\"X\")\n",
    "    axs[1].set_title(\"Conditional target density: q(x | y={:.1f})\".format(y_val))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_y_cond(x_val):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(12)\n",
    "    axs[0].contour(X,Y,Z, levels = 20, cmap=cm.plasma, alpha = 0.8, linewidths=0.8)\n",
    "    axs[0].axvline(x_val,  ls=\"--\", color = 'olive', lw = 2)\n",
    "    axs[0].set_xlabel(\"X\")\n",
    "    axs[0].set_ylabel(\"Y\")\n",
    "    axs[0].set_title(\"Contour plot of the target density\")\n",
    "    \n",
    "    axs[1].plot(COORDINATES, target_density(x_val, COORDINATES), color = 'olive')\n",
    "    axs[1].set_ylim(0,10)\n",
    "    axs[1].set_xlim(0,3)\n",
    "    axs[1].set_xlabel(\"Y\")\n",
    "    axs[1].set_title(\"Conditional target density: q(y | x={:.1f})\".format(x_val))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8978375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interactive plot\n",
    "interactive_plot = interactive(plot_x_cond, y_val=(0, 3, 0.1), add_proposal=False)\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dbe876",
   "metadata": {},
   "source": [
    "Set different values of `y_val`, observe the changes in the conditional target density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654146d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interactive plot\n",
    "interactive_plot = interactive(plot_y_cond, x_val=(0, 3, 0.1), add_proposal=False)\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba1b3a",
   "metadata": {},
   "source": [
    "Set different values of `x_val`, observe the changes in the conditional target density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04468363",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### A Quick Understanding Check:\n",
    "\n",
    "We said that $q$ is an unnormalized density function. What does this mean? How could we test whether or not the function is normalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ced492",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0024f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 1: Rejection Sampling\n",
    "\n",
    "In this question, we will build a rejection sampler. First, let's review the basics. \n",
    "\n",
    "Assume we want to sample from an unnormalized target density $q(x)$, using a proposal distribution $F$, with density $f(x)$. The proposal distribution is chosen such that we have access to samples from it. \n",
    "\n",
    "#### Rejection sampling proceeds as follows:\n",
    "\n",
    "- Find constant $c$, such that $cq(x)\\leq f(x)$ on the support\n",
    "- At each iteration:\n",
    "    - Sample $x_i \\sim F$\n",
    "    - Compute the ratio $r = \\frac{c(q(x_i))}{f(x_i)} \\leq 1$\n",
    "    - Sample $\\gamma_i \\sim Uniform(0,1)$:\n",
    "        - `accept` the sample if $\\gamma_i \\leq r$: Add $x_i$ to the list of samples.\n",
    "        - `reject` the sample otherwise: do nothing\n",
    "        \n",
    "### 1a) Sample from the one-dimensional density $q(x, y=1.2)$\n",
    "Throughout part 1.a, we will restrict our attention to the range $[0,3]$ for simplicity. That way we can use Uniform(0,3) as our proposal distribution. Meaning that $f(x) = \\frac{1}{3} \\ \\forall x\\in[0,3]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target 1D density q(x, y = 1.2)\n",
    "def target_1D_density(x):\n",
    "    return(target_density(x, 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c906c",
   "metadata": {},
   "source": [
    "Finish implementing the steps of the rejection sampling algorithm by filling in the following code.\n",
    "\n",
    "*Hint: both scipy and numpy provide methods for drawing from a uniform distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21791d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_1D_proposed_distribution(N):\n",
    "    \"\"\" \n",
    "    Produces N samples from the Uniform(0,3) proposal distribution\n",
    "    \n",
    "    Inputs:\n",
    "        N : int, desired number of samples\n",
    "        \n",
    "    Outputs:\n",
    "        proposed_samples : an 1d-array of size N which contains N independent samples from the proposal\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "@np.vectorize\n",
    "def compute_ratio_1D(proposed_sample, c):\n",
    "    \"\"\"\n",
    "    Computes the ratio between the scaled target density and proposal density evaluated at the \n",
    "    proposed sample point\n",
    "    \n",
    "    Inputs:\n",
    "        proposed_sample : float, proposed sample\n",
    "        c : float, constant scaling factor that ensures that the proposal density is above the target density\n",
    "        \n",
    "    Outputs:\n",
    "        ratio : float\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "@np.vectorize\n",
    "def accept_proposal(ratio):\n",
    "    \"\"\" \n",
    "    Accepts or rejects a proposal with probability equal to ratio\n",
    "    \n",
    "    Inputs: \n",
    "        ratio: float, probability of acceptance\n",
    "    \n",
    "    Outputs:\n",
    "        accept: True/False, if True, accept the proposal\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856fa712",
   "metadata": {},
   "source": [
    "You can use the following cell to test your functions to convince yourself that they work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b78234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR TEST CASES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb84a54",
   "metadata": {},
   "source": [
    "Now we have all the ingredients for making a sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a182f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_1D_samples(N, c): \n",
    "    \"\"\" \n",
    "    Produces samples from target_1D_density\n",
    "    \n",
    "    Inputs:\n",
    "        N : int, number of proposed_samples\n",
    "        c : float, constant scaling factor that ensures that the proposal density is above the target density\n",
    "        \n",
    "    Outputs:\n",
    "        rejection_samples : an 1d-array which contains independent samples from the target\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b890f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a_ii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26c545c",
   "metadata": {},
   "source": [
    "From the interactive plots we made earlier, we can see that $q(x, y=1.2)$ is allways smaller than 5. Hence to make it smaller than $f(x) = 1/3$ we need to scale the target density by a factor $c \\leq \\frac{1}{3}\\cdot\\frac{1}{5} = 1/15$. \n",
    "\n",
    "#### Let's use $c=1/15$, compute target samples and plot their histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b388e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here\n",
    "# Just run it once you passed the tests above\n",
    "\n",
    "fig = plt.figure(figsize = (6, 4))\n",
    "c = 1/15\n",
    "target_samples = get_1D_samples(1000, c)\n",
    "density_values =  target_1D_density(COORDINATES)*c\n",
    "plt.plot(COORDINATES, density_values, label='Target')\n",
    "plt.axhline(1/3, ls = '--', label = 'Proposal')\n",
    "n, bins, rects = plt.hist(target_samples, density = True, label=\"Accepted Samples\")\n",
    "max_height = np.max([r.get_height() for r in rects])\n",
    "for r in rects:\n",
    "    r.set_height(r.get_height()*np.max(density_values)/max_height)\n",
    "plt.legend()\n",
    "plt.xlim(0,3)\n",
    "plt.ylim(0,0.45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9322c4",
   "metadata": {},
   "source": [
    "#### Computing the acceptance ratio for varying scaling constants c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b06bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here\n",
    "# Just run it and comment in the section below\n",
    "\n",
    "N = 1000\n",
    "c_values = [0.06, 0.05, 0.04, 0.03, 0.02, 0.01]\n",
    "for c in c_values:\n",
    "    # compute target samples\n",
    "    target_samples = get_1D_samples(N, c)\n",
    "    acceptance_percentage = 100*len(target_samples)/N\n",
    "    print(\"For c = {:.2f}, the acceptance percentage is {:.1f}%\".format(c, acceptance_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6e2c9",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### In the cell below explain why the accepted percentage decreases as $c$ decreases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3ba98",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef1643",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 1.b Sample from the two-dimensional density $q(x, y)$\n",
    "\n",
    "In two dimensions Rejection Sampling is nearly identical to the 1-dimension case:\n",
    "\n",
    "- Find constant $c$, such that $cq(x, y)\\leq f(x, y)$ on the support\n",
    "- At each iteration:\n",
    "    - Sample $(x_i, y_i) \\sim F$\n",
    "    - Compute the ratio $r = \\frac{c(q(x_i, y_i))}{f(x_i, y_i)} \\leq 1$\n",
    "    - Sample $\\gamma_i \\sim Uniform(0,1)$:\n",
    "        - `accept` the sample if $\\gamma_i \\leq r$: add $(x_i, y_i)$ to the list of samples.\n",
    "        - `reject` the sample otherwise: do nothing\n",
    "\n",
    "Throughout part 1.b we will consider $(x, y)\\sim Uniform(0,3)\\times Uniform(0,3)$ as our proposal distribution. Meaning that $f(x, y) = \\frac{1}{9}\\ \\forall x, y\\in[0,3]$\n",
    "\n",
    "Fill in the 2-d ratio calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b94b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def compute_ratio_2D(proposed_sample_x, proposed_sample_y, c):\n",
    "    \"\"\"\n",
    "    Computes the ratio between the scaled target density and proposal density evaluated at the \n",
    "    proposed sample point\n",
    "    \n",
    "    Inputs:\n",
    "        proposed_sample_x : float, x components of the proposed sample point\n",
    "        proposed_sample_y : float, y components of the proposed sample point\n",
    "        c : float, constant scaling factor that ensures that the proposal density is above the target density\n",
    "        \n",
    "    Outputs:\n",
    "        ratio : float\n",
    "    \"\"\"\n",
    "    ratio = ...\n",
    "    assert(ratio <= 1)\n",
    "    return(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d56886",
   "metadata": {},
   "source": [
    "Use the following cell to convince yourself that your code is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR TEST CASES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff88ed",
   "metadata": {},
   "source": [
    "Now we have all the ingredients for making the 2-d sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run the 2D version of the functions we built in 1.a\n",
    "def get_2D_samples(N, c): \n",
    "    \"\"\" \n",
    "    Produces samples from target_density\n",
    "    \n",
    "    Inputs:\n",
    "        N : int, number of proposed_samples\n",
    "        c : float, constant scaling factor that ensures that the proposal density is above the target density\n",
    "        \n",
    "    Outputs:\n",
    "        rejection_samples : ndarray of which contains independent samples from the target\n",
    "    \"\"\"\n",
    "    proposed_samples_x = sample_1D_proposed_distribution(N)\n",
    "    proposed_samples_y = sample_1D_proposed_distribution(N)\n",
    "    ratios = compute_ratio_2D(proposed_samples_x, proposed_samples_y, c)\n",
    "    accept_array = accept_proposal(ratios)\n",
    "    proposed_samples = np.concatenate((proposed_samples_x.reshape(N,1), proposed_samples_y.reshape(N,1)), axis = 1)\n",
    "    rejection_samples = proposed_samples[accept_array]\n",
    "    return(rejection_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf20e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2fc67",
   "metadata": {},
   "source": [
    "From the contour plot we made previously, we can see that $q(x, y=1.2)$ is allways smaller than 7.4. Hence to make it smaller than $f(x) = 1/9$ we need to scale the target density by a factor $c \\leq \\frac{1}{7.4}\\cdot\\frac{1}{8} = 0.015$. \n",
    "\n",
    "#### Let's use $c=0.015$, compute target samples and plot them on top the contour lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "# Plot the contour plot of the density\n",
    "cont = plt.contour(X,Y,Z, levels = 20, cmap=cm.plasma, linewidths=1, alpha = 0.8)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Scatterplot of samples obtained via Rejection Sampling\")\n",
    "\n",
    "# Add sample points obtained via rejection sampling\n",
    "c = 1/72\n",
    "target_samples = get_2D_samples(3000, c)\n",
    "plt.scatter(target_samples[:,0], target_samples[:,1], c='b', alpha = 1, s = 10, label = 'Samples')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdcc4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify this\n",
    "# just run it and comment in the section below\n",
    "\n",
    "N = 3000\n",
    "c_values = [0.015, 0.01, 0.005, 0.001]\n",
    "for c in c_values:\n",
    "    # compute target samples\n",
    "    target_samples = get_2D_samples(N, c)\n",
    "    acceptance_percentage = 100*len(target_samples)/N\n",
    "    print(\"For c = {:.3f}, the acceptance percentage is {:.1f}%\".format(c, acceptance_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b03451",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### In the cell below explain why the accepted percentage when sampling from 2D distribution is so much smaller than sampling from the 1D version in 1.a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2555a08",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04f426",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2: Bayesian A/B Testing\n",
    "\n",
    "Now, we will pivot back to PyMC so you can get more practice working with PyMC. In particular, we'll be taking a look at Bayesian approaches to hypothesis testing.\n",
    "\n",
    "Recall from Data 8 that you can perform [hypothesis testing using the permutation test](https://inferentialthinking.com/chapters/12/1/AB_Testing.html) (Chapter 12.1). Before continuing, we highly encourage you to read the above linked Data 8 chapters; this question is a direct continuation on the example from this chapter!\n",
    "\n",
    "In a particular medical study, a sample of newborn babies was obtained from a large hospital system.  We will treat the data as if it were a simple random sample, though the sampling was done in multiple stages. Deborah Nolan and Terry Speed discuss the larger dataset in [Stat Labs](https://www.stat.berkeley.edu/~statlabs/).\n",
    "\n",
    "One of the aims of the study was to see whether maternal smoking was associated with birth weight. Following the standard hypothesis testing procedure, they proposed the following two hypotheses:\n",
    "\n",
    "> **Null hypothesis ($H_0$)**: In the population, the distribution of birth weights of babies is the same for mothers who don’t smoke as for mothers who do. The (observed) difference in the sample is due to chance. In other words, let $\\mu_0$ be the population mean of the birth weights of babies of non-smoking mothers, and $\\mu_1$ be the population mean of the birth weights of babies of smoking mothers, we claim that $\\mu_0 = \\mu_1$.\n",
    "\n",
    "> **Alternative hypothesis ($H_1$)**: In the population, the babies of the mothers who smoke have a <i>**different**</i> birth weight, on average, than the babies of the non-smokers. In other words, we claim $\\mu_0 \\neq \\mu_1$.\n",
    "\n",
    "In recent years, however, the validity of hypothesis testing has been called into question, with the [ASA going so far as to acknowledge the limitations of $p$-values](https://amstat.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108). As a result, Bayesian alternatives to traditional hypothesis testing have begun to rise in popularity. In this lab, we'll now be revisiting this A/B Testing example from a **Bayesian approach**, with the help of `PyMC`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea976dcc",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2(a) Limitations of the $p$-value\n",
    "\n",
    "In this study, after conducting a permutation test, researchers were able to find enough evidence to reject the null hypothesis (i.e a p-value less than 0.05). Would it be equivalent to say that they found evidence that the alternative hypothesis is true? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95408d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf397a0",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "In class, you learned that one of the advantages to Bayesian statistics is its ability to handle **data sparsity**. In the kidney cancer example, we were able to utilize priors to get around a lack of data, improving the performance of detection metrics in counties with low population counts.\n",
    "\n",
    "Here, we'll see yet another reason why these approaches are becoming widely adopted: their **ease of interpretability**. As opposed to frequentist hypothesis testing and its arcane interpretation of the $p$-value, Bayesians are able to directly prove and disprove the hypotheses in question. In particular, we can find out just how probable our alternative hypothesis is.\n",
    "\n",
    "To see how, let's start by opening up our data, and isolating our variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b15abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby = pd.read_csv(\"baby.csv\")\n",
    "baby.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b230f3",
   "metadata": {},
   "source": [
    "This is the same dataset you've seen in Data 8/100. We will focus on the `'Birth Weight'` and `'Maternal Smoker'` columns for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby = baby[['Birth Weight', 'Maternal Smoker']]\n",
    "baby.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9714e83",
   "metadata": {},
   "source": [
    "To formalize what we're looking for, let's revisit and define some notations.\n",
    "\n",
    "- Let $z_i$ denote the smoker status of record $i$: if $z_i = 1$, then the $i$th baby's mother is a smoker.  \n",
    "- $\\mu_0$ and $\\mu_1$ are the population mean of the weights of babies born to non-smokers (0) and smokers (1).\n",
    "\n",
    "To gain information about these population parameters, we'll also need to incorporate the data we have in some way. We'll call these data $X_i$, where $X_i$ represents the $i$th baby's weight. We then define the mixture model as follows:\n",
    "\n",
    "- $X_i \\mid z_i = 0, \\mu_0, \\mu_1 \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$. In words, the birth weights of babies born to non-smokers follows the normal distribution with mean $\\mu_0$ and variance $\\sigma_0^2$.\n",
    "- $X_i \\mid z_i = 1, \\mu_0, \\mu_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$. In words, the birth weights of babies born to smokers follows the normal distribution with mean $\\mu_1$ and variance $\\sigma_1^2$.\n",
    "\n",
    "For simplicity of anaylysis, we will assume $\\sigma_0^2$ and $\\sigma_1^2$ is known in advance.\n",
    "\n",
    "The difference in these population parameters ($\\mu_{0} - \\mu_{1}$) serves as our quantity of interest, or in the case of frequentist testing, our **test statistic**.\n",
    "\n",
    "The goal, then, is to find the distribution $p(\\mu_0 - \\mu_1 | X_1,...,X_n)$ by utilizing our posterior distribution $p(\\mu_0, \\mu_1 | X_1,...,X_n)$.\n",
    "\n",
    "To do that, we'll need a prior!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a76d03",
   "metadata": {},
   "source": [
    "### 2(b) Finding a prior via Empirical Bayes\n",
    "\n",
    "First, we'll find a prior on $\\mu_0$ and $\\mu_1$. As in lecture, we'll do this by utilizing the data that we already have in an Empirical Bayes approach.\n",
    "\n",
    "In frequentist settings, researchers often use the sample mean as a way of estimating the true population mean. With bootstrap resamples of our data, we can simulate new draws from the population, and get the distribution of sample means.\n",
    "\n",
    "For this question, we'd like to use this distribution of sample means as a prior on $\\mu_0$ and $\\mu_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b187f1",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 2b (i)\n",
    "\n",
    "Fill in the blanks in the following statement:\n",
    "\n",
    "With the assumptions that sample size is large and the sample is drawn i.i.d. from the population, the sample mean follows the ____ A ____ distribution. This is a result of the _____ B _____. \n",
    "\n",
    "Fill in blank A with the name of a known distribution, blank B with the name of a famous theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ed5df",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111714a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 2b (ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffc7bd",
   "metadata": {},
   "source": [
    "Since $\\mu_0$ and $\\mu_1$ represent independent draws from this prior distribution, calculate $p(\\mu_0 > \\mu_1)$. What does this say about our belief regarding the average birthweights of children of smoking and non-smoking mothers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2513cb",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc06887",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### 2b (iii)\n",
    "Now that we have some intuition about the shape of our prior, and the beliefs it encodes, let's go ahead and find the distribution of our sample means, and fit our prior over it!\n",
    "\n",
    "Fill in the code cell below to get bootstrapped estimates of the population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f1b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) #Do not change this line!\n",
    "\n",
    "def get_bootstrap_mean():\n",
    "    ...\n",
    "\n",
    "means = np.array([])\n",
    "\n",
    "for i in np.arange(10000):\n",
    "    means = ...\n",
    "\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa3aeb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b_iii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09605a90",
   "metadata": {},
   "source": [
    "### 2b (iv)\n",
    "\n",
    "With our bootstrapped sample means in hand, we can now fit our prior distribution over it!\n",
    "\n",
    "Fill in the code cell below to define and visualize our prior distribution. \n",
    "\n",
    "**Remember**: since we're using an empirical Bayes approach here, your visualized prior should fit the data almost exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc2363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "support = np.linspace(116, 122, 100) # Defines points to evaluate our pdf over\n",
    "\n",
    "prior_pdf = ...\n",
    "\n",
    "# Plot the histogram of bootstrapped means\n",
    "sns.histplot(means, stat='density');\n",
    "\n",
    "# Plot the prior pdf\n",
    "plt.plot(support, prior_pdf);\n",
    "plt.title(r\"Prior Distribution for $\\mu_0$ and $\\mu_1$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eef718",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b_iv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42898b",
   "metadata": {},
   "source": [
    "## Question 3: Defining our Model\n",
    "\n",
    "Now, with the pieces you've defined above, craft your final A/B Testing model in the code cell below. As a reminder, our model currently consists of the following likelihoods and priors:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_0, \\mu_1 &\\sim \\text{distribution we discovered in 2(b)}\\\\\n",
    "X_i \\mid z_i = 0, \\mu_0, \\mu_1 &\\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)\\\\\n",
    "X_i \\mid z_i = 1, \\mu_0, \\mu_1 &\\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To simplify your calculations, we utilized a similar bootstrapping strategy to define $\\sigma_0^2$ and $\\sigma_1^2$. Without proof, you can use $\\sigma_0^2 = \\sigma_1^2 = 18.3^2$ for your model.\n",
    "\n",
    "**Note:** To pass the test, make sure the name parameter you pass to each `Distribution` object matches the variable name it's assigned to. Your answers should follow the following format: \n",
    "\n",
    "```\n",
    "with pm.Model() as model:\n",
    "\n",
    "    mu = pm.Some_Distribution('mu', ...)\n",
    "    X = pm.Some_Distribution('X', ...)`\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Hint 1**: Since we're interested in calculating a test statistic based on our posterior distribution of $\\mu_0$ and $\\mu_1$, we can utilize `pm.Deterministic` ([documentation](https://www.pymc.io/projects/docs/en/v5.6.0/api/generated/pymc.Deterministic.html)) to calculate it for every simulated sample generated by PyMC.\n",
    "\n",
    "**Hint 2**: When defining `X`, it might be helpful to use `NumPy`'s [fancy indexing](https://jakevdp.github.io/PythonDataScienceHandbook/02.07-fancy-indexing.html) to indicate smoker status of mothers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e439b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = ...\n",
    "    X = ...\n",
    "    test_stat = ...\n",
    "\n",
    "    # Do not modify trace; settings required to pass autograder\n",
    "    trace = pm.sample(500, chains=4, tune=1000, target_accept=0.95, return_inferencedata=False, progressbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161e9de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbaa98",
   "metadata": {},
   "source": [
    "## Question 4: Interpreting our Results\n",
    "Now that we have our posterior samples, let's take a look at the posterior distribution of $\\mu_0 - \\mu_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed577287",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statistics = trace['test_stat']\n",
    "\n",
    "# Draw the credible interval\n",
    "plt.hlines(0, np.percentile(test_statistics, 2.5), np.percentile(test_statistics, 97.5), colors='blue', linewidth=10)\n",
    "\n",
    "sns.histplot(test_statistics, stat = \"density\");\n",
    "plt.title(r\"Posterior Distribution of $\\mu_0 - \\mu_1$\");\n",
    "plt.xlabel(r\"$\\mu_0 - \\mu_1$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5cfad7",
   "metadata": {},
   "source": [
    "Notice the blue interval that we've placed on our posterior: this is called the **credible interval** (not to be confused with the frequentist *confidence interval*). In particular, the credible interval we're looking at above represents the **95% credible interval**. This means with 95% confidence, we believe that $\\mu_0 - \\mu_1$ lies between 1.737 and 4.154.\n",
    "\n",
    "**Note**: your exact bounds may differ slightly due to randomness in the sampling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c471a",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4(a) Credible vs. Confidence Intervals\n",
    "In Data 8, you've seen a frequentist interval called the confidence interval. For your convenience, we list their definitions side-by-side:\n",
    "\n",
    "> **95% Confidence Interval**: With a large number of repeated samples, 95% of such calculated confidence intervals would include the true value of the parameter. We say we are 95% confident that the true estimate would lie within the interval.\n",
    "\n",
    "> **95% Credible Interval**: With our prior belief of the parameter and the data we observe, there's 95% probability that the true parameter would lie within this interval.\n",
    "\n",
    "How are the interpretations of these intervals different? Answer the following questions:\n",
    "1. What is random in the frequentist setting?\n",
    "2. What is random in the Bayesian setting?\n",
    "3. Which one is more intuitive to interpret?\n",
    "\n",
    "Answer each question with **1-2 sentences**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572e4da",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d32f1",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4(b) Interpreting the Credible Interval\n",
    "\n",
    "Notice that our 95% credible interval lies between 1.737 and 4.154. Does this prove or disprove our alternative hypothesis? If we're using the 95% credible interval as our rejection criteria, how sure are we that the alternative hypothesis is true?\n",
    "\n",
    "Answer the question with **2 sentences** or less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40390f17",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ff788f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Congratulations! You have finished Lab 4! ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef35fbf",
   "metadata": {},
   "source": [
    "Below, you will see two cells. Running the first cell will automatically generate a PDF of all questions that need to be manually graded, and running the second cell will automatically generate a zip with your autograded answers. **You are responsible for both the coding portion (the zip from Lab 4) and the written portion (the PDF of written responses from Lab 4) to their respective Gradescope portals.** The coding proportion should be submitted to the `Lab 4` assignment as a single zip file, and the written portion should be submitted to `Lab 4 PDF` assignment as a single pdf file. When submitting the written portion, please ensure you select pages appropriately.\n",
    "\n",
    "If there are issues with automatically generating the PDF in the first cell, you can try downloading the notebook as a PDF by clicking on `File -> Save and Export Notebook As... -> PDF`. If that doesn't work either, you can manually take screenshots of your answers to the manually graded questions and submit those. Either way, **you are responsible for ensuring your submission follows our requirements, we will NOT be granting regrade requests for submissions that don't follow instructions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"lab04.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('lab04.pdf')):\n",
    "    img = mpimg.imread('baby_seal.png')\n",
    "    imgplot = plt.imshow(img)\n",
    "    imgplot.axes.get_xaxis().set_visible(False)\n",
    "    imgplot.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    display(HTML(\"Download your PDF <a href='lab04.pdf' download>here</a>.\"))\n",
    "else:\n",
    "    print(\"\\n Pdf generation fails, please try the other methods described above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f87bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dbad6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34179689",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a_ii": {
     "name": "q1a_ii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> N = 1000\n>>> tests = [np.abs(1.5-np.mean(get_1D_samples(N, 1/15))) < 0.1,\n...          np.abs(0.3-len(get_1D_samples(N, 1/15))/N) < 0.05,\n...          np.abs(0.23-len(get_1D_samples(N, 1/20))/N) < 0.05,\n...          np.abs(0.18-len(get_1D_samples(N, 1/25))/N) < 0.05]\n>>> assert np.all(tests)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b_i": {
     "name": "q1b_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> N = 5000\n>>> tests = [np.abs(0.075-len(get_2D_samples(N, 0.015))/N) < 0.015,\n...          np.abs(0.045-len(get_2D_samples(N, 0.01))/N) < 0.015]\n>>> assert np.all(tests)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b_iii": {
     "name": "q2b_iii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(means) == 10000\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> checks = []\n>>> mean_of_means = np.mean(means)\n>>> checks.append(np.isclose(mean_of_means, 119.45713373082474))\n>>> \n>>> answer_first_10 = [120.99488927, 119.22061329, 119.44633731, 118.00596252,119.42759796, 119.28194208, 119.93015332, 119.49829642,120.95315162, 118.80238501]\n>>> first_10 = means[:10]\n>>> checks.append(np.allclose(first_10, answer_first_10))\n>>> \n>>> answer_last_10 = [120.62606474, 119.45229983, 119.62265758, 118.2197615,119.0451448 , 119.47103918, 118.90630324, 118.98551959, 119.53321976, 119.50596252]\n>>> last_10 = means[-10:]\n>>> checks.append(np.allclose(last_10, answer_last_10))\n>>> \n>>> assert np.any(checks)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b_iv": {
     "name": "q2b_iv",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> answers_50_60 = np.array([0.54167851, 0.58875147, 0.63181654, 0.66945065, 0.70034935,\n...        0.72340163, 0.73775613, 0.74287332, 0.73855915, 0.72497728])\n>>> \n>>> assert np.allclose(prior_pdf[50:60], answers_50_60)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert trace['mu'].shape == (2000, 2)\n>>> assert np.allclose(trace['test_stat'], trace['mu'][:, 0] - trace['mu'][:, 1])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
