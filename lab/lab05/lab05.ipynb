{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57923a0c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab05.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb73be2",
   "metadata": {},
   "source": [
    "# Lab 5: Gibbs Sampling and Linear Models\n",
    "Welcome to the Data 102 Lab 5. In this lab, we are going to wrap up our discussion of approximate inference methods with Gibbs Sampling, and give you a chance to build GLMs from a Bayesian and Frequentist perspective.\n",
    "\n",
    "#### The code and responses you need to write are are represented by `...`. There is additional documentation for each part as you go along.\n",
    "\n",
    "##### Please read carefully the introduction and the instructions to each problem.\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74a664",
   "metadata": {},
   "source": [
    "**Collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e237e8",
   "metadata": {},
   "source": [
    "## Submission\n",
    "See the [Gradescope Submission Guidelines](https://edstem.org/us/courses/42657/discussion/3350112) for details on how to submit your lab.\n",
    "\n",
    "Again, since this lab involves sampling, **tests may take awhile to run. Please submit as early as possible, as last minute submissions may overwhelm datahub, preventing yourself and others from submitting on-time.**\n",
    "\n",
    "**For full credit, this assignment should be completed and submitted before Wednesday, October 4th, 2023 at 11:59 PM PST.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, interactive\n",
    "import itertools\n",
    "import hashlib\n",
    "from scipy.stats import poisson, norm, gamma, uniform, multivariate_normal\n",
    "#!pip install pymc3\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "  \n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from pymc import *\n",
    "import pymc as pm\n",
    "import bambi as bmb\n",
    "\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96166a",
   "metadata": {},
   "source": [
    "# Part I: Gibbs Sampling\n",
    "\n",
    "As in Lab 4 Question 1, you are given a two-dimensional unnormalized density function $q(x,y)$ represented by `target_density` below. At the end of that question, you were asked to reflect on the inefficiency of rejection sampling in high-dimensional settings. Here, we consider an alternative sampling method called **Gibb's Sampling**.\n",
    "\n",
    "In this question, you'll have the chance to implement Gibb's Sampling using a 1-D rejection sampling subroutine and compare its performance to that of rejection sampling.\n",
    "\n",
    "*Throughout this question we will assume that our computers have access only to normal and uniform random variables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f358add",
   "metadata": {},
   "source": [
    "### Importing Setup from Lab 4\n",
    "\n",
    "Here, we start by importing the setup to Lab 4 Question 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9650e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the target unnormalized density from which we would like to sample\n",
    "# Run this to define the function\n",
    "# No TODOs here\n",
    "@np.vectorize # <- decorator, makes function run faster\n",
    "def target_density(x, y):\n",
    "    mean1 = [1, 1.7]\n",
    "    mean2 = [2, 1.3]\n",
    "    mean3 = [1.5, 1.5]\n",
    "    mean4 = [2, 2.1]\n",
    "    mean5 = [1, 1.2]\n",
    "    cov1=0.2*np.array([[0.2, -0.05], [-0.05, 0.1]])\n",
    "    cov2 = 0.3*np.array([[0.1, 0.07], [0.07, 0.2]])\n",
    "    cov3= np.array([[0.1, 0], [0, 0.1]])\n",
    "    cov4 = 0.1*np.array([[0.3, 0.04], [0.04, 0.2]])\n",
    "    cov5 = 0.1*np.array([[0.4, -0.04], [-0.04, 0.2]])\n",
    "    return(multivariate_normal.pdf([x, y], mean=mean1, cov=cov1) + \n",
    "           multivariate_normal.pdf([x, y], mean=mean2, cov=cov2) +\n",
    "           2*multivariate_normal.pdf([x, y], mean=mean3, cov=cov3) +\n",
    "           0.5*multivariate_normal.pdf([x, y], mean=mean4, cov=cov4)+\n",
    "           0.5*multivariate_normal.pdf([x, y], mean=mean5, cov=cov5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6227834",
   "metadata": {},
   "source": [
    "#### Let's visualize this density. \n",
    "\n",
    "Run the cell below to see a 3D plot of the function along with a contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c898cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run the cell to make plots\n",
    "# Create a meshgrid of coordinates\n",
    "coords = np.arange(0.5, 2.5, 0.02)\n",
    "X, Y = np.meshgrid(coords, coords)\n",
    "\n",
    "# Compute the value of the target density at all pairs of (x,y) values\n",
    "Z = target_density(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70652bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the 3D plot of the target density\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "ax0 = fig.add_subplot(121, projection='3d')\n",
    "ax1 = fig.add_subplot(122)\n",
    "\n",
    "surf = ax0.plot_surface(X,Y,Z, cmap=cm.plasma, linewidth=0, antialiased=False,alpha = 0.9,)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax0.set_zlim(0, 7)\n",
    "ax0.set_xlabel(\"X\")\n",
    "ax0.set_ylabel(\"Y\")\n",
    "ax0.set_zlabel(\"Z\")\n",
    "ax0.set_title(\"3D plot of the target density\")\n",
    "\n",
    "# Rotate the axes: you can change these numbers in order to see the distribution from other angles\n",
    "ax0.view_init(50, 25)\n",
    "\n",
    "# Plot the contour plot of the density\n",
    "cont = ax1.contour(X,Y,Z, levels = 20, cmap=cm.plasma, linewidths=1)\n",
    "ax1.set_xlabel(\"X\")\n",
    "ax1.set_ylabel(\"Y\")\n",
    "ax1.set_title(\"Contour plot of the target density\")\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5, ax=ax1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc4772",
   "metadata": {},
   "source": [
    "Take a moment to examine the plots. Make sure you can see correspondances between each peak in the 3D plot on the left; and the \"high-altitude\" regions in the countour plot on the right.\n",
    "\n",
    "Next we will plot 1-dimensional projections of the target densities onto the $X$ and $Y$ axis. These correspond to conditional target distributions of the form $q(x, y=y')$ and $q(x=x', y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09031270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "# Run the cell below to define the plotting functions\n",
    "\n",
    "COORDINATES = np.arange(0, 3, 0.02)\n",
    "def plot_x_cond(y_val):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(12)\n",
    "    axs[0].contour(X,Y,Z, levels = 20, cmap=cm.plasma, alpha = 0.8, linewidths=0.8)\n",
    "    axs[0].axhline(y_val,  ls=\"--\", color = 'olive', lw = 2)\n",
    "    axs[0].set_xlabel(\"X\")\n",
    "    axs[0].set_ylabel(\"Y\")\n",
    "    axs[0].set_title(\"Contour plot of the target density\")\n",
    "    \n",
    "    axs[1].plot(COORDINATES, target_density(COORDINATES, y_val), color = 'olive')\n",
    "    axs[1].set_ylim(0,10)\n",
    "    axs[1].set_xlim(0,3)\n",
    "    axs[1].set_xlabel(\"X\")\n",
    "    axs[1].set_title(\"Conditional target density: q(x | y={:.1f})\".format(y_val))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_y_cond(x_val):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(12)\n",
    "    axs[0].contour(X,Y,Z, levels = 20, cmap=cm.plasma, alpha = 0.8, linewidths=0.8)\n",
    "    axs[0].axvline(x_val,  ls=\"--\", color = 'olive', lw = 2)\n",
    "    axs[0].set_xlabel(\"X\")\n",
    "    axs[0].set_ylabel(\"Y\")\n",
    "    axs[0].set_title(\"Contour plot of the target density\")\n",
    "    \n",
    "    axs[1].plot(COORDINATES, target_density(x_val, COORDINATES), color = 'olive')\n",
    "    axs[1].set_ylim(0,10)\n",
    "    axs[1].set_xlim(0,3)\n",
    "    axs[1].set_xlabel(\"Y\")\n",
    "    axs[1].set_title(\"Conditional target density: q(y | x={:.1f})\".format(x_val))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bba746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interactive plot\n",
    "interactive_plot = interactive(plot_x_cond, y_val=(0, 3, 0.1), add_proposal=False)\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb7608",
   "metadata": {},
   "source": [
    "## Importing Rejection Sampling Helper Functions\n",
    "\n",
    "From Lab 4 Question 1, we also bring back the helper functions that you defined in the question. These will be helpful later when we implement our Gibb's sampler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_1D_proposed_distribution(N):\n",
    "    \"\"\" \n",
    "    Produces N samples from the Uniform(0,3) proposal distribution\n",
    "    \n",
    "    Inputs:\n",
    "        N : int, desired number of samples\n",
    "        \n",
    "    Outputs:\n",
    "        proposed_samples : an 1d-array of size N which contains N independent samples from the proposal\n",
    "    \"\"\"\n",
    "    \n",
    "    proposed_samples = uniform.rvs(0,3, N) \n",
    "    return(proposed_samples)\n",
    "\n",
    "@np.vectorize\n",
    "def compute_ratio_1D(proposed_sample, c):\n",
    "    \"\"\"\n",
    "    Computes the ratio between the scaled target density and proposal density evaluated at the \n",
    "    proposed sample point\n",
    "    \n",
    "    Inputs:\n",
    "        proposed_sample : float, proposed sample\n",
    "        c : float, constant scaling factor that ensures that the proposal density is above the target density\n",
    "        \n",
    "    Outputs:\n",
    "        ratio : float\n",
    "    \"\"\"\n",
    "\n",
    "    ratio = target_1D_density(proposed_sample)*c / (1/3) \n",
    "    assert(ratio <= 1)\n",
    "    return(ratio)\n",
    "\n",
    "@np.vectorize\n",
    "def accept_proposal(ratio):\n",
    "    \"\"\" \n",
    "    Accepts or rejects a proposal with probability equal to ratio\n",
    "    \n",
    "    Inputs: \n",
    "        ratio: float, probability of acceptance\n",
    "    \n",
    "    Outputs:\n",
    "        accept: True/False, if True, accept the proposal\n",
    "    \"\"\"\n",
    "    gamma = uniform.rvs(0, 1)\n",
    "    accept = gamma <= ratio\n",
    "    return(accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df88e5",
   "metadata": {},
   "source": [
    "## Question 1: Gibbs Sampling\n",
    "\n",
    "In this question, we will build a Gibbs sampler and apply it to the same density. First, let's go over the basics of Gibbs Sampling.\n",
    "\n",
    "Assume we want to sample from an unnormalized target density $q(x, y)$. \n",
    "\n",
    "#### Gibbs Sampling proceeds as follows:\n",
    "\n",
    "- Start at an initial point $(x_0, y_0)$\n",
    "- For `i` in `number of iterations`:\n",
    "    - Condition on $y=y_{i-1}$: Sample $x_i \\sim q(x| y=y_{i-1})$ \n",
    "        - Add $(x_i, y_{i-1})$ to the list of samples\n",
    "    - Condition on $x=x_{i}$: : Sample $y_i \\sim q(y| x=x_{i})$ \n",
    "        - Add $(x_i, y_{i})$ to the list of samples\n",
    "    \n",
    "In many problems we can sample the univariate distributions directly. In this case we don't know how to sample them directly, but we can use the 1-D rejection sampler that we computed in Lab 4 Question 1 (functions imported above).\n",
    "\n",
    "In the cell below, we wrote helper functions that sample from the conditionals above. They are essentially the same function you wrote in Lab 4, just slightly modified such that we perform rejection sampling until we get one valid sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a068dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here:\n",
    "# Just look at these helper functions and make sure you understand the syntax\n",
    "\n",
    "def sample_x_cond(fixed_y_val):\n",
    "    \"\"\" \n",
    "    Produces one sample from x_i ~ q(x | y=fixed_y_val)\n",
    "    \n",
    "    Inputs:\n",
    "        fixed_y_val : float, current value of y, on which we condition\n",
    "    \n",
    "    Outputs:\n",
    "        x_sample: float, one sample from x_i ~ q(x, y=fixed_y_val)\n",
    "        num_samples : int, number of tries until we accepted a sample\n",
    "        \n",
    "    \"\"\"\n",
    "    def conditional_density(x):\n",
    "        return(target_density(x, fixed_y_val))\n",
    "    x_sample = None\n",
    "    num_samples = 0\n",
    "    c = 0.33/(0.2+max(conditional_density(np.arange(0.5, 2.5, 0.05)))) # <- we are cheating a bit here by \n",
    "                                                                       # looking for a tight c value\n",
    "    while x_sample is None:\n",
    "        proposed_sample = sample_1D_proposed_distribution(1)\n",
    "        num_samples += 1\n",
    "        ratio = conditional_density(proposed_sample)*3*c\n",
    "        assert(ratio <= 1)\n",
    "        accept = accept_proposal(ratio)\n",
    "        if accept:\n",
    "            x_sample = proposed_sample[0]\n",
    "    return(x_sample, num_samples)\n",
    "\n",
    "\n",
    "def sample_y_cond(fixed_x_val):\n",
    "    \"\"\" \n",
    "    Produces one sample from y_i ~ q(y | x=fixed_x_val)\n",
    "    \n",
    "    Inputs:\n",
    "        fixed_x_val : float, current value of y, on which we condition\n",
    "    \n",
    "    Outputs:\n",
    "        y_sample: float, one sample from y_i ~ q(y | x=fixed_x_val)\n",
    "        num_samples : int, number of tries until we accepted a sample\n",
    "        \n",
    "    \"\"\"\n",
    "    def conditional_density(y):\n",
    "        return(target_density(fixed_x_val, y))\n",
    "    y_sample = None\n",
    "    num_samples = 0\n",
    "    c = 0.33/(0.2+max(conditional_density(np.arange(0.5, 2.5, 0.05))))\n",
    "    while y_sample is None:\n",
    "        proposed_sample = sample_1D_proposed_distribution(1)\n",
    "        num_samples += 1\n",
    "        ratio = conditional_density(proposed_sample)*3*c\n",
    "        assert(ratio <= 1)\n",
    "        accept = accept_proposal(ratio)\n",
    "        if accept:\n",
    "            y_sample = proposed_sample[0]\n",
    "    return(y_sample, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5465ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1a) Build a Gibbs sampler using the helper functions above\n",
    "**Note**: Don't forget that at each iteration the Gibbs sampler adds two samples to the list of samples: $(x_i, y_{i-1})$ and $(x_i, y_{i})$\n",
    "\n",
    "**Hint 1**: If you cannot pass the test, try making a simple test case with `N=2` and adding some print statements to your code.\n",
    "\n",
    "**Hint 2**: In our implementation of gibbs sampling, `num_samples` is not trivially equal to `N`, since sampling $x_i \\sim f(x| y=y_{i-1})$ and $y_i \\sim f(y| x=x_{i})$ rely on rejection sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349a6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_2D_Gibbs_samples(N, x_0, y_0):\n",
    "    \"\"\"\n",
    "    Produces N samples from the target density using Gibbs Sampling\n",
    "    \n",
    "    Inputs: \n",
    "        N : desired number of samples\n",
    "        x_0, y_0 : floats, the coordinates of the starting point\n",
    "        \n",
    "    Outputs:\n",
    "        gibbs_samples : array of dimension (N, 2) where each row is a sample from the target distribution\n",
    "                        of the form (x_i, y_i)\n",
    "        num_samples : total number of samples required\n",
    "    \"\"\"\n",
    "    gibbs_samples = [] # Each entry corresponds to a (x_i, y_i)\n",
    "    num_samples = 0 # Add the number of samples to this variable, note this is not equal to N since rejection sampling\n",
    "                    # does not accept every sample\n",
    "    x_curr = x_0 # Current value of x, initialized to x_0\n",
    "    y_curr = y_0 # Current value of y, initialized to y_0\n",
    "    \n",
    "    for i in range(N//2): # The range is N//2 since we are generating two gibbs samples in one iteration\n",
    "        ...\n",
    "        \n",
    "    return(gibbs_samples, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b74ed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a88504",
   "metadata": {},
   "source": [
    "### 1b) Path traced by the Gibbs sampler\n",
    "Run the code below to overlay the path traced by the Gibbs Sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "# Just run this once you've passed the validation tests above\n",
    "N = 20\n",
    "target_samples, total_samples = get_2D_Gibbs_samples(N, 1, 1)\n",
    "target_samples = np.array(target_samples)\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "# Plot the contour plot of the density\n",
    "cont = plt.contour(X,Y,Z, levels = 20, cmap=cm.plasma, linewidths=1, alpha = 0.8)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Path of the Gibbs Sampler\")\n",
    "\n",
    "# Add sample points obtained via Gibbs sampling\n",
    "plt.scatter(target_samples[:,0], target_samples[:,1], c='b', alpha = 1, s=50, label = 'Samples')\n",
    "for i in range(N):\n",
    "    plt.annotate(i, (target_samples[i,0], target_samples[i,1]), fontsize = 20)\n",
    "plt.plot(target_samples[:,0], target_samples[:,1], c='r', alpha = 1, label = 'Path of the Gibbs Sampler')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b362e25",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Inspect the scatter plot above. Trace the Gibbs sampler path from the initial point (labeled 0) to the final point. What do you notice about the orientation of the paths between each point? Why are they oriented in this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755c2dd",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b7074",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 1c) 'Efficiency' of Gibbs Sampling\n",
    "Let's compute 1000 Gibbs samples and compute how many times the rejection sampling subroutine accepted the proposed sample (running this might take a little while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ce4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify this\n",
    "# just run it and comment in the section below\n",
    "N = 1000\n",
    "target_samples, total_samples = get_2D_Gibbs_samples(N, 1, 1)\n",
    "acceptance_rate = N/total_samples*100\n",
    "print(\"The acceptance rate for Gibbs Sampling is {:.1f}%\".format(acceptance_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d544e",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### How does Gibbs Sampling compare to vanilla Rejection Sampling from Lab 4? Is this approach more efficient or less efficient? What is the source of this increase/decrease in efficiency?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097254a",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc70dec",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Part II: GLM\n",
    "\n",
    "Now, we will pivot to discussing frequentist and Bayesian approaches to generalized linear models.\n",
    "## Question 2: Atlantic Hurricane Season\n",
    "\n",
    "With 30 named storms, the 2020 Atlantic hurricane season was the most active on record. Climate scientists argue that the culprit is human induced global warming. There is a an evergrowing body of research linking increased average temperatures and rising sea levels to more frequent, more intense and more destructive storms. \n",
    "\n",
    "In this lab we will investigate the number of named storms recorded since 1880, and we will argue that there is a statistically significant relationship between rising Sea Surface Temperature (SST) and the frequency of named storms.\n",
    "\n",
    "For this lab we extracted the number of tropical storms from the [HURDAT Database](https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2019-052520.txt). We also extracted data on Sea Surface Temperatures from the [National Center for Atmosferic Research](https://climatedataguide.ucar.edu/climate-data/global-surface-temperature-data-gistemp-nasa-goddard-institute-space-studies-giss). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f19cf7",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b023ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run the code to load the data\n",
    "data_source = \"hurricane_data.csv\"\n",
    "df = pd.read_csv(data_source)\n",
    "df = df[[\"Year\", \"Num_Storms\", \"Temp_Anomaly\"]]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070870f",
   "metadata": {},
   "source": [
    "### Model Specifications\n",
    "\n",
    "The `Num_Storms` column contains the number of named storms recorded each year between 1880 and 2019. The `Temp_Anomaly` column contains the deviation in yearly SST from the mean of 1951-1980.\n",
    "\n",
    "In this question, to show that there is a statistically significant relationship between rising Sea Surface Temperature (SST) and the frequency of named storms, we will model the number of named storms in Year $i$ using **Poisson Regression**:\n",
    "\n",
    "$$\\lambda_i = e^{q_0 + q_1 X_i}$$ \n",
    "\n",
    "$$C_i \\sim \\text{Poisson}(\\lambda_i),$$\n",
    "\n",
    "where $X_i$ is the SST deviation in Year $i$, and $C_i$ is the number of named storms in year $i$.\n",
    "\n",
    "This isn't something that we can easily solve from scratch, so we have to use software packages. In this question, we'll explore the two approaches to GLMs that we covered in class: \n",
    "\n",
    "1. **(Q2b) Frequentist Regression** using [`statsmodels.api`](https://www.statsmodels.org/stable/glm.html) \n",
    "2. **(Q2c) Bayesian Regression** via sampling using [`PyMC`](https://www.pymc.io/welcome.html) and [`Bambi`](https://bambinos.github.io/bambi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96bb708",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2a) Understanding Check\n",
    "\n",
    "The model we described above is a GLM. What is \"Linear\" about this GLM model? What's the inverse link function? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e4704",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cfa450",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2b) Frequentist Regression\n",
    "\n",
    "Let's start by considering the problem from a frequentist lens. To do this, we'll use the `statsmodels.api`, which allows us to create a model in just a few lines of code.\n",
    "\n",
    "After fitting our model, we can call the `.summary()` method, and get a breakdown of our model and some details on how well it fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15717428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Poisson GLM model where Temp_Anomaly is a covariate (exogenous variable): No need to modify\n",
    "freq_model = sm.GLM(df[\"Num_Storms\"], exog = sm.add_constant(df[\"Temp_Anomaly\"]), \n",
    "                  family=sm.families.Poisson())\n",
    "freq_res = freq_model.fit()\n",
    "print(freq_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081a62e",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2bi) Understanding the table\n",
    "\n",
    "What variable does `Temp_Anomaly`'s `coef` in the table correspond to in our model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f908e81",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abd61f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2bii) Inspecting the results of fitting `freq_model`. \n",
    "\n",
    "Does the model suggest that increased SST relate to more storms? Is the influnce of SST on number of storms statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529297f",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb513c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2c) Bayesian Regression via PyMC\n",
    "\n",
    "Now that we've done Poisson regression the frequentist way with the `statsmodels` package, let's try implementing it the Bayesian way! In this lab we'll explore two ways of doing this:\n",
    "1. Building the model from scratch in `PyMC`\n",
    "2. Using `Bambi`, a wrapper on `PyMC` that simplifies model construction\n",
    "\n",
    "To start, let's build our model from scratch in `PyMC`. Unlike the `statsmodels` package, `PyMC` requires us to specify our model piece by piece. That means that, as with any Bayesian parameter estimation task, we have to distill our problem setup into a likelihood and prior before we can proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69691ca2",
   "metadata": {},
   "source": [
    "### 2ci) Unpacking $C_i \\sim \\text{Poisson}(\\lambda_i)$\n",
    "\n",
    "Recall the problem setup:\n",
    "\n",
    "Our model involves the relationship between rising Sea Surface Temperature (SST) and the frequency of named storms, where:\n",
    "\n",
    "$$\\lambda_i = e^{q_0 + q_1 X_i}$$ \n",
    "\n",
    "$$C_i \\sim \\text{Poisson}(\\lambda_i),$$\n",
    "\n",
    "where $X_i$ is the SST deviation in Year $i$, and $C_i$ is the number of named storms in year $i$.\n",
    "\n",
    "**In the cell below, Choose the option that best fills in the blank in the following statement:**\n",
    "\n",
    "$C_i \\sim \\text{Poisson}(\\lambda_i)$ represents our _____:\n",
    "\n",
    "**A.** Prior\n",
    "\n",
    "**B.** Likelihood\n",
    "\n",
    "**C.** Posterior\n",
    "\n",
    "Your answer should be a string, either `\"A\"`, `\"B\"`, or `\"C\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecda845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q2c_i = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb7b70",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c498f",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2cii) Picking our prior(s)\n",
    "\n",
    "Now, let's examine the parameter $\\lambda_i = e^{q_0 + q_1 X_i}$. As discussed in lecture, $\\lambda_i$ is comprised of a linear function of our observations ($q^TX$) and an inverse-link function ($e$). Given the setup of our problem, answer the following questions using 1 sentence each:\n",
    "1. How many prior distributions do we need to define?\n",
    "2. What parameters will we define these prior distributions for?\n",
    "3. Since we don't have any prior information about these random variables, what distribution (i.e Normal, Beta, etc.) should we pick for their priors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33167a",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe48f8",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 2civ) Defining our PyMC Model\n",
    "Given your answers to `q2c_i` and `q2c_ii`, you're now ready to make your PyMC model! Fill out the code cell below to build your model and sample from the posterior.\n",
    "\n",
    "**Note:** To pass the test, \n",
    "1. Do not remove the variables currently present in the model and/or add your own additional variables. Just fill in any ellipsis; the variables defined here are more than enough for you to solve the question!\n",
    "2. Make sure the name parameter you pass to each `Distribution` object matches the variable name it's assigned to. Your answers should follow the following format: \n",
    "\n",
    "```\n",
    "with pm.Model() as model:\n",
    "\n",
    "    q0 = pm.Some_Distribution('q0', ...)\n",
    "    q1 = pm.Some_Distribution('q1', ...)`\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Hint 1**: Remember that random variables can be added, subtracted, multiplied, etc. just like normal numbers!\n",
    "\n",
    "**Hint 2**: Not all variables defined in a model context necessarily have to be defined using `pm.Some_Distribution(...)`. In particular, we do not need `pm.Deterministic` when defining `lam`, since we're not interested in its posterior samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2254d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    q0 = ...\n",
    "    q1 = ...\n",
    "\n",
    "    lam = ...\n",
    "    \n",
    "    Y_obs = ...\n",
    "\n",
    "    # DO NOT CHANGE THE SAMPLING ROUTINE\n",
    "    trace = pm.sample(2000, chains = 4, random_seed = 0, return_inferencedata = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581dfcb9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c_iv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b206e15",
   "metadata": {},
   "source": [
    "Now that we've ran PyMC, we can visualize the posterior distributions of $q_0$ and $q_1$ and find our estimates $\\hat{q}_0$ and $\\hat{q}_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66757f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace, round_to=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccfcf6f",
   "metadata": {},
   "source": [
    "## 2d) Bayesian Regression via Bambi\n",
    "\n",
    "Whew! `q2c` took *a lot* of code just to build a simple Poisson regression. In practice, building models from scratch like this is usually unnecessary unless we're looking for a custom solution. Instead, we can rely on packages like `Bambi` that *wrap* the functionality of `PyMC` with a simpler interface purely designed for GLMs.\n",
    "\n",
    "Using [`Bambi` documentation](https://bambinos.github.io/bambi/) as a guide, fill out the code cell below to fit a Bayesian Poisson Regression model on our data. \n",
    "\n",
    "**Note 1**: To pass the autograder, make sure you pass your model the `random_seed = 0` when you fit it!\n",
    "\n",
    "**Note 2**: Notice that the `Bambi` package doesn't need us to specify a prior: when we don't specify a prior for our model parameters, Bambi automatically supplies a weakly informative one based on your data by default. For this question, we are okay with this behavior: **to pass the test, do not specify a prior on your parameters!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c997c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_model = ...\n",
    "my_model_samples = ...\n",
    "\n",
    "az.plot_posterior(my_model_samples, round_to=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae966d2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdaabd",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2e) Understanding the plots\n",
    "What the are x-axis and y-axis in each of the plots in 2d? Your answer should be in terms of the parameters of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7ce90",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762cf48a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2f) Comparison\n",
    "\n",
    "Compare the results of `freq_model` in 2b with the plot in 2d. Are the estimates of Frequentist and Bayesian Regression close to each other? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67beef",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf3751",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Congratulations! You have finished Lab 5! ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13021ce8",
   "metadata": {},
   "source": [
    "Below, you will see two cells. Running the first cell will automatically generate a PDF of all questions that need to be manually graded, and running the second cell will automatically generate a zip with your autograded answers. **You are responsible for both the coding portion (the zip from Lab 5) and the written portion (the PDF of written responses from Lab 5) to their respective Gradescope portals.** The coding proportion should be submitted to the `Lab 5` assignment as a single zip file, and the written portion should be submitted to `Lab 5 PDF` assignment as a single pdf file. When submitting the written portion, please ensure you select pages appropriately.\n",
    "\n",
    "If there are issues with automatically generating the PDF in the first cell, you can try downloading the notebook as a PDF by clicking on `File -> Save and Export Notebook As... -> PDF`. If that doesn't work either, you can manually take screenshots of your answers to the manually graded questions and submit those. Either way, **you are responsible for ensuring your submission follows our requirements, we will NOT be granting regrade requests for submissions that don't follow instructions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"lab05.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('lab05.pdf')):\n",
    "    img = mpimg.imread('chinchilla.jpg')\n",
    "    imgplot = plt.imshow(img)\n",
    "    imgplot.axes.get_xaxis().set_visible(False)\n",
    "    imgplot.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    display(HTML(\"Download your PDF <a href='lab05.pdf' download>here</a>.\"))\n",
    "else:\n",
    "    print(\"\\n Pdf generation fails, please try the other methods described above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce3f14",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fd769",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24243cd",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> N = 100\n>>> output = get_2D_Gibbs_samples(N, 1, 1)\n>>> assert len(output) == 2\n>>> assert len(output[0]) == 100\n>>> assert len(output[0][0]) == 2\n>>> assert np.abs(410 - output[1]) < 100\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c_i": {
     "name": "q2c_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert q2c_i.upper() == \"B\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c_iv": {
     "name": "q2c_iv",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> first_q0_10 = np.array([2.31040879, 2.28676486, 2.21883405, 2.21736877, 2.22888946,\n...        2.21329085, 2.23160438, 2.24657932, 2.27997392, 2.26348672])\n>>> assert np.allclose(trace.posterior.q0[:, :].to_numpy().flatten()[:10], first_q0_10)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> first_q1_10 = np.array([0.48528855, 0.45162568, 0.58493267, 0.56287812, 0.59027936,\n...        0.54309099, 0.42550684, 0.4674696 , 0.46644434, 0.55409565])\n>>> assert np.allclose(trace.posterior.q1[:, :].to_numpy().flatten()[:10], first_q1_10)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bambi_ta_10 = np.array([0.4844136 , 0.4844136 , 0.52475228, 0.5088275 , 0.5088275 ,\n...        0.37721286, 0.44015103, 0.50478428, 0.40513035, 0.43697324])\n>>> \n>>> assert np.allclose(my_model_samples.posterior.Temp_Anomaly.to_numpy().flatten()[:10], bambi_ta_10)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bambi_int_10 = np.array([2.20638771, 2.20638771, 2.26402576, 2.2418237 , 2.2418237 ,\n...        2.21996355, 2.26259078, 2.22356198, 2.23562492, 2.27393475])\n>>> \n>>> assert np.allclose(my_model_samples.posterior.Intercept.to_numpy().flatten()[:10], bambi_int_10)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
